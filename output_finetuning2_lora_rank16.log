Epoch 1 Training:   0%|                                                                                                          | 0/13500 [00:00<?, ?it/s]C:\Projects\Sereact_coding_challenge\.venv\Lib\site-packages\torch\utils\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
outputs_pos.loss : 1.1950231790542603
Epoch 1 Training:   0%|â–                                                                        | 30/13500 [12:17<184:55:05, 49.42s/it, loss=1.4, batch=29]
outputs_pos.loss : 2.0076074600219727
outputs_pos.loss : 1.7201679944992065
outputs_pos.loss : 2.0216784477233887
outputs_pos.loss : 1.643480896949768
outputs_pos.loss : 1.8926275968551636
outputs_pos.loss : 1.2842937707901
outputs_pos.loss : 1.3496466875076294
Epoch 00001: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.2772575616836548
outputs_pos.loss : 1.8918412923812866
outputs_pos.loss : 1.547792911529541
outputs_pos.loss : 1.6635559797286987
outputs_pos.loss : 1.5787055492401123
outputs_pos.loss : 1.069934368133545
outputs_pos.loss : 1.6379739046096802
outputs_pos.loss : 1.352436900138855
outputs_pos.loss : 1.2580763101577759
outputs_pos.loss : 1.6870263814926147
outputs_pos.loss : 1.443030595779419
outputs_pos.loss : 1.8061375617980957
outputs_pos.loss : 1.2283953428268433
outputs_pos.loss : 1.5773720741271973
outputs_pos.loss : 1.4858968257904053
outputs_pos.loss : 2.4604945182800293
Epoch 00002: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.6036406755447388
outputs_pos.loss : 1.3553627729415894
outputs_pos.loss : 1.6290936470031738
outputs_pos.loss : 1.575728416442871
outputs_pos.loss : 2.5840625762939453
outputs_pos.loss : 1.2499909400939941
outputs_pos.loss : 1.516373634338379
outputs_pos.loss : 1.901257872581482
Epoch 00003: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.7138071060180664
outputs_pos.loss : 1.6910269260406494
outputs_pos.loss : 1.697477102279663
outputs_pos.loss : 2.015723943710327
outputs_pos.loss : 1.1072596311569214
outputs_pos.loss : 1.5784157514572144
outputs_pos.loss : 1.488884449005127
outputs_pos.loss : 1.5651074647903442
Epoch 00004: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.7945423126220703
outputs_pos.loss : 1.259902834892273
outputs_pos.loss : 1.2665705680847168
outputs_pos.loss : 1.3323918581008911
outputs_pos.loss : 1.603041172027588
outputs_pos.loss : 1.3969062566757202
outputs_pos.loss : 1.6952548027038574
outputs_pos.loss : 1.2857877016067505
Epoch 00005: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.8108711242675781
outputs_pos.loss : 1.5862356424331665
outputs_pos.loss : 1.5078707933425903
outputs_pos.loss : 1.681962251663208
outputs_pos.loss : 1.8138751983642578
outputs_pos.loss : 1.8957499265670776
outputs_pos.loss : 1.253440499305725
outputs_pos.loss : 1.4192639589309692
outputs_pos.loss : 1.7799656391143799
outputs_pos.loss : 1.1856857538223267
outputs_pos.loss : 1.0583583116531372
outputs_pos.loss : 1.343823790550232
outputs_pos.loss : 1.2157059907913208
outputs_pos.loss : 1.1581910848617554
outputs_pos.loss : 1.672842025756836
outputs_pos.loss : 1.2599157094955444
Epoch 00006: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 2.281984329223633
outputs_pos.loss : 1.3094559907913208
outputs_pos.loss : 1.833275318145752
outputs_pos.loss : 1.1382795572280884
outputs_pos.loss : 1.6786268949508667
outputs_pos.loss : 1.6259346008300781
outputs_pos.loss : 1.1312259435653687
outputs_pos.loss : 1.696564793586731
Epoch 00007: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.713295817375183
outputs_pos.loss : 1.8307234048843384
outputs_pos.loss : 1.6233336925506592
outputs_pos.loss : 1.45474374294281
outputs_pos.loss : 1.579879641532898
outputs_pos.loss : 1.3907878398895264
outputs_pos.loss : 1.2313954830169678
outputs_pos.loss : 2.04876446723938
Epoch 00008: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.959564208984375
outputs_pos.loss : 0.9367711544036865
outputs_pos.loss : 1.6401152610778809
outputs_pos.loss : 1.0262112617492676
outputs_pos.loss : 1.2007062435150146
outputs_pos.loss : 1.8275535106658936
outputs_pos.loss : 1.4353998899459839
outputs_pos.loss : 1.8736003637313843
Epoch 00009: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 2.204667091369629
outputs_pos.loss : 1.8291358947753906
outputs_pos.loss : 1.9608445167541504
outputs_pos.loss : 1.6867657899856567
outputs_pos.loss : 2.2435929775238037
outputs_pos.loss : 1.6453173160552979
outputs_pos.loss : 1.1332387924194336
outputs_pos.loss : 2.164717674255371
Epoch 00010: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.8463298082351685
outputs_pos.loss : 1.5807708501815796
outputs_pos.loss : 1.3252347707748413
outputs_pos.loss : 1.4280980825424194
outputs_pos.loss : 1.7027441263198853
outputs_pos.loss : 1.902980089187622
outputs_pos.loss : 1.0795979499816895
outputs_pos.loss : 1.463308572769165
Epoch 00011: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.5491907596588135
outputs_pos.loss : 1.295633316040039
outputs_pos.loss : 1.0941152572631836
outputs_pos.loss : 1.9693450927734375
outputs_pos.loss : 1.6219549179077148
outputs_pos.loss : 1.6162023544311523
outputs_pos.loss : 1.3352099657058716
outputs_pos.loss : 1.4273626804351807
Epoch 00012: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.6820882558822632
outputs_pos.loss : 1.6982542276382446
outputs_pos.loss : 1.5732066631317139
outputs_pos.loss : 1.7065012454986572
outputs_pos.loss : 1.9249740839004517
outputs_pos.loss : 1.525237798690796
outputs_pos.loss : 1.544613242149353
outputs_pos.loss : 1.5638346672058105
Epoch 00013: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.7207248210906982
outputs_pos.loss : 1.3469172716140747
outputs_pos.loss : 1.9797837734222412
outputs_pos.loss : 1.41889226436615
outputs_pos.loss : 2.0018012523651123
outputs_pos.loss : 1.5764729976654053
outputs_pos.loss : 1.251582145690918
outputs_pos.loss : 1.7441331148147583
Epoch 00014: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.3559901714324951
outputs_pos.loss : 1.9965112209320068
outputs_pos.loss : 1.491905927658081
outputs_pos.loss : 2.04758620262146
outputs_pos.loss : 2.1170098781585693
outputs_pos.loss : 1.687699317932129
outputs_pos.loss : 1.3526520729064941
outputs_pos.loss : 1.8529969453811646
Epoch 00015: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.8639439344406128
outputs_pos.loss : 1.4325672388076782
outputs_pos.loss : 1.1779224872589111
outputs_pos.loss : 1.2365832328796387
outputs_pos.loss : 1.5346401929855347
outputs_pos.loss : 1.1734510660171509
outputs_pos.loss : 1.2218718528747559
outputs_pos.loss : 1.3984744548797607
Epoch 00016: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 2.2193331718444824
outputs_pos.loss : 1.749957799911499
outputs_pos.loss : 1.1148966550827026
outputs_pos.loss : 1.4755076169967651
outputs_pos.loss : 1.4987863302230835
outputs_pos.loss : 1.5502521991729736
outputs_pos.loss : 1.2477140426635742
outputs_pos.loss : 0.8917991518974304
Epoch 00017: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.981412410736084
outputs_pos.loss : 1.970686674118042
outputs_pos.loss : 1.6115913391113281
outputs_pos.loss : 2.8959834575653076
outputs_pos.loss : 1.6641062498092651
outputs_pos.loss : 1.2090039253234863
outputs_pos.loss : 1.723852276802063
outputs_pos.loss : 1.2887728214263916
Epoch 00018: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.1080596446990967
outputs_pos.loss : 1.271936058998108
outputs_pos.loss : 2.1475653648376465
outputs_pos.loss : 1.1816006898880005
outputs_pos.loss : 1.2001844644546509
outputs_pos.loss : 1.4662038087844849
outputs_pos.loss : 1.5367878675460815
outputs_pos.loss : 1.34904944896698
Epoch 00019: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.0038652420043945
outputs_pos.loss : 1.5976306200027466
outputs_pos.loss : 1.2030342817306519
outputs_pos.loss : 1.2555103302001953
outputs_pos.loss : 1.1781736612319946
outputs_pos.loss : 1.8354612588882446
outputs_pos.loss : 0.883335292339325
outputs_pos.loss : 2.3825013637542725
Epoch 00020: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.1057395935058594
outputs_pos.loss : 1.8783345222473145
outputs_pos.loss : 1.8208917379379272
outputs_pos.loss : 1.2142664194107056
outputs_pos.loss : 1.776593804359436
outputs_pos.loss : 1.3781380653381348
outputs_pos.loss : 1.8281140327453613
outputs_pos.loss : 2.060601234436035
Epoch 00021: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.6588369607925415
outputs_pos.loss : 1.7900832891464233
outputs_pos.loss : 1.8698126077651978
outputs_pos.loss : 1.4284605979919434
outputs_pos.loss : 1.5238163471221924
outputs_pos.loss : 1.6385338306427002
outputs_pos.loss : 1.4000883102416992
outputs_pos.loss : 2.0422780513763428
Epoch 00022: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.1025915145874023
outputs_pos.loss : 0.9230778813362122
outputs_pos.loss : 1.5668039321899414
outputs_pos.loss : 1.2592847347259521
outputs_pos.loss : 2.108646869659424
outputs_pos.loss : 1.1584532260894775
outputs_pos.loss : 1.5383446216583252
outputs_pos.loss : 1.4930520057678223
Epoch 00023: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.4857555627822876
outputs_pos.loss : 1.3767848014831543
outputs_pos.loss : 1.24569571018219
outputs_pos.loss : 1.2068049907684326
outputs_pos.loss : 1.5909864902496338
outputs_pos.loss : 1.0082041025161743
outputs_pos.loss : 1.590903401374817
outputs_pos.loss : 1.2010587453842163
Epoch 00024: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.7350575923919678
outputs_pos.loss : 1.4734439849853516
outputs_pos.loss : 1.3803837299346924
outputs_pos.loss : 1.20497465133667
outputs_pos.loss : 1.752186894416809
outputs_pos.loss : 1.735164999961853
outputs_pos.loss : 1.2198930978775024
outputs_pos.loss : 1.424502968788147
Epoch 00025: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.530326008796692
outputs_pos.loss : 1.5512008666992188
outputs_pos.loss : 1.5945309400558472
outputs_pos.loss : 1.886427879333496
outputs_pos.loss : 1.7553513050079346
outputs_pos.loss : 1.6301456689834595
outputs_pos.loss : 1.5238960981369019
outputs_pos.loss : 1.813886284828186
Epoch 00026: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.7123324871063232
outputs_pos.loss : 1.79108464717865
outputs_pos.loss : 1.4843882322311401
outputs_pos.loss : 1.9133350849151611
outputs_pos.loss : 1.818580150604248
outputs_pos.loss : 1.5691815614700317
outputs_pos.loss : 1.619158387184143
outputs_pos.loss : 1.6665223836898804
Epoch 00027: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.313754677772522
outputs_pos.loss : 1.7287979125976562
outputs_pos.loss : 1.1124963760375977
outputs_pos.loss : 1.8966306447982788
outputs_pos.loss : 1.3482489585876465
outputs_pos.loss : 1.4412901401519775
outputs_pos.loss : 1.3178391456604004
outputs_pos.loss : 1.4754472970962524
Epoch 00028: adjusting learning rate of group 0 to 2.5000e-06.
outputs_pos.loss : 1.1951829195022583
outputs_pos.loss : 1.2096712589263916
outputs_pos.loss : 1.3016613721847534
outputs_pos.loss : 1.3882842063903809
outputs_pos.loss : 1.1576331853866577
outputs_pos.loss : 2.213608980178833
outputs_pos.loss : 1.2570499181747437
outputs_pos.loss : 1.7797882556915283
Epoch 00029: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.2114062309265137
outputs_pos.loss : 1.559307336807251
outputs_pos.loss : 1.2495514154434204
outputs_pos.loss : 1.4607038497924805
outputs_pos.loss : 1.8454965353012085
outputs_pos.loss : 1.2129231691360474
outputs_pos.loss : 1.345568060874939
outputs_pos.loss : 1.5081489086151123
Epoch 00030: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.4261889457702637
outputs_pos.loss : 1.4296019077301025
outputs_pos.loss : 1.5538115501403809
outputs_pos.loss : 1.317511796951294
outputs_pos.loss : 1.5452613830566406
outputs_pos.loss : 1.4981229305267334
outputs_pos.loss : 1.4711315631866455
outputs_pos.loss : 1.547534465789795
Epoch 00031: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.0313153266906738
outputs_pos.loss : 1.7181110382080078
outputs_pos.loss : 1.4058963060379028
outputs_pos.loss : 1.637948989868164
outputs_pos.loss : 1.1295652389526367
outputs_pos.loss : 1.697688102722168
outputs_pos.loss : 1.8518911600112915
outputs_pos.loss : 1.0796905755996704
Epoch 00032: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.6664302349090576
outputs_pos.loss : 1.6369913816452026
outputs_pos.loss : 1.9589996337890625
outputs_pos.loss : 0.9299453496932983
outputs_pos.loss : 1.7421460151672363
outputs_pos.loss : 2.822080135345459
outputs_pos.loss : 1.2385475635528564
outputs_pos.loss : 1.2705039978027344
Epoch 00033: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.5887867212295532
outputs_pos.loss : 1.5407462120056152
outputs_pos.loss : 1.3491231203079224
outputs_pos.loss : 1.7144138813018799
outputs_pos.loss : 1.6116046905517578
outputs_pos.loss : 1.8790229558944702
outputs_pos.loss : 1.6234829425811768
outputs_pos.loss : 1.3098478317260742
Epoch 00034: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.127242088317871
outputs_pos.loss : 1.246958613395691
outputs_pos.loss : 1.199241280555725
outputs_pos.loss : 1.4260438680648804
outputs_pos.loss : 1.4745354652404785
outputs_pos.loss : 1.1328964233398438
outputs_pos.loss : 1.145696759223938
outputs_pos.loss : 1.0576099157333374
Epoch 00035: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.2385573387145996
outputs_pos.loss : 1.1196644306182861
outputs_pos.loss : 1.0791741609573364
outputs_pos.loss : 1.8148995637893677
outputs_pos.loss : 1.5780776739120483
outputs_pos.loss : 1.7200361490249634
outputs_pos.loss : 1.436211109161377
outputs_pos.loss : 1.0790107250213623
Epoch 00036: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.5348697900772095
outputs_pos.loss : 1.6812667846679688
outputs_pos.loss : 1.6817084550857544
outputs_pos.loss : 1.276405930519104
outputs_pos.loss : 1.3767520189285278
outputs_pos.loss : 1.2002440690994263
outputs_pos.loss : 1.6851329803466797
outputs_pos.loss : 1.2656042575836182
Epoch 00037: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.343528151512146
outputs_pos.loss : 1.7358372211456299
outputs_pos.loss : 1.5187069177627563
outputs_pos.loss : 1.8816777467727661
outputs_pos.loss : 1.3886091709136963
outputs_pos.loss : 1.494578242301941
outputs_pos.loss : 1.324061632156372
outputs_pos.loss : 1.2859588861465454
Epoch 00038: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.847381830215454
outputs_pos.loss : 1.284440040588379
outputs_pos.loss : 1.3697975873947144
outputs_pos.loss : 1.3376611471176147
outputs_pos.loss : 2.0688209533691406
outputs_pos.loss : 1.508109211921692
outputs_pos.loss : 1.277294397354126
outputs_pos.loss : 1.301920771598816
Epoch 00039: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 0.9477628469467163
outputs_pos.loss : 1.0820690393447876
outputs_pos.loss : 1.6003479957580566
outputs_pos.loss : 1.2034549713134766
outputs_pos.loss : 1.1196643114089966
outputs_pos.loss : 1.0960853099822998
outputs_pos.loss : 1.5439584255218506
outputs_pos.loss : 1.432220220565796
Epoch 00040: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.6343460083007812
outputs_pos.loss : 1.3017956018447876
outputs_pos.loss : 2.0946240425109863
outputs_pos.loss : 1.127807855606079
outputs_pos.loss : 0.8234712481498718
outputs_pos.loss : 1.0661462545394897
outputs_pos.loss : 1.7328722476959229
outputs_pos.loss : 1.2463910579681396
Epoch 00041: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.2193634510040283
outputs_pos.loss : 1.5100966691970825
outputs_pos.loss : 1.2382746934890747
outputs_pos.loss : 1.329012393951416
outputs_pos.loss : 1.1214566230773926
outputs_pos.loss : 1.415237545967102
outputs_pos.loss : 1.534960389137268
outputs_pos.loss : 1.289894938468933
Epoch 00042: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.390432596206665
outputs_pos.loss : 1.3406219482421875
outputs_pos.loss : 1.5103007555007935
outputs_pos.loss : 1.1330112218856812
outputs_pos.loss : 1.5745534896850586
outputs_pos.loss : 1.3945951461791992
outputs_pos.loss : 1.3232189416885376
outputs_pos.loss : 1.6150630712509155
Epoch 00043: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.525664210319519
outputs_pos.loss : 2.234457492828369
outputs_pos.loss : 1.4418840408325195
outputs_pos.loss : 1.2107309103012085
outputs_pos.loss : 1.874524474143982
outputs_pos.loss : 1.8287297487258911
outputs_pos.loss : 1.3226587772369385
outputs_pos.loss : 1.636839747428894
Epoch 00044: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.3414889574050903
outputs_pos.loss : 1.4101283550262451
outputs_pos.loss : 1.4634745121002197
outputs_pos.loss : 1.4403047561645508
outputs_pos.loss : 1.3066025972366333
outputs_pos.loss : 1.9008584022521973
outputs_pos.loss : 2.149522304534912
outputs_pos.loss : 1.632572054862976
Epoch 00045: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.4287062883377075
outputs_pos.loss : 1.4360566139221191
outputs_pos.loss : 1.0638668537139893
outputs_pos.loss : 1.6164329051971436
outputs_pos.loss : 2.0095975399017334
outputs_pos.loss : 1.370951533317566
outputs_pos.loss : 1.3828684091567993
outputs_pos.loss : 1.3625366687774658
Epoch 00046: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.7148264646530151
outputs_pos.loss : 0.9544719457626343
outputs_pos.loss : 1.025471568107605
outputs_pos.loss : 1.1404078006744385
outputs_pos.loss : 1.4575934410095215
outputs_pos.loss : 0.9939963817596436
outputs_pos.loss : 1.6029289960861206
outputs_pos.loss : 1.8189226388931274
Epoch 00047: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.3242483139038086
outputs_pos.loss : 1.5107979774475098
outputs_pos.loss : 1.160348892211914
outputs_pos.loss : 1.1860495805740356
outputs_pos.loss : 1.361711025238037
outputs_pos.loss : 1.5705020427703857
outputs_pos.loss : 1.618798017501831
outputs_pos.loss : 1.2034590244293213
Epoch 00048: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.4231691360473633
outputs_pos.loss : 1.3307794332504272
outputs_pos.loss : 1.0934793949127197
outputs_pos.loss : 1.4687004089355469
outputs_pos.loss : 1.553576946258545
outputs_pos.loss : 1.3796420097351074
outputs_pos.loss : 1.9186887741088867
outputs_pos.loss : 1.6346091032028198
Epoch 00049: adjusting learning rate of group 0 to 2.4999e-06.
outputs_pos.loss : 1.8969343900680542
outputs_pos.loss : 1.4484121799468994
outputs_pos.loss : 1.7293198108673096
outputs_pos.loss : 1.1455906629562378
outputs_pos.loss : 1.3084301948547363
outputs_pos.loss : 1.0515285730361938
outputs_pos.loss : 1.7595980167388916
outputs_pos.loss : 1.2598990201950073
Epoch 00050: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.3368966579437256
outputs_pos.loss : 1.2463078498840332
outputs_pos.loss : 1.50251042842865
outputs_pos.loss : 1.0899497270584106
outputs_pos.loss : 1.2380610704421997
outputs_pos.loss : 0.9729924201965332
outputs_pos.loss : 1.469454050064087
outputs_pos.loss : 1.3618961572647095
Epoch 00051: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.3674653768539429
outputs_pos.loss : 1.1547490358352661
outputs_pos.loss : 1.5552425384521484
outputs_pos.loss : 1.675211787223816
outputs_pos.loss : 1.5494306087493896
outputs_pos.loss : 1.546113133430481
outputs_pos.loss : 1.7811005115509033
outputs_pos.loss : 1.1588435173034668
Epoch 00052: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.0197300910949707
outputs_pos.loss : 1.0059884786605835
outputs_pos.loss : 1.491441249847412
outputs_pos.loss : 1.5601385831832886
outputs_pos.loss : 1.1486318111419678
outputs_pos.loss : 1.5629138946533203
outputs_pos.loss : 1.450484037399292
outputs_pos.loss : 0.9662796854972839
Epoch 00053: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.1146093606948853
outputs_pos.loss : 1.4347327947616577
outputs_pos.loss : 1.5694396495819092
outputs_pos.loss : 1.4284523725509644
outputs_pos.loss : 1.5313488245010376
outputs_pos.loss : 1.469252109527588
outputs_pos.loss : 1.3609018325805664
outputs_pos.loss : 1.3942234516143799
Epoch 00054: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.5132263898849487
outputs_pos.loss : 0.9893943071365356
outputs_pos.loss : 1.4303715229034424
outputs_pos.loss : 1.1315582990646362
outputs_pos.loss : 1.0322233438491821
outputs_pos.loss : 1.4052317142486572
outputs_pos.loss : 1.4281264543533325
outputs_pos.loss : 1.4829784631729126
Epoch 00055: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.4521642923355103
outputs_pos.loss : 1.187617540359497
outputs_pos.loss : 1.253603219985962
outputs_pos.loss : 1.3423447608947754
outputs_pos.loss : 1.1811991930007935
outputs_pos.loss : 1.487197756767273
outputs_pos.loss : 1.1407307386398315
outputs_pos.loss : 1.6629918813705444
Epoch 00056: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 2.375426769256592
outputs_pos.loss : 1.6495999097824097
outputs_pos.loss : 1.879668116569519
outputs_pos.loss : 2.050424098968506
outputs_pos.loss : 1.7366282939910889
outputs_pos.loss : 1.2315385341644287
outputs_pos.loss : 1.1473801136016846
outputs_pos.loss : 1.9232090711593628
Epoch 00057: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.2113842964172363
outputs_pos.loss : 1.3226711750030518
outputs_pos.loss : 0.7219711542129517
outputs_pos.loss : 1.3745681047439575
outputs_pos.loss : 1.3623806238174438
outputs_pos.loss : 1.4412167072296143
outputs_pos.loss : 1.1665663719177246
outputs_pos.loss : 1.4183528423309326
Epoch 00058: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.8665038347244263
outputs_pos.loss : 1.3791041374206543
outputs_pos.loss : 1.105911374092102
outputs_pos.loss : 1.2319786548614502
outputs_pos.loss : 1.39936363697052
outputs_pos.loss : 1.4242254495620728
outputs_pos.loss : 1.357263445854187
outputs_pos.loss : 1.4229505062103271
Epoch 00059: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.7702120542526245
outputs_pos.loss : 1.5770570039749146
outputs_pos.loss : 1.6183428764343262
outputs_pos.loss : 1.7198396921157837
outputs_pos.loss : 1.026256799697876
outputs_pos.loss : 1.7900351285934448
outputs_pos.loss : 1.464231014251709
outputs_pos.loss : 1.0381536483764648
Epoch 00060: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.4209977388381958
outputs_pos.loss : 1.1145280599594116
outputs_pos.loss : 1.299115777015686
outputs_pos.loss : 1.444058895111084
outputs_pos.loss : 1.6887224912643433
outputs_pos.loss : 0.9910532236099243
outputs_pos.loss : 1.4518773555755615
outputs_pos.loss : 1.4285749197006226
Epoch 00061: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.3911412954330444
outputs_pos.loss : 1.4220614433288574
outputs_pos.loss : 1.1719950437545776
outputs_pos.loss : 1.3159282207489014
outputs_pos.loss : 1.2087514400482178
outputs_pos.loss : 1.4792370796203613
outputs_pos.loss : 1.7247614860534668
outputs_pos.loss : 1.2430758476257324
Epoch 00062: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 0.9191030859947205
outputs_pos.loss : 1.3868043422698975
outputs_pos.loss : 1.3201863765716553
outputs_pos.loss : 1.4167461395263672
outputs_pos.loss : 1.692291021347046
outputs_pos.loss : 1.2686634063720703
outputs_pos.loss : 2.078871488571167
outputs_pos.loss : 1.8163294792175293
outputs_pos.loss : 1.3985716104507446
outputs_pos.loss : 1.4661214351654053
outputs_pos.loss : 1.50672447681427
outputs_pos.loss : 1.454900860786438
outputs_pos.loss : 1.2648555040359497
outputs_pos.loss : 1.2580870389938354
outputs_pos.loss : 1.171067476272583
outputs_pos.loss : 1.3776863813400269
Epoch 00063: adjusting learning rate of group 0 to 2.4998e-06.
outputs_pos.loss : 1.7925989627838135
outputs_pos.loss : 0.9699417352676392
outputs_pos.loss : 1.183016300201416
outputs_pos.loss : 1.5346784591674805
outputs_pos.loss : 1.2976024150848389
outputs_pos.loss : 0.8996967673301697
outputs_pos.loss : 1.5380165576934814
outputs_pos.loss : 1.6593047380447388
Epoch 00064: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.3416997194290161
outputs_pos.loss : 1.3661929368972778
outputs_pos.loss : 2.0549099445343018
outputs_pos.loss : 1.1456927061080933
outputs_pos.loss : 1.4180779457092285
outputs_pos.loss : 1.2525876760482788
outputs_pos.loss : 1.2387950420379639
outputs_pos.loss : 1.550075888633728
Epoch 00065: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 0.920718252658844
outputs_pos.loss : 1.629665732383728
outputs_pos.loss : 1.2527830600738525
outputs_pos.loss : 1.1181148290634155
outputs_pos.loss : 1.449798345565796
outputs_pos.loss : 1.1592230796813965
outputs_pos.loss : 1.2531332969665527
outputs_pos.loss : 1.6453156471252441
Epoch 00066: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.357097864151001
outputs_pos.loss : 1.1150957345962524
outputs_pos.loss : 1.1704356670379639
outputs_pos.loss : 1.6037962436676025
outputs_pos.loss : 1.2619965076446533
outputs_pos.loss : 2.1181435585021973
outputs_pos.loss : 1.58060622215271
outputs_pos.loss : 1.6819562911987305
Epoch 00067: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.687680721282959
outputs_pos.loss : 1.2281756401062012
outputs_pos.loss : 1.149082899093628
outputs_pos.loss : 1.7222377061843872
outputs_pos.loss : 1.140156626701355
outputs_pos.loss : 1.3669863939285278
outputs_pos.loss : 1.3304104804992676
outputs_pos.loss : 1.2908506393432617
Epoch 00068: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.3250222206115723
outputs_pos.loss : 1.0967990159988403
outputs_pos.loss : 1.54561185836792
outputs_pos.loss : 1.4391136169433594
outputs_pos.loss : 1.6879574060440063
outputs_pos.loss : 2.261794090270996
outputs_pos.loss : 1.423280954360962
outputs_pos.loss : 1.3646901845932007
Epoch 00069: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.2687932252883911
outputs_pos.loss : 1.1695454120635986
outputs_pos.loss : 1.435902714729309
outputs_pos.loss : 1.6639304161071777
outputs_pos.loss : 1.3654545545578003
outputs_pos.loss : 1.0285805463790894
outputs_pos.loss : 1.7688490152359009
outputs_pos.loss : 1.577728509902954
Epoch 00070: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.4567385911941528
outputs_pos.loss : 1.0942085981369019
outputs_pos.loss : 0.9861001968383789
outputs_pos.loss : 1.5919480323791504
outputs_pos.loss : 1.502098798751831
outputs_pos.loss : 1.3955684900283813
outputs_pos.loss : 1.631367802619934
outputs_pos.loss : 1.2169208526611328
Epoch 00071: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.8491255044937134
outputs_pos.loss : 1.0668001174926758
outputs_pos.loss : 1.2412965297698975
outputs_pos.loss : 1.3463428020477295
outputs_pos.loss : 1.2021404504776
outputs_pos.loss : 1.1121490001678467
outputs_pos.loss : 1.0433310270309448
outputs_pos.loss : 1.066011905670166
Epoch 00072: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.2518779039382935
outputs_pos.loss : 1.7093610763549805
outputs_pos.loss : 1.1848787069320679
outputs_pos.loss : 1.035622477531433
outputs_pos.loss : 1.6118942499160767
outputs_pos.loss : 1.2853350639343262
outputs_pos.loss : 1.6980804204940796
outputs_pos.loss : 2.0897135734558105
Epoch 00073: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.0768494606018066
outputs_pos.loss : 1.2383335828781128
outputs_pos.loss : 1.293816328048706
outputs_pos.loss : 2.022608757019043
outputs_pos.loss : 1.41881263256073
outputs_pos.loss : 1.0622026920318604
outputs_pos.loss : 1.104562520980835
outputs_pos.loss : 1.5626345872879028
Epoch 00074: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.1882599592208862
outputs_pos.loss : 1.2373868227005005
outputs_pos.loss : 0.9100636839866638
outputs_pos.loss : 1.1376755237579346
outputs_pos.loss : 1.5143163204193115
outputs_pos.loss : 1.767364740371704
outputs_pos.loss : 1.123546838760376
outputs_pos.loss : 0.9787724018096924
Epoch 00075: adjusting learning rate of group 0 to 2.4997e-06.
outputs_pos.loss : 1.3725703954696655
outputs_pos.loss : 1.695609450340271
outputs_pos.loss : 1.6485109329223633
outputs_pos.loss : 1.2388883829116821
outputs_pos.loss : 1.2077149152755737
outputs_pos.loss : 0.9135510325431824
outputs_pos.loss : 1.066656231880188
outputs_pos.loss : 0.9784837961196899
Epoch 00076: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 0.8833562731742859
outputs_pos.loss : 1.5754504203796387
outputs_pos.loss : 1.2243726253509521
outputs_pos.loss : 1.338053822517395
outputs_pos.loss : 1.5268107652664185
outputs_pos.loss : 1.432910442352295
outputs_pos.loss : 1.1881415843963623
outputs_pos.loss : 1.9079569578170776
Epoch 00077: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 1.351361632347107
outputs_pos.loss : 1.7506037950515747
outputs_pos.loss : 1.28939950466156
outputs_pos.loss : 1.0016413927078247
outputs_pos.loss : 1.3882147073745728
outputs_pos.loss : 1.2698631286621094
outputs_pos.loss : 1.1830352544784546
outputs_pos.loss : 1.4115960597991943
Epoch 00078: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 1.4228981733322144
outputs_pos.loss : 1.3042500019073486
outputs_pos.loss : 1.2887438535690308
outputs_pos.loss : 1.5384056568145752
outputs_pos.loss : 1.4509234428405762
outputs_pos.loss : 1.2979103326797485
outputs_pos.loss : 1.6454256772994995
outputs_pos.loss : 1.3170907497406006
Epoch 00079: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 1.0999069213867188
outputs_pos.loss : 1.2498695850372314
outputs_pos.loss : 1.367025375366211
outputs_pos.loss : 1.357401728630066
outputs_pos.loss : 2.037100076675415
outputs_pos.loss : 1.9303165674209595
outputs_pos.loss : 1.6170250177383423
outputs_pos.loss : 1.4902034997940063
Epoch 00080: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 1.3605046272277832
outputs_pos.loss : 1.2403451204299927
outputs_pos.loss : 1.3343722820281982
outputs_pos.loss : 1.04825758934021
outputs_pos.loss : 1.5648932456970215
outputs_pos.loss : 1.2047948837280273
outputs_pos.loss : 1.433247685432434
outputs_pos.loss : 1.3838286399841309
Epoch 00081: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 1.4872229099273682
outputs_pos.loss : 1.1885673999786377
outputs_pos.loss : 1.2348800897598267
outputs_pos.loss : 1.0263805389404297
outputs_pos.loss : 1.5025222301483154
outputs_pos.loss : 1.6883065700531006
outputs_pos.loss : 1.6253689527511597
outputs_pos.loss : 1.5052250623703003
Epoch 00082: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 0.9498515725135803
outputs_pos.loss : 1.3271772861480713
outputs_pos.loss : 1.7127225399017334
outputs_pos.loss : 1.4688754081726074
outputs_pos.loss : 0.9817280173301697
outputs_pos.loss : 1.642019271850586
outputs_pos.loss : 0.9629802703857422
outputs_pos.loss : 2.109606981277466
Epoch 00083: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 1.3502957820892334
outputs_pos.loss : 1.466417670249939
outputs_pos.loss : 1.3401432037353516
outputs_pos.loss : 1.7203761339187622
outputs_pos.loss : 1.3085594177246094
outputs_pos.loss : 1.2848318815231323
outputs_pos.loss : 1.7621172666549683
outputs_pos.loss : 1.6976476907730103
Epoch 00084: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 1.0856406688690186
outputs_pos.loss : 1.3523204326629639
outputs_pos.loss : 1.1905646324157715
outputs_pos.loss : 1.360062837600708
outputs_pos.loss : 1.295610785484314
outputs_pos.loss : 1.2528319358825684
outputs_pos.loss : 1.308092713356018
outputs_pos.loss : 1.0994651317596436
Epoch 00085: adjusting learning rate of group 0 to 2.4996e-06.
outputs_pos.loss : 1.5344351530075073
outputs_pos.loss : 1.1733388900756836
outputs_pos.loss : 1.2692736387252808
outputs_pos.loss : 1.007603406906128
outputs_pos.loss : 1.1386252641677856
outputs_pos.loss : 1.1441370248794556
outputs_pos.loss : 1.9749788045883179
outputs_pos.loss : 1.3867032527923584
Epoch 00086: adjusting learning rate of group 0 to 2.4995e-06.
outputs_pos.loss : 0.9843490719795227
outputs_pos.loss : 1.314408779144287
outputs_pos.loss : 1.2430025339126587
outputs_pos.loss : 1.5046979188919067
outputs_pos.loss : 1.9395267963409424
outputs_pos.loss : 1.0134351253509521
outputs_pos.loss : 1.2632999420166016
outputs_pos.loss : 1.4223617315292358
Epoch 00087: adjusting learning rate of group 0 to 2.4995e-06.
outputs_pos.loss : 1.240006685256958
outputs_pos.loss : 1.317999243736267
outputs_pos.loss : 1.5422484874725342
outputs_pos.loss : 1.541821002960205
outputs_pos.loss : 1.3522121906280518
outputs_pos.loss : 1.2263137102127075
outputs_pos.loss : 1.2645084857940674
outputs_pos.loss : 1.570016860961914
Epoch 00088: adjusting learning rate of group 0 to 2.4995e-06.
outputs_pos.loss : 1.08145010471344
outputs_pos.loss : 1.284531831741333
outputs_pos.loss : 1.124530553817749
outputs_pos.loss : 1.1488908529281616
outputs_pos.loss : 1.3676077127456665
outputs_pos.loss : 1.0661832094192505
outputs_pos.loss : 1.5538729429244995
outputs_pos.loss : 1.0144201517105103
Epoch 00089: adjusting learning rate of group 0 to 2.4995e-06.
outputs_pos.loss : 0.781022846698761
outputs_pos.loss : 1.5396296977996826
outputs_pos.loss : 1.1628446578979492
outputs_pos.loss : 1.0835657119750977
outputs_pos.loss : 1.3840757608413696
outputs_pos.loss : 1.4688085317611694
outputs_pos.loss : 1.73771071434021
outputs_pos.loss : 1.1279596090316772
Epoch 00090: adjusting learning rate of group 0 to 2.4995e-06.
outputs_pos.loss : 1.3223493099212646
outputs_pos.loss : 0.8611859083175659
outputs_pos.loss : 1.0875649452209473
outputs_pos.loss : 1.5402464866638184
outputs_pos.loss : 1.4212411642074585
outputs_pos.loss : 1.3599677085876465
outputs_pos.loss : 1.6664632558822632
outputs_pos.loss : 1.3233548402786255
Epoch 00091: adjusting learning rate of group 0 to 2.4995e-06.
outputs_pos.loss : 0.9014077186584473
outputs_pos.loss : 1.3203898668289185
outputs_pos.loss : 1.7768064737319946
outputs_pos.loss : 1.5942555665969849
outputs_pos.loss : 1.2706190347671509
outputs_pos.loss : 1.2171275615692139
outputs_pos.loss : 1.6185909509658813
outputs_pos.loss : 1.6269068717956543
Epoch 00092: adjusting learning rate of group 0 to 2.4995e-06.
outputs_pos.loss : 0.907333493232727
outputs_pos.loss : 1.1262260675430298
outputs_pos.loss : 1.2504172325134277
outputs_pos.loss : 1.6932052373886108
outputs_pos.loss : 1.412015676498413
outputs_pos.loss : 1.315245270729065
outputs_pos.loss : 0.8886715173721313
outputs_pos.loss : 1.5558844804763794
Epoch 00093: adjusting learning rate of group 0 to 2.4995e-06.
outputs_pos.loss : 1.218207836151123
outputs_pos.loss : 1.4273831844329834
outputs_pos.loss : 1.2644782066345215
outputs_pos.loss : 1.0203744173049927
outputs_pos.loss : 1.9867793321609497
outputs_pos.loss : 1.1849982738494873
outputs_pos.loss : 1.0828005075454712
outputs_pos.loss : 1.3857883214950562
Epoch 00094: adjusting learning rate of group 0 to 2.4995e-06.
outputs_pos.loss : 1.9536100625991821
outputs_pos.loss : 0.824945330619812
outputs_pos.loss : 1.3887906074523926
outputs_pos.loss : 1.145221471786499
outputs_pos.loss : 1.4669957160949707
outputs_pos.loss : 0.9524802565574646
outputs_pos.loss : 1.4996492862701416
outputs_pos.loss : 1.2678935527801514
Epoch 00095: adjusting learning rate of group 0 to 2.4994e-06.
outputs_pos.loss : 1.3542759418487549
outputs_pos.loss : 1.1227201223373413
outputs_pos.loss : 1.1628578901290894
outputs_pos.loss : 1.3623013496398926
outputs_pos.loss : 1.5728787183761597
outputs_pos.loss : 1.573458194732666
outputs_pos.loss : 0.8921790719032288
outputs_pos.loss : 1.1876022815704346
Epoch 00096: adjusting learning rate of group 0 to 2.4994e-06.
outputs_pos.loss : 0.8959395885467529
outputs_pos.loss : 1.5653272867202759
outputs_pos.loss : 1.0140964984893799
outputs_pos.loss : 1.2048181295394897
outputs_pos.loss : 1.4742850065231323
outputs_pos.loss : 1.2965160608291626
outputs_pos.loss : 1.4140219688415527
outputs_pos.loss : 1.175959825515747
Epoch 00097: adjusting learning rate of group 0 to 2.4994e-06.
outputs_pos.loss : 1.2866237163543701
outputs_pos.loss : 1.195534110069275
outputs_pos.loss : 1.3762696981430054
outputs_pos.loss : 1.3376868963241577
outputs_pos.loss : 1.1851894855499268
outputs_pos.loss : 1.1131327152252197
outputs_pos.loss : 1.3887856006622314
outputs_pos.loss : 1.488632321357727
Epoch 00098: adjusting learning rate of group 0 to 2.4994e-06.
outputs_pos.loss : 1.5154653787612915
outputs_pos.loss : 1.593506097793579
outputs_pos.loss : 1.3899173736572266
outputs_pos.loss : 1.0285027027130127
outputs_pos.loss : 1.5898345708847046
outputs_pos.loss : 1.1383137702941895
outputs_pos.loss : 1.204256296157837
outputs_pos.loss : 1.2054635286331177
Epoch 00099: adjusting learning rate of group 0 to 2.4994e-06.
outputs_pos.loss : 1.5018397569656372
outputs_pos.loss : 1.4538156986236572
outputs_pos.loss : 1.1606390476226807
outputs_pos.loss : 1.3174593448638916
outputs_pos.loss : 1.2880452871322632
outputs_pos.loss : 1.4236977100372314
outputs_pos.loss : 1.2121959924697876
outputs_pos.loss : 1.3973931074142456
Epoch 00100: adjusting learning rate of group 0 to 2.4994e-06.
outputs_pos.loss : 1.1876014471054077
outputs_pos.loss : 1.5791043043136597
outputs_pos.loss : 1.758811354637146
outputs_pos.loss : 1.2595075368881226
outputs_pos.loss : 1.1110731363296509
outputs_pos.loss : 1.4574296474456787
outputs_pos.loss : 1.35829496383667
outputs_pos.loss : 1.1737332344055176
Epoch 00101: adjusting learning rate of group 0 to 2.4994e-06.
outputs_pos.loss : 1.4167604446411133
outputs_pos.loss : 1.2303086519241333
outputs_pos.loss : 1.213589072227478
outputs_pos.loss : 1.5554940700531006
outputs_pos.loss : 1.2451512813568115
outputs_pos.loss : 1.323716402053833
outputs_pos.loss : 1.6392313241958618
outputs_pos.loss : 1.166633129119873
Epoch 00102: adjusting learning rate of group 0 to 2.4994e-06.
outputs_pos.loss : 1.328420877456665
outputs_pos.loss : 1.089454174041748
outputs_pos.loss : 0.9720945358276367
outputs_pos.loss : 0.981313169002533
outputs_pos.loss : 1.2997218370437622
outputs_pos.loss : 0.9430260062217712
outputs_pos.loss : 1.3856903314590454
outputs_pos.loss : 1.7555890083312988
Epoch 00103: adjusting learning rate of group 0 to 2.4993e-06.
outputs_pos.loss : 2.3578989505767822
outputs_pos.loss : 1.3023359775543213
outputs_pos.loss : 1.24034583568573
outputs_pos.loss : 1.4156177043914795
outputs_pos.loss : 1.6824171543121338
outputs_pos.loss : 1.4889953136444092
outputs_pos.loss : 1.050692081451416
outputs_pos.loss : 1.5592597723007202
Epoch 00104: adjusting learning rate of group 0 to 2.4993e-06.
outputs_pos.loss : 1.8354045152664185
outputs_pos.loss : 1.4522169828414917
outputs_pos.loss : 1.2511398792266846
outputs_pos.loss : 1.131127953529358
outputs_pos.loss : 1.0487596988677979
outputs_pos.loss : 0.8773086071014404
outputs_pos.loss : 1.3185992240905762
outputs_pos.loss : 1.9877289533615112
Epoch 00105: adjusting learning rate of group 0 to 2.4993e-06.
outputs_pos.loss : 1.1854666471481323
outputs_pos.loss : 1.2665659189224243
outputs_pos.loss : 1.8578969240188599
outputs_pos.loss : 1.7367498874664307
outputs_pos.loss : 1.3126288652420044
outputs_pos.loss : 1.3632533550262451
outputs_pos.loss : 1.6889466047286987
outputs_pos.loss : 1.9260797500610352
Epoch 00106: adjusting learning rate of group 0 to 2.4993e-06.
outputs_pos.loss : 1.4472792148590088
outputs_pos.loss : 1.2228877544403076
outputs_pos.loss : 1.6873594522476196
outputs_pos.loss : 1.0384013652801514
outputs_pos.loss : 1.2170852422714233
outputs_pos.loss : 1.5112158060073853
outputs_pos.loss : 1.177773118019104
outputs_pos.loss : 1.4934319257736206
Epoch 00107: adjusting learning rate of group 0 to 2.4993e-06.
outputs_pos.loss : 1.593780279159546
outputs_pos.loss : 1.1586077213287354
outputs_pos.loss : 2.031043291091919
outputs_pos.loss : 1.46737539768219
outputs_pos.loss : 1.8283250331878662
outputs_pos.loss : 1.363145351409912
outputs_pos.loss : 1.0122419595718384
outputs_pos.loss : 1.0512869358062744
Epoch 00108: adjusting learning rate of group 0 to 2.4993e-06.
outputs_pos.loss : 2.209785223007202
outputs_pos.loss : 1.2383493185043335
outputs_pos.loss : 2.106071710586548
outputs_pos.loss : 1.2731451988220215
outputs_pos.loss : 1.0809073448181152
outputs_pos.loss : 1.1824893951416016
outputs_pos.loss : 1.069641351699829
outputs_pos.loss : 1.4378517866134644
Epoch 00109: adjusting learning rate of group 0 to 2.4993e-06.
outputs_pos.loss : 1.266219973564148
outputs_pos.loss : 0.8459419012069702
outputs_pos.loss : 1.0109071731567383
outputs_pos.loss : 1.1871654987335205
outputs_pos.loss : 1.5040950775146484
outputs_pos.loss : 0.7974268794059753
outputs_pos.loss : 1.670979380607605
outputs_pos.loss : 1.297808289527893
Epoch 00110: adjusting learning rate of group 0 to 2.4993e-06.
outputs_pos.loss : 2.027555465698242
outputs_pos.loss : 1.4616283178329468
outputs_pos.loss : 1.622475028038025
outputs_pos.loss : 1.047160029411316
outputs_pos.loss : 1.534997820854187
outputs_pos.loss : 1.100775122642517
outputs_pos.loss : 1.2041183710098267
outputs_pos.loss : 1.576415777206421
Epoch 00111: adjusting learning rate of group 0 to 2.4992e-06.
outputs_pos.loss : 1.4960849285125732
outputs_pos.loss : 1.2465583086013794
outputs_pos.loss : 0.9990299940109253
outputs_pos.loss : 1.3404675722122192
outputs_pos.loss : 1.1117762327194214
outputs_pos.loss : 0.7457388639450073
outputs_pos.loss : 1.3129438161849976
outputs_pos.loss : 1.7931854724884033
Epoch 00112: adjusting learning rate of group 0 to 2.4992e-06.
outputs_pos.loss : 1.563680648803711
outputs_pos.loss : 1.4496009349822998
outputs_pos.loss : 1.2257784605026245
outputs_pos.loss : 2.1276767253875732
outputs_pos.loss : 1.8550667762756348
outputs_pos.loss : 1.4025025367736816
outputs_pos.loss : 1.0952318906784058
outputs_pos.loss : 1.2322932481765747
Epoch 00113: adjusting learning rate of group 0 to 2.4992e-06.
outputs_pos.loss : 1.3171703815460205
outputs_pos.loss : 1.8897536993026733
outputs_pos.loss : 1.1874983310699463
outputs_pos.loss : 1.3735758066177368
outputs_pos.loss : 1.0842703580856323
outputs_pos.loss : 1.5656328201293945
outputs_pos.loss : 1.7012736797332764
outputs_pos.loss : 1.1370773315429688
Epoch 00114: adjusting learning rate of group 0 to 2.4992e-06.
outputs_pos.loss : 1.47502601146698
outputs_pos.loss : 1.4100075960159302
outputs_pos.loss : 1.1603288650512695
outputs_pos.loss : 1.1195650100708008
outputs_pos.loss : 0.9754490852355957
outputs_pos.loss : 1.1958937644958496
outputs_pos.loss : 1.6753901243209839
outputs_pos.loss : 1.2506084442138672
Epoch 00115: adjusting learning rate of group 0 to 2.4992e-06.
outputs_pos.loss : 1.2978113889694214
outputs_pos.loss : 0.9564314484596252
outputs_pos.loss : 1.200099229812622
outputs_pos.loss : 1.1515408754348755
outputs_pos.loss : 1.0482330322265625
outputs_pos.loss : 1.1329318284988403
outputs_pos.loss : 1.5032052993774414
outputs_pos.loss : 1.5377525091171265
Epoch 00116: adjusting learning rate of group 0 to 2.4992e-06.
outputs_pos.loss : 1.0259666442871094
outputs_pos.loss : 1.765425205230713
outputs_pos.loss : 0.93502277135849
outputs_pos.loss : 1.532317876815796
outputs_pos.loss : 1.0570147037506104
outputs_pos.loss : 1.4550282955169678
outputs_pos.loss : 1.1506242752075195
outputs_pos.loss : 1.4741369485855103
Epoch 00117: adjusting learning rate of group 0 to 2.4992e-06.
outputs_pos.loss : 2.263458251953125
outputs_pos.loss : 1.1711355447769165
outputs_pos.loss : 1.41017484664917
outputs_pos.loss : 1.1205185651779175
outputs_pos.loss : 1.0986605882644653
outputs_pos.loss : 1.3337361812591553
outputs_pos.loss : 1.4190011024475098
outputs_pos.loss : 1.3399534225463867
Epoch 00118: adjusting learning rate of group 0 to 2.4991e-06.
outputs_pos.loss : 1.2366540431976318
outputs_pos.loss : 1.7181602716445923
outputs_pos.loss : 0.9429879784584045
outputs_pos.loss : 1.9449812173843384
outputs_pos.loss : 1.3038017749786377
outputs_pos.loss : 1.2049930095672607
outputs_pos.loss : 1.4686377048492432
outputs_pos.loss : 1.730159044265747
Epoch 00119: adjusting learning rate of group 0 to 2.4991e-06.
outputs_pos.loss : 1.2802499532699585
outputs_pos.loss : 1.8472846746444702
outputs_pos.loss : 2.306875228881836
outputs_pos.loss : 1.1968733072280884
outputs_pos.loss : 1.8583821058273315
outputs_pos.loss : 1.1469076871871948
outputs_pos.loss : 1.102945327758789
outputs_pos.loss : 1.29642915725708
Epoch 00120: adjusting learning rate of group 0 to 2.4991e-06.
outputs_pos.loss : 1.1218498945236206
outputs_pos.loss : 1.1632493734359741
outputs_pos.loss : 2.000439167022705
outputs_pos.loss : 2.268740653991699
outputs_pos.loss : 1.1505837440490723
outputs_pos.loss : 1.594942331314087
outputs_pos.loss : 0.9807197451591492
outputs_pos.loss : 1.161137580871582
Epoch 00121: adjusting learning rate of group 0 to 2.4991e-06.
outputs_pos.loss : 1.4870460033416748
outputs_pos.loss : 1.0669562816619873
outputs_pos.loss : 1.1917295455932617
outputs_pos.loss : 1.2904785871505737
outputs_pos.loss : 1.2569342851638794
outputs_pos.loss : 1.4075040817260742
outputs_pos.loss : 1.1414438486099243
outputs_pos.loss : 2.2152397632598877
Epoch 00122: adjusting learning rate of group 0 to 2.4991e-06.
outputs_pos.loss : 1.6821352243423462
outputs_pos.loss : 1.9216047525405884
outputs_pos.loss : 1.7735710144042969
outputs_pos.loss : 1.343004584312439
outputs_pos.loss : 1.1342833042144775
outputs_pos.loss : 1.2767210006713867
outputs_pos.loss : 1.3341768980026245
outputs_pos.loss : 1.3049057722091675
Epoch 00123: adjusting learning rate of group 0 to 2.4991e-06.
outputs_pos.loss : 1.5136888027191162
outputs_pos.loss : 2.047791004180908
outputs_pos.loss : 1.4521832466125488
outputs_pos.loss : 1.4084727764129639
outputs_pos.loss : 1.8607288599014282
outputs_pos.loss : 0.8770186305046082
outputs_pos.loss : 1.2228419780731201
outputs_pos.loss : 1.3135619163513184
Epoch 00124: adjusting learning rate of group 0 to 2.4991e-06.
outputs_pos.loss : 0.9783108234405518
outputs_pos.loss : 1.1569056510925293
outputs_pos.loss : 1.0862993001937866
outputs_pos.loss : 1.04336678981781
outputs_pos.loss : 1.2218352556228638
outputs_pos.loss : 1.498938798904419
outputs_pos.loss : 1.1897039413452148
outputs_pos.loss : 1.5200854539871216
Epoch 00125: adjusting learning rate of group 0 to 2.4990e-06.
outputs_pos.loss : 1.4220545291900635
outputs_pos.loss : 1.5159510374069214
outputs_pos.loss : 1.289414882659912
outputs_pos.loss : 1.6905947923660278
outputs_pos.loss : 2.1530508995056152
outputs_pos.loss : 0.9876242876052856
outputs_pos.loss : 1.0802090167999268
outputs_pos.loss : 1.504204273223877
Epoch 00126: adjusting learning rate of group 0 to 2.4990e-06.
outputs_pos.loss : 1.047095775604248
outputs_pos.loss : 1.1041206121444702
outputs_pos.loss : 1.5775821208953857
outputs_pos.loss : 1.4780374765396118
outputs_pos.loss : 1.1938822269439697
outputs_pos.loss : 1.7272660732269287
outputs_pos.loss : 1.4960402250289917
outputs_pos.loss : 1.2692686319351196
Epoch 00127: adjusting learning rate of group 0 to 2.4990e-06.
outputs_pos.loss : 0.9437477588653564
outputs_pos.loss : 1.4303685426712036
outputs_pos.loss : 1.0188194513320923
outputs_pos.loss : 1.160271167755127
outputs_pos.loss : 1.4399163722991943
outputs_pos.loss : 1.768423080444336
outputs_pos.loss : 1.358962059020996
outputs_pos.loss : 1.5890189409255981
Epoch 00128: adjusting learning rate of group 0 to 2.4990e-06.
outputs_pos.loss : 1.0547449588775635
outputs_pos.loss : 1.3009833097457886
outputs_pos.loss : 1.8615410327911377
outputs_pos.loss : 1.261101484298706
outputs_pos.loss : 1.1757299900054932
outputs_pos.loss : 1.4033244848251343
outputs_pos.loss : 1.4569607973098755
outputs_pos.loss : 1.3632782697677612
Epoch 00129: adjusting learning rate of group 0 to 2.4990e-06.
outputs_pos.loss : 1.913346529006958
outputs_pos.loss : 1.290425181388855
outputs_pos.loss : 1.128990650177002
outputs_pos.loss : 1.4331998825073242
outputs_pos.loss : 0.9837586283683777
outputs_pos.loss : 1.2836538553237915
outputs_pos.loss : 1.1682696342468262
outputs_pos.loss : 1.1551170349121094
Epoch 00130: adjusting learning rate of group 0 to 2.4990e-06.
outputs_pos.loss : 1.101749062538147
outputs_pos.loss : 1.2836270332336426
outputs_pos.loss : 1.214875340461731
outputs_pos.loss : 1.2753809690475464
outputs_pos.loss : 1.5580554008483887
outputs_pos.loss : 1.7819222211837769
outputs_pos.loss : 1.4650614261627197
outputs_pos.loss : 1.2642114162445068
Epoch 00131: adjusting learning rate of group 0 to 2.4989e-06.
outputs_pos.loss : 1.3704993724822998
outputs_pos.loss : 1.372704029083252
outputs_pos.loss : 1.0330021381378174
outputs_pos.loss : 1.332959771156311
outputs_pos.loss : 1.3636797666549683
outputs_pos.loss : 1.6471455097198486
outputs_pos.loss : 1.7159500122070312
outputs_pos.loss : 1.2555228471755981
Epoch 00132: adjusting learning rate of group 0 to 2.4989e-06.
outputs_pos.loss : 1.6070650815963745
outputs_pos.loss : 0.9735000729560852
outputs_pos.loss : 1.2301685810089111
outputs_pos.loss : 1.2140963077545166
outputs_pos.loss : 1.3477873802185059
outputs_pos.loss : 1.075137972831726
outputs_pos.loss : 1.6682780981063843
outputs_pos.loss : 1.2531700134277344
Epoch 00133: adjusting learning rate of group 0 to 2.4989e-06.
outputs_pos.loss : 1.4759209156036377
outputs_pos.loss : 1.4288386106491089
outputs_pos.loss : 1.0675442218780518
outputs_pos.loss : 1.4911282062530518
outputs_pos.loss : 1.3561290502548218
outputs_pos.loss : 1.1044251918792725
outputs_pos.loss : 1.2269543409347534
outputs_pos.loss : 1.4596490859985352
Epoch 00134: adjusting learning rate of group 0 to 2.4989e-06.
outputs_pos.loss : 0.755946934223175
outputs_pos.loss : 1.0244519710540771
outputs_pos.loss : 1.0216838121414185
outputs_pos.loss : 1.3005491495132446
outputs_pos.loss : 1.2639997005462646
outputs_pos.loss : 1.5875011682510376
outputs_pos.loss : 1.5330686569213867
outputs_pos.loss : 1.1314841508865356
Epoch 00135: adjusting learning rate of group 0 to 2.4989e-06.
outputs_pos.loss : 1.505799412727356
outputs_pos.loss : 1.4666873216629028
outputs_pos.loss : 1.3462287187576294
outputs_pos.loss : 1.158834457397461
outputs_pos.loss : 1.2800102233886719
outputs_pos.loss : 1.3223767280578613
outputs_pos.loss : 1.1074761152267456
outputs_pos.loss : 0.9004445672035217
Epoch 00136: adjusting learning rate of group 0 to 2.4989e-06.
outputs_pos.loss : 1.8211628198623657
outputs_pos.loss : 1.413516640663147
outputs_pos.loss : 1.1968342065811157
outputs_pos.loss : 0.968562662601471
outputs_pos.loss : 1.3110239505767822
outputs_pos.loss : 1.1647478342056274
outputs_pos.loss : 1.698341727256775
outputs_pos.loss : 1.3627773523330688
Epoch 00137: adjusting learning rate of group 0 to 2.4988e-06.
outputs_pos.loss : 1.0785092115402222
outputs_pos.loss : 1.0892647504806519
outputs_pos.loss : 1.0752770900726318
outputs_pos.loss : 1.3378996849060059
outputs_pos.loss : 0.9894363880157471
outputs_pos.loss : 1.6510381698608398
outputs_pos.loss : 1.2420244216918945
outputs_pos.loss : 1.8163819313049316
Epoch 00138: adjusting learning rate of group 0 to 2.4988e-06.
outputs_pos.loss : 1.3167524337768555
outputs_pos.loss : 1.6778427362442017
outputs_pos.loss : 1.297829270362854
outputs_pos.loss : 1.356477975845337
outputs_pos.loss : 1.5207334756851196
outputs_pos.loss : 1.4346024990081787
outputs_pos.loss : 1.5245918035507202
outputs_pos.loss : 1.4771432876586914
Epoch 00139: adjusting learning rate of group 0 to 2.4988e-06.
outputs_pos.loss : 2.089818239212036
outputs_pos.loss : 1.4081244468688965
outputs_pos.loss : 1.2214057445526123
outputs_pos.loss : 1.2227669954299927
outputs_pos.loss : 1.8039904832839966
outputs_pos.loss : 1.5242706537246704
outputs_pos.loss : 1.1880199909210205
outputs_pos.loss : 1.3270825147628784
Epoch 00140: adjusting learning rate of group 0 to 2.4988e-06.
outputs_pos.loss : 1.4705835580825806
outputs_pos.loss : 0.9945396184921265
outputs_pos.loss : 1.1233538389205933
outputs_pos.loss : 0.9230794310569763
outputs_pos.loss : 1.172410011291504
outputs_pos.loss : 1.4795187711715698
outputs_pos.loss : 0.9204922318458557
outputs_pos.loss : 1.90646231174469
Epoch 00141: adjusting learning rate of group 0 to 2.4988e-06.
outputs_pos.loss : 1.2112905979156494
outputs_pos.loss : 0.8699469566345215
outputs_pos.loss : 1.3705167770385742
outputs_pos.loss : 1.4692754745483398
outputs_pos.loss : 1.5278230905532837
outputs_pos.loss : 1.3912254571914673
outputs_pos.loss : 0.9333915114402771
outputs_pos.loss : 1.0890827178955078
Epoch 00142: adjusting learning rate of group 0 to 2.4988e-06.
outputs_pos.loss : 2.258819580078125
outputs_pos.loss : 1.0130761861801147
outputs_pos.loss : 1.4670380353927612
outputs_pos.loss : 1.0624884366989136
outputs_pos.loss : 1.1213958263397217
outputs_pos.loss : 1.0708122253417969
outputs_pos.loss : 1.1994191408157349
outputs_pos.loss : 1.191285252571106
Epoch 00143: adjusting learning rate of group 0 to 2.4987e-06.
outputs_pos.loss : 1.3829468488693237
outputs_pos.loss : 1.2452867031097412
outputs_pos.loss : 0.7560191750526428
outputs_pos.loss : 1.258493423461914
outputs_pos.loss : 1.2728503942489624
outputs_pos.loss : 1.7117915153503418
outputs_pos.loss : 1.5903196334838867
outputs_pos.loss : 1.2167567014694214
Epoch 00144: adjusting learning rate of group 0 to 2.4987e-06.
outputs_pos.loss : 1.4970505237579346
outputs_pos.loss : 2.1004421710968018
outputs_pos.loss : 1.6522676944732666
outputs_pos.loss : 1.2026137113571167
outputs_pos.loss : 1.0030004978179932
outputs_pos.loss : 1.199970006942749
outputs_pos.loss : 1.4595624208450317
outputs_pos.loss : 0.6745406985282898
Epoch 00145: adjusting learning rate of group 0 to 2.4987e-06.
outputs_pos.loss : 1.4681967496871948
outputs_pos.loss : 1.3109259605407715
outputs_pos.loss : 1.4619301557540894
outputs_pos.loss : 1.297495722770691
outputs_pos.loss : 1.0114070177078247
outputs_pos.loss : 1.2210603952407837
outputs_pos.loss : 1.9711421728134155
outputs_pos.loss : 1.060958981513977
Epoch 00146: adjusting learning rate of group 0 to 2.4987e-06.
outputs_pos.loss : 1.4046480655670166
outputs_pos.loss : 0.9539898037910461
outputs_pos.loss : 1.1597696542739868
outputs_pos.loss : 0.7423242926597595
outputs_pos.loss : 1.139288306236267
outputs_pos.loss : 1.5652012825012207
outputs_pos.loss : 1.9632223844528198
outputs_pos.loss : 1.3893839120864868
Epoch 00147: adjusting learning rate of group 0 to 2.4987e-06.
outputs_pos.loss : 1.4717130661010742
outputs_pos.loss : 1.3997421264648438
outputs_pos.loss : 1.9992538690567017
outputs_pos.loss : 2.0044398307800293
outputs_pos.loss : 1.2157968282699585
outputs_pos.loss : 1.1862767934799194
outputs_pos.loss : 1.1329116821289062
outputs_pos.loss : 1.157656192779541
Epoch 00148: adjusting learning rate of group 0 to 2.4986e-06.
outputs_pos.loss : 1.277486801147461
outputs_pos.loss : 1.2277988195419312
outputs_pos.loss : 1.532239556312561
outputs_pos.loss : 1.0670955181121826
outputs_pos.loss : 0.9691559672355652
outputs_pos.loss : 1.0507872104644775
outputs_pos.loss : 1.1395041942596436
outputs_pos.loss : 1.1423754692077637
Epoch 00149: adjusting learning rate of group 0 to 2.4986e-06.
outputs_pos.loss : 1.4053683280944824
outputs_pos.loss : 1.182141661643982
outputs_pos.loss : 1.439529538154602
outputs_pos.loss : 1.3227787017822266
outputs_pos.loss : 1.2217810153961182
outputs_pos.loss : 1.06171715259552
outputs_pos.loss : 1.3091681003570557
outputs_pos.loss : 1.437058925628662
Epoch 00150: adjusting learning rate of group 0 to 2.4986e-06.
outputs_pos.loss : 1.2997307777404785
outputs_pos.loss : 1.116417407989502
outputs_pos.loss : 1.0359728336334229
outputs_pos.loss : 1.1537418365478516
outputs_pos.loss : 1.3473442792892456
outputs_pos.loss : 0.9355385303497314
outputs_pos.loss : 1.1141657829284668
outputs_pos.loss : 1.4211857318878174
Epoch 00151: adjusting learning rate of group 0 to 2.4986e-06.
outputs_pos.loss : 1.8927899599075317
outputs_pos.loss : 1.2650766372680664
outputs_pos.loss : 1.3975965976715088
outputs_pos.loss : 1.282814621925354
outputs_pos.loss : 1.4120923280715942
outputs_pos.loss : 0.7981874346733093
outputs_pos.loss : 0.765204906463623
outputs_pos.loss : 1.2782875299453735
Epoch 00152: adjusting learning rate of group 0 to 2.4986e-06.
outputs_pos.loss : 1.4744651317596436
outputs_pos.loss : 1.3523176908493042
outputs_pos.loss : 1.2691117525100708
outputs_pos.loss : 1.0409833192825317
outputs_pos.loss : 1.3116252422332764
outputs_pos.loss : 1.029544711112976
outputs_pos.loss : 1.4091062545776367
outputs_pos.loss : 1.4262062311172485
Epoch 00153: adjusting learning rate of group 0 to 2.4986e-06.
outputs_pos.loss : 1.1601113080978394
outputs_pos.loss : 0.9445297122001648
outputs_pos.loss : 1.53806471824646
outputs_pos.loss : 1.150571584701538
outputs_pos.loss : 1.0248583555221558
outputs_pos.loss : 1.5133107900619507
outputs_pos.loss : 1.6569740772247314
outputs_pos.loss : 1.625994086265564
Epoch 00154: adjusting learning rate of group 0 to 2.4985e-06.
outputs_pos.loss : 0.946349561214447
outputs_pos.loss : 1.1095657348632812
outputs_pos.loss : 1.0209555625915527
outputs_pos.loss : 1.0401886701583862
outputs_pos.loss : 1.1864668130874634
outputs_pos.loss : 1.2615447044372559
outputs_pos.loss : 1.476980209350586
outputs_pos.loss : 1.3032761812210083
Epoch 00155: adjusting learning rate of group 0 to 2.4985e-06.
outputs_pos.loss : 1.1991325616836548
outputs_pos.loss : 1.2862919569015503
outputs_pos.loss : 1.5421298742294312
outputs_pos.loss : 1.364613652229309
outputs_pos.loss : 1.2078680992126465
outputs_pos.loss : 1.3610433340072632
outputs_pos.loss : 1.2867767810821533
outputs_pos.loss : 1.3403444290161133
Epoch 00156: adjusting learning rate of group 0 to 2.4985e-06.
outputs_pos.loss : 1.5183507204055786
outputs_pos.loss : 1.0302637815475464
outputs_pos.loss : 1.0627517700195312
outputs_pos.loss : 1.402185320854187
outputs_pos.loss : 1.1886653900146484
outputs_pos.loss : 0.9800748229026794
outputs_pos.loss : 1.4469192028045654
outputs_pos.loss : 1.0443066358566284
Epoch 00157: adjusting learning rate of group 0 to 2.4985e-06.
outputs_pos.loss : 1.1213459968566895
outputs_pos.loss : 1.3175568580627441
outputs_pos.loss : 1.3007168769836426
outputs_pos.loss : 1.250001072883606
outputs_pos.loss : 1.7055790424346924
outputs_pos.loss : 0.8524178862571716
outputs_pos.loss : 2.28629469871521
outputs_pos.loss : 1.9196099042892456
Epoch 00158: adjusting learning rate of group 0 to 2.4985e-06.
outputs_pos.loss : 1.2717034816741943
outputs_pos.loss : 1.5661791563034058
outputs_pos.loss : 1.2345770597457886
outputs_pos.loss : 1.3446836471557617
outputs_pos.loss : 0.9870489835739136
outputs_pos.loss : 1.118644118309021
outputs_pos.loss : 1.1968131065368652
outputs_pos.loss : 1.4403949975967407
Epoch 00159: adjusting learning rate of group 0 to 2.4984e-06.
outputs_pos.loss : 1.584995150566101
outputs_pos.loss : 1.6066910028457642
outputs_pos.loss : 1.6634892225265503
outputs_pos.loss : 0.8749784231185913
outputs_pos.loss : 1.6882368326187134
outputs_pos.loss : 1.2512097358703613
outputs_pos.loss : 1.3016798496246338
outputs_pos.loss : 1.3159763813018799
Epoch 00160: adjusting learning rate of group 0 to 2.4984e-06.
outputs_pos.loss : 1.3188207149505615
outputs_pos.loss : 0.9948258996009827
outputs_pos.loss : 1.3207054138183594
outputs_pos.loss : 1.730959177017212
outputs_pos.loss : 0.932807445526123
outputs_pos.loss : 1.2256413698196411
outputs_pos.loss : 1.485540747642517
outputs_pos.loss : 1.4925837516784668
Epoch 00161: adjusting learning rate of group 0 to 2.4984e-06.
outputs_pos.loss : 1.5359978675842285
outputs_pos.loss : 1.3751001358032227
outputs_pos.loss : 1.2712492942810059
outputs_pos.loss : 1.1297951936721802
outputs_pos.loss : 1.1869721412658691
outputs_pos.loss : 1.2447516918182373
outputs_pos.loss : 1.4417226314544678
outputs_pos.loss : 0.7613644599914551
Epoch 00162: adjusting learning rate of group 0 to 2.4984e-06.
outputs_pos.loss : 1.2430675029754639
outputs_pos.loss : 0.9827663898468018
outputs_pos.loss : 1.698551893234253
outputs_pos.loss : 1.538265347480774
outputs_pos.loss : 1.7900590896606445
outputs_pos.loss : 1.6676936149597168
outputs_pos.loss : 1.1136138439178467
outputs_pos.loss : 0.9868146181106567
Epoch 00163: adjusting learning rate of group 0 to 2.4984e-06.
outputs_pos.loss : 1.2611998319625854
outputs_pos.loss : 1.2684192657470703
outputs_pos.loss : 0.8896095752716064
outputs_pos.loss : 1.2039252519607544
outputs_pos.loss : 1.3255022764205933
outputs_pos.loss : 1.3060133457183838
outputs_pos.loss : 1.063302993774414
outputs_pos.loss : 1.5886651277542114
Epoch 00164: adjusting learning rate of group 0 to 2.4983e-06.
outputs_pos.loss : 1.020723819732666
outputs_pos.loss : 1.1789095401763916
outputs_pos.loss : 1.0967638492584229
outputs_pos.loss : 1.3008341789245605
outputs_pos.loss : 1.229026436805725
outputs_pos.loss : 1.70298433303833
outputs_pos.loss : 1.5061619281768799
outputs_pos.loss : 1.6893609762191772
Epoch 00165: adjusting learning rate of group 0 to 2.4983e-06.
outputs_pos.loss : 1.1506166458129883
outputs_pos.loss : 1.3948725461959839
outputs_pos.loss : 1.2776758670806885
outputs_pos.loss : 1.192775011062622
outputs_pos.loss : 1.3395776748657227
outputs_pos.loss : 1.348225712776184
outputs_pos.loss : 1.4725102186203003
outputs_pos.loss : 1.302751898765564
Epoch 00166: adjusting learning rate of group 0 to 2.4983e-06.
outputs_pos.loss : 1.269962191581726
outputs_pos.loss : 1.2999696731567383
outputs_pos.loss : 1.0342766046524048
outputs_pos.loss : 1.1045337915420532
outputs_pos.loss : 1.1481865644454956
outputs_pos.loss : 1.6690003871917725
outputs_pos.loss : 1.4262968301773071
outputs_pos.loss : 1.1342815160751343
Epoch 00167: adjusting learning rate of group 0 to 2.4983e-06.
outputs_pos.loss : 1.0639996528625488
outputs_pos.loss : 0.8450344800949097
outputs_pos.loss : 1.4065943956375122
outputs_pos.loss : 1.4260921478271484
outputs_pos.loss : 0.9723893404006958
outputs_pos.loss : 1.2460488080978394
outputs_pos.loss : 1.1146488189697266
outputs_pos.loss : 0.8119121789932251
Epoch 00168: adjusting learning rate of group 0 to 2.4983e-06.
outputs_pos.loss : 1.3762954473495483
outputs_pos.loss : 1.1304150819778442
outputs_pos.loss : 1.093772292137146
outputs_pos.loss : 1.1814547777175903
outputs_pos.loss : 1.2755601406097412
outputs_pos.loss : 1.193375587463379
outputs_pos.loss : 1.3499910831451416
outputs_pos.loss : 1.566665768623352
Epoch 00169: adjusting learning rate of group 0 to 2.4982e-06.
outputs_pos.loss : 1.4578726291656494
outputs_pos.loss : 1.0833159685134888
outputs_pos.loss : 1.4535727500915527
outputs_pos.loss : 1.2219618558883667
outputs_pos.loss : 0.8389644026756287
outputs_pos.loss : 1.1076531410217285
outputs_pos.loss : 1.3143302202224731
outputs_pos.loss : 1.1376209259033203
Epoch 00170: adjusting learning rate of group 0 to 2.4982e-06.
outputs_pos.loss : 1.1427254676818848
outputs_pos.loss : 1.454742193222046
outputs_pos.loss : 1.0321613550186157
outputs_pos.loss : 1.401089072227478
outputs_pos.loss : 1.2178919315338135
outputs_pos.loss : 1.1449716091156006
outputs_pos.loss : 0.8339426517486572
outputs_pos.loss : 1.1779650449752808
Epoch 00171: adjusting learning rate of group 0 to 2.4982e-06.
outputs_pos.loss : 1.116633415222168
outputs_pos.loss : 1.3770802021026611
outputs_pos.loss : 0.8001378178596497
outputs_pos.loss : 1.2004430294036865
outputs_pos.loss : 1.1593574285507202
outputs_pos.loss : 1.2965552806854248
outputs_pos.loss : 1.3562175035476685
outputs_pos.loss : 1.4857146739959717
Epoch 00172: adjusting learning rate of group 0 to 2.4982e-06.
outputs_pos.loss : 1.2543565034866333
outputs_pos.loss : 0.857667863368988
outputs_pos.loss : 1.262963891029358
outputs_pos.loss : 1.438618540763855
outputs_pos.loss : 1.4537373781204224
outputs_pos.loss : 1.0918035507202148
outputs_pos.loss : 1.4815644025802612
outputs_pos.loss : 1.6837966442108154
Epoch 00173: adjusting learning rate of group 0 to 2.4982e-06.
outputs_pos.loss : 1.0479621887207031
outputs_pos.loss : 1.1698896884918213
outputs_pos.loss : 1.1341667175292969
outputs_pos.loss : 1.2859413623809814
outputs_pos.loss : 1.5405503511428833
outputs_pos.loss : 1.012082815170288
outputs_pos.loss : 1.05255925655365
outputs_pos.loss : 0.964034914970398
Epoch 00174: adjusting learning rate of group 0 to 2.4981e-06.
outputs_pos.loss : 1.676525592803955
outputs_pos.loss : 1.1303938627243042
outputs_pos.loss : 1.006032943725586
outputs_pos.loss : 1.0551366806030273
outputs_pos.loss : 1.049912929534912
outputs_pos.loss : 0.9985367655754089
outputs_pos.loss : 1.2324939966201782
outputs_pos.loss : 0.9910464882850647
Epoch 00175: adjusting learning rate of group 0 to 2.4981e-06.
outputs_pos.loss : 2.003113269805908
outputs_pos.loss : 1.0169504880905151
outputs_pos.loss : 1.3596746921539307
outputs_pos.loss : 1.66146981716156
outputs_pos.loss : 1.4764503240585327
outputs_pos.loss : 1.0227446556091309
outputs_pos.loss : 1.48408043384552
outputs_pos.loss : 1.1031280755996704
Epoch 00176: adjusting learning rate of group 0 to 2.4981e-06.
outputs_pos.loss : 1.152635097503662
outputs_pos.loss : 1.3553229570388794
outputs_pos.loss : 1.2446602582931519
outputs_pos.loss : 1.012392282485962
outputs_pos.loss : 1.695123314857483
outputs_pos.loss : 1.4858061075210571
outputs_pos.loss : 1.859403371810913
outputs_pos.loss : 1.0745128393173218
Epoch 00177: adjusting learning rate of group 0 to 2.4981e-06.
outputs_pos.loss : 1.026031255722046
outputs_pos.loss : 1.7263715267181396
outputs_pos.loss : 1.3620511293411255
outputs_pos.loss : 1.2294533252716064
outputs_pos.loss : 1.2502235174179077
outputs_pos.loss : 1.1512370109558105
outputs_pos.loss : 0.8836957216262817
outputs_pos.loss : 1.7681050300598145
Epoch 00178: adjusting learning rate of group 0 to 2.4980e-06.
outputs_pos.loss : 1.035223364830017
outputs_pos.loss : 1.0077661275863647
outputs_pos.loss : 1.4967701435089111
outputs_pos.loss : 1.9903613328933716
outputs_pos.loss : 1.5626753568649292
outputs_pos.loss : 0.9283938407897949
outputs_pos.loss : 1.511464238166809
outputs_pos.loss : 1.3868052959442139
Epoch 00179: adjusting learning rate of group 0 to 2.4980e-06.
outputs_pos.loss : 1.1773474216461182
outputs_pos.loss : 1.2495503425598145
outputs_pos.loss : 0.9903436899185181
outputs_pos.loss : 0.9075828790664673
outputs_pos.loss : 1.3482773303985596
outputs_pos.loss : 1.3893133401870728
outputs_pos.loss : 1.1785311698913574
outputs_pos.loss : 1.2984055280685425
Epoch 00180: adjusting learning rate of group 0 to 2.4980e-06.
outputs_pos.loss : 1.2719204425811768
outputs_pos.loss : 1.337192416191101
outputs_pos.loss : 0.9515522718429565
outputs_pos.loss : 1.1147921085357666
outputs_pos.loss : 1.25754714012146
outputs_pos.loss : 1.5238443613052368
outputs_pos.loss : 1.1618155241012573
outputs_pos.loss : 1.1533839702606201
Epoch 00181: adjusting learning rate of group 0 to 2.4980e-06.
outputs_pos.loss : 1.600395917892456
outputs_pos.loss : 1.3713817596435547
outputs_pos.loss : 1.260445237159729
outputs_pos.loss : 1.333508849143982
outputs_pos.loss : 0.9466607570648193
outputs_pos.loss : 1.2225674390792847
outputs_pos.loss : 1.1416397094726562
outputs_pos.loss : 1.404123067855835
Epoch 00182: adjusting learning rate of group 0 to 2.4980e-06.
outputs_pos.loss : 1.3013540506362915
outputs_pos.loss : 1.0572381019592285
outputs_pos.loss : 1.0918097496032715
outputs_pos.loss : 1.414505958557129
outputs_pos.loss : 1.3719738721847534
outputs_pos.loss : 1.1727793216705322
outputs_pos.loss : 1.5772087574005127
outputs_pos.loss : 1.5343070030212402
Epoch 00183: adjusting learning rate of group 0 to 2.4979e-06.
outputs_pos.loss : 0.857818067073822
outputs_pos.loss : 1.313288927078247
outputs_pos.loss : 1.1756596565246582
outputs_pos.loss : 0.8695007562637329
outputs_pos.loss : 1.1441513299942017
outputs_pos.loss : 1.6152496337890625
outputs_pos.loss : 1.2411220073699951
outputs_pos.loss : 1.1046396493911743
Epoch 00184: adjusting learning rate of group 0 to 2.4979e-06.
outputs_pos.loss : 1.3812230825424194
outputs_pos.loss : 1.6167999505996704
outputs_pos.loss : 1.4004522562026978
outputs_pos.loss : 1.3294504880905151
outputs_pos.loss : 1.3690202236175537
outputs_pos.loss : 1.530455231666565
outputs_pos.loss : 1.0479518175125122
outputs_pos.loss : 1.177663803100586
Epoch 00185: adjusting learning rate of group 0 to 2.4979e-06.
outputs_pos.loss : 1.433849811553955
outputs_pos.loss : 1.5941435098648071
outputs_pos.loss : 1.0753716230392456
outputs_pos.loss : 0.9954071044921875
outputs_pos.loss : 0.7820906043052673
outputs_pos.loss : 1.051748514175415
outputs_pos.loss : 1.0300483703613281
outputs_pos.loss : 1.6442856788635254
Epoch 00186: adjusting learning rate of group 0 to 2.4979e-06.
outputs_pos.loss : 1.1106038093566895
outputs_pos.loss : 1.375750184059143
outputs_pos.loss : 1.3057342767715454
outputs_pos.loss : 1.5598838329315186
outputs_pos.loss : 1.4214016199111938
outputs_pos.loss : 1.7236032485961914
outputs_pos.loss : 1.2302955389022827
outputs_pos.loss : 1.1083048582077026
Epoch 00187: adjusting learning rate of group 0 to 2.4978e-06.
outputs_pos.loss : 2.234487295150757
outputs_pos.loss : 1.1784875392913818
outputs_pos.loss : 1.431828498840332
outputs_pos.loss : 0.9798839092254639
outputs_pos.loss : 1.6166887283325195
outputs_pos.loss : 1.2350692749023438
outputs_pos.loss : 1.3642479181289673
outputs_pos.loss : 1.2544023990631104
Epoch 00188: adjusting learning rate of group 0 to 2.4978e-06.
outputs_pos.loss : 1.3411496877670288
outputs_pos.loss : 1.4472929239273071
outputs_pos.loss : 1.4747321605682373
outputs_pos.loss : 1.4051403999328613
outputs_pos.loss : 1.0967698097229004
outputs_pos.loss : 1.555213212966919
outputs_pos.loss : 1.1186327934265137
outputs_pos.loss : 1.3374519348144531
Epoch 00189: adjusting learning rate of group 0 to 2.4978e-06.
outputs_pos.loss : 1.5204261541366577
outputs_pos.loss : 1.475355625152588
outputs_pos.loss : 1.3026748895645142
outputs_pos.loss : 1.181917428970337
outputs_pos.loss : 1.7078076601028442
outputs_pos.loss : 1.4604308605194092
outputs_pos.loss : 1.2045818567276
outputs_pos.loss : 1.6237597465515137
Epoch 00190: adjusting learning rate of group 0 to 2.4978e-06.
outputs_pos.loss : 1.1625099182128906
outputs_pos.loss : 1.217826247215271
outputs_pos.loss : 1.938746452331543
outputs_pos.loss : 1.3146125078201294
outputs_pos.loss : 0.9165751934051514
outputs_pos.loss : 0.8728016018867493
outputs_pos.loss : 1.4067622423171997
outputs_pos.loss : 1.4587548971176147
Epoch 00191: adjusting learning rate of group 0 to 2.4978e-06.
outputs_pos.loss : 1.3888506889343262
outputs_pos.loss : 1.130570411682129
outputs_pos.loss : 1.2248903512954712
outputs_pos.loss : 1.0869712829589844
outputs_pos.loss : 1.1179571151733398
outputs_pos.loss : 1.697028636932373
outputs_pos.loss : 1.2606910467147827
outputs_pos.loss : 1.2761553525924683
Epoch 00192: adjusting learning rate of group 0 to 2.4977e-06.
outputs_pos.loss : 2.57409405708313
outputs_pos.loss : 1.1406747102737427
outputs_pos.loss : 1.2143058776855469
outputs_pos.loss : 1.3327833414077759
outputs_pos.loss : 1.229183554649353
outputs_pos.loss : 1.1080580949783325
outputs_pos.loss : 1.2559399604797363
outputs_pos.loss : 1.1322497129440308
Epoch 00193: adjusting learning rate of group 0 to 2.4977e-06.
outputs_pos.loss : 1.2564935684204102
outputs_pos.loss : 1.6365303993225098
outputs_pos.loss : 1.3140257596969604
outputs_pos.loss : 1.2767239809036255
outputs_pos.loss : 0.9912082552909851
outputs_pos.loss : 0.9647239446640015
outputs_pos.loss : 1.1025384664535522
outputs_pos.loss : 1.0210320949554443
Epoch 00194: adjusting learning rate of group 0 to 2.4977e-06.
outputs_pos.loss : 1.5733968019485474
outputs_pos.loss : 1.4550176858901978
outputs_pos.loss : 1.6487032175064087
outputs_pos.loss : 1.3962693214416504
outputs_pos.loss : 1.2929123640060425
outputs_pos.loss : 1.1367648839950562
outputs_pos.loss : 1.522249460220337
outputs_pos.loss : 1.156843662261963
Epoch 00195: adjusting learning rate of group 0 to 2.4977e-06.
outputs_pos.loss : 1.0407887697219849
outputs_pos.loss : 1.2169325351715088
outputs_pos.loss : 1.3630692958831787
outputs_pos.loss : 1.175976037979126
outputs_pos.loss : 1.0296058654785156
outputs_pos.loss : 1.7131832838058472
outputs_pos.loss : 1.4671472311019897
outputs_pos.loss : 1.0237617492675781
Epoch 00196: adjusting learning rate of group 0 to 2.4976e-06.
outputs_pos.loss : 1.352292776107788
outputs_pos.loss : 1.384387731552124
outputs_pos.loss : 1.4820878505706787
outputs_pos.loss : 1.4262886047363281
outputs_pos.loss : 0.7238728404045105
outputs_pos.loss : 1.1320728063583374
outputs_pos.loss : 1.3134400844573975
outputs_pos.loss : 1.1911182403564453
Epoch 00197: adjusting learning rate of group 0 to 2.4976e-06.
outputs_pos.loss : 1.0936249494552612
outputs_pos.loss : 1.2550197839736938
outputs_pos.loss : 1.2416958808898926
outputs_pos.loss : 1.801509141921997
outputs_pos.loss : 0.9526543021202087
outputs_pos.loss : 1.8985873460769653
outputs_pos.loss : 1.273249626159668
outputs_pos.loss : 1.212068796157837
Epoch 00198: adjusting learning rate of group 0 to 2.4976e-06.
outputs_pos.loss : 1.1332192420959473
outputs_pos.loss : 1.507413387298584
outputs_pos.loss : 0.8580052852630615
outputs_pos.loss : 0.9423792362213135
outputs_pos.loss : 1.9655581712722778
outputs_pos.loss : 1.381547212600708
outputs_pos.loss : 1.1718097925186157
outputs_pos.loss : 1.0589168071746826
Epoch 00199: adjusting learning rate of group 0 to 2.4976e-06.
outputs_pos.loss : 1.5995934009552002
outputs_pos.loss : 1.2688363790512085
outputs_pos.loss : 1.3103646039962769
outputs_pos.loss : 0.9644350409507751
outputs_pos.loss : 1.5589654445648193
outputs_pos.loss : 1.4274346828460693
outputs_pos.loss : 1.643125057220459
outputs_pos.loss : 0.9778323769569397
Epoch 00200: adjusting learning rate of group 0 to 2.4975e-06.
outputs_pos.loss : 1.3614118099212646
outputs_pos.loss : 1.4443652629852295
outputs_pos.loss : 1.233598232269287
outputs_pos.loss : 1.715030312538147
outputs_pos.loss : 1.2243324518203735
outputs_pos.loss : 1.1923919916152954
outputs_pos.loss : 1.2180064916610718
outputs_pos.loss : 1.1078463792800903
Epoch 00201: adjusting learning rate of group 0 to 2.4975e-06.
outputs_pos.loss : 0.7841062545776367
outputs_pos.loss : 1.3679879903793335
outputs_pos.loss : 1.6452488899230957
outputs_pos.loss : 1.2113780975341797
outputs_pos.loss : 1.544481635093689
outputs_pos.loss : 1.0640522241592407
outputs_pos.loss : 1.2211929559707642
outputs_pos.loss : 1.2038025856018066
Epoch 00202: adjusting learning rate of group 0 to 2.4975e-06.
outputs_pos.loss : 0.8527690172195435
outputs_pos.loss : 1.1670186519622803
outputs_pos.loss : 1.062940239906311
outputs_pos.loss : 1.7507882118225098
outputs_pos.loss : 1.1222823858261108
outputs_pos.loss : 1.096928596496582
outputs_pos.loss : 1.7435187101364136
outputs_pos.loss : 1.3746912479400635
Epoch 00203: adjusting learning rate of group 0 to 2.4975e-06.
outputs_pos.loss : 1.131042242050171
outputs_pos.loss : 1.490217924118042
outputs_pos.loss : 1.3856532573699951
outputs_pos.loss : 1.3539761304855347
outputs_pos.loss : 1.4645040035247803
outputs_pos.loss : 1.121801733970642
outputs_pos.loss : 1.174189805984497
outputs_pos.loss : 1.7767590284347534
Epoch 00204: adjusting learning rate of group 0 to 2.4974e-06.
outputs_pos.loss : 0.999846339225769
outputs_pos.loss : 1.3066240549087524
outputs_pos.loss : 1.5259060859680176
outputs_pos.loss : 0.9653842449188232
outputs_pos.loss : 1.370006799697876
outputs_pos.loss : 1.1320831775665283
outputs_pos.loss : 1.3753124475479126
outputs_pos.loss : 1.1740689277648926
Epoch 00205: adjusting learning rate of group 0 to 2.4974e-06.
outputs_pos.loss : 0.967998743057251
outputs_pos.loss : 1.5789307355880737
outputs_pos.loss : 1.1607633829116821
outputs_pos.loss : 1.1368540525436401
outputs_pos.loss : 1.1252506971359253
outputs_pos.loss : 1.4337941408157349
outputs_pos.loss : 1.2575539350509644
outputs_pos.loss : 1.4730409383773804
Epoch 00206: adjusting learning rate of group 0 to 2.4974e-06.
outputs_pos.loss : 1.2977200746536255
outputs_pos.loss : 1.3204113245010376
outputs_pos.loss : 1.528110384941101
outputs_pos.loss : 1.1543922424316406
outputs_pos.loss : 0.9986866116523743
outputs_pos.loss : 1.0287638902664185
outputs_pos.loss : 1.1869244575500488
outputs_pos.loss : 1.5414725542068481
Epoch 00207: adjusting learning rate of group 0 to 2.4974e-06.
outputs_pos.loss : 1.7490700483322144
outputs_pos.loss : 1.3378640413284302
outputs_pos.loss : 1.491741418838501
outputs_pos.loss : 1.522947907447815
outputs_pos.loss : 1.5084034204483032
outputs_pos.loss : 1.7300515174865723
outputs_pos.loss : 1.3541818857192993
outputs_pos.loss : 1.0887792110443115
Epoch 00208: adjusting learning rate of group 0 to 2.4973e-06.
outputs_pos.loss : 1.2504054307937622
outputs_pos.loss : 1.0041370391845703
outputs_pos.loss : 1.1363474130630493
outputs_pos.loss : 1.2128570079803467
outputs_pos.loss : 1.0487147569656372
outputs_pos.loss : 1.1266556978225708
outputs_pos.loss : 1.0823631286621094
outputs_pos.loss : 1.6027523279190063
Epoch 00209: adjusting learning rate of group 0 to 2.4973e-06.
outputs_pos.loss : 1.0856621265411377
outputs_pos.loss : 1.5034693479537964
outputs_pos.loss : 1.3457921743392944
outputs_pos.loss : 1.4142037630081177
outputs_pos.loss : 1.3111982345581055
outputs_pos.loss : 1.4485129117965698
outputs_pos.loss : 1.0048809051513672
outputs_pos.loss : 1.4213273525238037
Epoch 00210: adjusting learning rate of group 0 to 2.4973e-06.
outputs_pos.loss : 1.141642689704895
outputs_pos.loss : 1.4910937547683716
outputs_pos.loss : 1.0589567422866821
outputs_pos.loss : 1.6687095165252686
outputs_pos.loss : 1.1023651361465454
outputs_pos.loss : 1.08290696144104
outputs_pos.loss : 1.6070163249969482
outputs_pos.loss : 1.393702507019043
Epoch 00211: adjusting learning rate of group 0 to 2.4973e-06.
outputs_pos.loss : 1.1837745904922485
outputs_pos.loss : 1.1027759313583374
outputs_pos.loss : 1.1546903848648071
outputs_pos.loss : 1.597228765487671
outputs_pos.loss : 1.4765838384628296
outputs_pos.loss : 1.2325401306152344
outputs_pos.loss : 1.3148279190063477
outputs_pos.loss : 0.9835337996482849
Epoch 00212: adjusting learning rate of group 0 to 2.4972e-06.
outputs_pos.loss : 1.2140485048294067
outputs_pos.loss : 1.4213004112243652
outputs_pos.loss : 1.6884207725524902
outputs_pos.loss : 1.323251485824585
outputs_pos.loss : 1.184013843536377
outputs_pos.loss : 0.864325225353241
outputs_pos.loss : 1.2833021879196167
outputs_pos.loss : 1.3749110698699951
Epoch 00213: adjusting learning rate of group 0 to 2.4972e-06.
outputs_pos.loss : 1.1454370021820068
outputs_pos.loss : 1.006603717803955
outputs_pos.loss : 1.7577118873596191
outputs_pos.loss : 1.3801484107971191
outputs_pos.loss : 1.306339144706726
outputs_pos.loss : 1.0442852973937988
outputs_pos.loss : 1.0392717123031616
outputs_pos.loss : 1.4683746099472046
Epoch 00214: adjusting learning rate of group 0 to 2.4972e-06.
outputs_pos.loss : 0.8961989283561707
outputs_pos.loss : 1.0875612497329712
outputs_pos.loss : 1.0961827039718628
outputs_pos.loss : 1.2546800374984741
outputs_pos.loss : 1.3818286657333374
outputs_pos.loss : 0.8203513026237488
outputs_pos.loss : 0.9952710270881653
outputs_pos.loss : 1.2529006004333496
Epoch 00215: adjusting learning rate of group 0 to 2.4971e-06.
outputs_pos.loss : 1.493518590927124
outputs_pos.loss : 1.4584965705871582
outputs_pos.loss : 1.1418333053588867
outputs_pos.loss : 1.22604501247406
outputs_pos.loss : 1.0890634059906006
outputs_pos.loss : 1.1252772808074951
outputs_pos.loss : 1.1317566633224487
outputs_pos.loss : 1.6582450866699219
Epoch 00216: adjusting learning rate of group 0 to 2.4971e-06.
outputs_pos.loss : 1.459368109703064
outputs_pos.loss : 1.278213620185852
outputs_pos.loss : 0.9651745557785034
outputs_pos.loss : 1.0704689025878906
outputs_pos.loss : 1.025514006614685
outputs_pos.loss : 1.1484284400939941
outputs_pos.loss : 1.546716332435608
outputs_pos.loss : 1.3196462392807007
Epoch 00217: adjusting learning rate of group 0 to 2.4971e-06.
outputs_pos.loss : 1.3904832601547241
outputs_pos.loss : 1.5279794931411743
outputs_pos.loss : 1.3012974262237549
outputs_pos.loss : 1.7132500410079956
outputs_pos.loss : 1.2856913805007935
outputs_pos.loss : 1.0922160148620605
outputs_pos.loss : 0.8810541033744812
outputs_pos.loss : 1.705869197845459
Epoch 00218: adjusting learning rate of group 0 to 2.4971e-06.
outputs_pos.loss : 1.1219677925109863
outputs_pos.loss : 1.410434603691101
outputs_pos.loss : 2.8415942192077637
outputs_pos.loss : 1.1658892631530762
outputs_pos.loss : 1.4840272665023804
outputs_pos.loss : 0.906887948513031
outputs_pos.loss : 1.1514418125152588
outputs_pos.loss : 1.472624659538269
Epoch 00219: adjusting learning rate of group 0 to 2.4970e-06.
outputs_pos.loss : 1.201342225074768
outputs_pos.loss : 1.0513603687286377
outputs_pos.loss : 1.0526360273361206
outputs_pos.loss : 1.8457353115081787
outputs_pos.loss : 1.1668142080307007
outputs_pos.loss : 1.2653602361679077
outputs_pos.loss : 0.9187528491020203
outputs_pos.loss : 1.1971516609191895
Epoch 00220: adjusting learning rate of group 0 to 2.4970e-06.
outputs_pos.loss : 1.119439721107483
outputs_pos.loss : 0.8760709762573242
outputs_pos.loss : 1.6327842473983765
outputs_pos.loss : 1.3546547889709473
outputs_pos.loss : 1.6272079944610596
outputs_pos.loss : 1.1073651313781738
outputs_pos.loss : 0.9128046631813049
outputs_pos.loss : 1.1116595268249512
Epoch 00221: adjusting learning rate of group 0 to 2.4970e-06.
outputs_pos.loss : 1.881406545639038
outputs_pos.loss : 1.2146626710891724
outputs_pos.loss : 1.4267239570617676
outputs_pos.loss : 1.2206403017044067
outputs_pos.loss : 1.4564365148544312
outputs_pos.loss : 1.6033798456192017
outputs_pos.loss : 1.5203351974487305
outputs_pos.loss : 1.3390567302703857
Epoch 00222: adjusting learning rate of group 0 to 2.4970e-06.
outputs_pos.loss : 0.958295464515686
outputs_pos.loss : 1.322260856628418
outputs_pos.loss : 1.4916338920593262
outputs_pos.loss : 1.1605859994888306
outputs_pos.loss : 0.8806558847427368
outputs_pos.loss : 1.5983061790466309
outputs_pos.loss : 1.4229516983032227
outputs_pos.loss : 1.3482838869094849
Epoch 00223: adjusting learning rate of group 0 to 2.4969e-06.
outputs_pos.loss : 1.4809472560882568
outputs_pos.loss : 2.1058318614959717
outputs_pos.loss : 1.058609127998352
outputs_pos.loss : 1.1515265703201294
outputs_pos.loss : 0.8541979789733887
outputs_pos.loss : 0.8337065577507019
outputs_pos.loss : 1.689784288406372
outputs_pos.loss : 1.5641634464263916
Epoch 00224: adjusting learning rate of group 0 to 2.4969e-06.
outputs_pos.loss : 1.169094443321228
outputs_pos.loss : 1.0498230457305908
outputs_pos.loss : 1.2709739208221436
outputs_pos.loss : 1.140092134475708
outputs_pos.loss : 1.3954377174377441
outputs_pos.loss : 0.903317391872406
outputs_pos.loss : 0.9712449312210083
outputs_pos.loss : 1.1412566900253296
Epoch 00225: adjusting learning rate of group 0 to 2.4969e-06.
outputs_pos.loss : 1.3779727220535278
outputs_pos.loss : 1.0341509580612183
outputs_pos.loss : 1.0032023191452026
outputs_pos.loss : 0.7801570296287537
outputs_pos.loss : 1.1652206182479858
outputs_pos.loss : 1.0755704641342163
outputs_pos.loss : 1.2987570762634277
outputs_pos.loss : 1.2879502773284912
Epoch 00226: adjusting learning rate of group 0 to 2.4969e-06.
outputs_pos.loss : 1.1997970342636108
outputs_pos.loss : 1.0139223337173462
outputs_pos.loss : 1.3607882261276245
outputs_pos.loss : 1.3401525020599365
outputs_pos.loss : 1.580538034439087
outputs_pos.loss : 1.2880762815475464
outputs_pos.loss : 1.6577322483062744
outputs_pos.loss : 0.97023606300354
Epoch 00227: adjusting learning rate of group 0 to 2.4968e-06.
outputs_pos.loss : 1.4549342393875122
outputs_pos.loss : 1.5465530157089233
outputs_pos.loss : 1.2461882829666138
outputs_pos.loss : 1.0249333381652832
outputs_pos.loss : 1.2133499383926392
outputs_pos.loss : 1.2459996938705444
outputs_pos.loss : 1.1907098293304443
outputs_pos.loss : 1.2187639474868774
Epoch 00228: adjusting learning rate of group 0 to 2.4968e-06.
outputs_pos.loss : 1.6306169033050537
outputs_pos.loss : 1.3624365329742432
outputs_pos.loss : 1.0714856386184692
outputs_pos.loss : 1.4248048067092896
outputs_pos.loss : 1.0623873472213745
outputs_pos.loss : 1.0678819417953491
outputs_pos.loss : 1.2663613557815552
outputs_pos.loss : 1.1366993188858032
Epoch 00229: adjusting learning rate of group 0 to 2.4968e-06.
outputs_pos.loss : 1.055612325668335
outputs_pos.loss : 1.1945056915283203
outputs_pos.loss : 1.402258038520813
outputs_pos.loss : 0.9306517839431763
outputs_pos.loss : 1.4229369163513184
outputs_pos.loss : 1.4018058776855469
outputs_pos.loss : 1.1850208044052124
outputs_pos.loss : 1.2456340789794922
Epoch 00230: adjusting learning rate of group 0 to 2.4967e-06.
outputs_pos.loss : 1.3169066905975342
outputs_pos.loss : 1.274266004562378
outputs_pos.loss : 1.6410824060440063
outputs_pos.loss : 1.28634512424469
outputs_pos.loss : 1.2205023765563965
outputs_pos.loss : 1.4485164880752563
outputs_pos.loss : 1.2619657516479492
outputs_pos.loss : 1.495023488998413
Epoch 00231: adjusting learning rate of group 0 to 2.4967e-06.
outputs_pos.loss : 1.1467201709747314
outputs_pos.loss : 1.218772292137146
outputs_pos.loss : 1.389775037765503
outputs_pos.loss : 1.921128749847412
outputs_pos.loss : 0.9284757375717163
outputs_pos.loss : 1.676690697669983
outputs_pos.loss : 1.1936674118041992
outputs_pos.loss : 1.2177679538726807
Epoch 00232: adjusting learning rate of group 0 to 2.4967e-06.
outputs_pos.loss : 1.0437694787979126
outputs_pos.loss : 0.9546713829040527
outputs_pos.loss : 1.1828981637954712
outputs_pos.loss : 1.0840877294540405
outputs_pos.loss : 0.7802667021751404
outputs_pos.loss : 0.8853200674057007
outputs_pos.loss : 1.0052101612091064
outputs_pos.loss : 1.304692268371582
Epoch 00233: adjusting learning rate of group 0 to 2.4967e-06.
outputs_pos.loss : 1.347493290901184
outputs_pos.loss : 1.2095463275909424
outputs_pos.loss : 1.417209506034851
outputs_pos.loss : 1.0810620784759521
outputs_pos.loss : 0.9764125347137451
outputs_pos.loss : 1.2142270803451538
outputs_pos.loss : 1.4692178964614868
outputs_pos.loss : 0.9832510948181152
Epoch 00234: adjusting learning rate of group 0 to 2.4966e-06.
outputs_pos.loss : 1.1368941068649292
outputs_pos.loss : 1.1397531032562256
outputs_pos.loss : 1.002288579940796
outputs_pos.loss : 1.1022478342056274
outputs_pos.loss : 1.3413585424423218
outputs_pos.loss : 1.0692753791809082
outputs_pos.loss : 1.2857376337051392
outputs_pos.loss : 1.2814981937408447
Epoch 00235: adjusting learning rate of group 0 to 2.4966e-06.
outputs_pos.loss : 0.9436442852020264
outputs_pos.loss : 1.3490442037582397
outputs_pos.loss : 1.3878103494644165
outputs_pos.loss : 1.1420681476593018
outputs_pos.loss : 0.7946822047233582
outputs_pos.loss : 1.3293389081954956
outputs_pos.loss : 1.1745843887329102
outputs_pos.loss : 1.4706672430038452
Epoch 00236: adjusting learning rate of group 0 to 2.4966e-06.
outputs_pos.loss : 0.7046393752098083
outputs_pos.loss : 0.9497485160827637
outputs_pos.loss : 1.2456133365631104
outputs_pos.loss : 1.3397071361541748
outputs_pos.loss : 1.2044868469238281
outputs_pos.loss : 1.171952724456787
outputs_pos.loss : 1.0317001342773438
outputs_pos.loss : 1.3198081254959106
Epoch 00237: adjusting learning rate of group 0 to 2.4965e-06.
outputs_pos.loss : 1.6887584924697876
outputs_pos.loss : 1.7039313316345215
outputs_pos.loss : 1.35041344165802
outputs_pos.loss : 0.9067410230636597
outputs_pos.loss : 1.2425416707992554
outputs_pos.loss : 0.9538044929504395
outputs_pos.loss : 1.2645542621612549
outputs_pos.loss : 1.8023048639297485
Epoch 00238: adjusting learning rate of group 0 to 2.4965e-06.
outputs_pos.loss : 0.838541567325592
outputs_pos.loss : 1.0150200128555298
outputs_pos.loss : 1.0481970310211182
outputs_pos.loss : 1.2579485177993774
outputs_pos.loss : 1.0539723634719849
outputs_pos.loss : 1.4459995031356812
outputs_pos.loss : 1.1927937269210815
outputs_pos.loss : 1.199816107749939
Epoch 00239: adjusting learning rate of group 0 to 2.4965e-06.
outputs_pos.loss : 1.7815686464309692
outputs_pos.loss : 1.3423736095428467
outputs_pos.loss : 1.2174370288848877
outputs_pos.loss : 1.3414887189865112
outputs_pos.loss : 1.1697192192077637
outputs_pos.loss : 1.389102578163147
outputs_pos.loss : 1.3128355741500854
outputs_pos.loss : 0.9623967409133911
Epoch 00240: adjusting learning rate of group 0 to 2.4964e-06.
outputs_pos.loss : 1.1550511121749878
outputs_pos.loss : 1.6889764070510864
outputs_pos.loss : 0.669438898563385
outputs_pos.loss : 1.624184489250183
outputs_pos.loss : 1.4625403881072998
outputs_pos.loss : 1.5642822980880737
outputs_pos.loss : 1.4792319536209106
outputs_pos.loss : 1.3165315389633179
Epoch 00241: adjusting learning rate of group 0 to 2.4964e-06.
outputs_pos.loss : 1.4610005617141724
outputs_pos.loss : 1.0214215517044067
outputs_pos.loss : 0.9914003610610962
outputs_pos.loss : 1.3831931352615356
outputs_pos.loss : 1.2315372228622437
outputs_pos.loss : 0.8677188158035278
outputs_pos.loss : 1.3569618463516235
outputs_pos.loss : 1.4622995853424072
Epoch 00242: adjusting learning rate of group 0 to 2.4964e-06.
outputs_pos.loss : 1.268638014793396
outputs_pos.loss : 0.8425819873809814
outputs_pos.loss : 1.1429386138916016
outputs_pos.loss : 0.984550416469574
outputs_pos.loss : 1.0811827182769775
outputs_pos.loss : 1.0554183721542358
outputs_pos.loss : 1.0987242460250854
outputs_pos.loss : 1.342832088470459
Epoch 00243: adjusting learning rate of group 0 to 2.4964e-06.
outputs_pos.loss : 1.532857060432434
outputs_pos.loss : 1.0218418836593628
outputs_pos.loss : 1.0635333061218262
outputs_pos.loss : 0.9262024164199829
outputs_pos.loss : 1.970613956451416
outputs_pos.loss : 1.6424894332885742
outputs_pos.loss : 1.2399581670761108
outputs_pos.loss : 1.4927258491516113
Epoch 00244: adjusting learning rate of group 0 to 2.4963e-06.
outputs_pos.loss : 0.9789942502975464
outputs_pos.loss : 1.0871528387069702
outputs_pos.loss : 1.0660749673843384
outputs_pos.loss : 1.275140643119812
outputs_pos.loss : 1.344490885734558
outputs_pos.loss : 1.7695997953414917
outputs_pos.loss : 1.1909418106079102
outputs_pos.loss : 1.608993649482727
Epoch 00245: adjusting learning rate of group 0 to 2.4963e-06.
outputs_pos.loss : 0.8774264454841614
outputs_pos.loss : 1.3701435327529907
outputs_pos.loss : 1.0614303350448608
outputs_pos.loss : 1.1488268375396729
outputs_pos.loss : 1.2455201148986816
outputs_pos.loss : 1.341599464416504
outputs_pos.loss : 0.8115001916885376
outputs_pos.loss : 1.2976584434509277
Epoch 00246: adjusting learning rate of group 0 to 2.4963e-06.
outputs_pos.loss : 1.3747446537017822
outputs_pos.loss : 1.6467348337173462
outputs_pos.loss : 1.3979237079620361
outputs_pos.loss : 1.5765483379364014
outputs_pos.loss : 2.43135929107666
outputs_pos.loss : 1.0123211145401
outputs_pos.loss : 1.2691515684127808
outputs_pos.loss : 1.702835202217102
Epoch 00247: adjusting learning rate of group 0 to 2.4962e-06.
outputs_pos.loss : 1.236398696899414
outputs_pos.loss : 2.3000307083129883
outputs_pos.loss : 1.3024600744247437
outputs_pos.loss : 1.3163572549819946
outputs_pos.loss : 1.5043625831604004
outputs_pos.loss : 1.0020523071289062
outputs_pos.loss : 1.5472996234893799
outputs_pos.loss : 1.459449052810669
Epoch 00248: adjusting learning rate of group 0 to 2.4962e-06.
outputs_pos.loss : 1.3910932540893555
outputs_pos.loss : 1.017728567123413
outputs_pos.loss : 1.5031639337539673
outputs_pos.loss : 1.0119023323059082
outputs_pos.loss : 1.433363676071167
outputs_pos.loss : 1.253886103630066
outputs_pos.loss : 1.0167227983474731
outputs_pos.loss : 0.9850556254386902
Epoch 00249: adjusting learning rate of group 0 to 2.4962e-06.
outputs_pos.loss : 1.245409369468689
outputs_pos.loss : 1.452985405921936
outputs_pos.loss : 1.2970237731933594
outputs_pos.loss : 1.0082237720489502
outputs_pos.loss : 0.9027820825576782
outputs_pos.loss : 1.6141865253448486
outputs_pos.loss : 1.346794605255127
outputs_pos.loss : 2.0403335094451904
Epoch 00250: adjusting learning rate of group 0 to 2.4961e-06.
outputs_pos.loss : 0.8662621378898621
outputs_pos.loss : 0.813240110874176
outputs_pos.loss : 1.3743102550506592
outputs_pos.loss : 1.46747624874115
outputs_pos.loss : 1.234001874923706
outputs_pos.loss : 1.0231523513793945
outputs_pos.loss : 1.8754169940948486
outputs_pos.loss : 1.275704026222229
Epoch 00251: adjusting learning rate of group 0 to 2.4961e-06.
outputs_pos.loss : 1.06640625
outputs_pos.loss : 1.361202359199524
outputs_pos.loss : 1.4678831100463867
outputs_pos.loss : 1.4493343830108643
outputs_pos.loss : 0.9377997517585754
outputs_pos.loss : 1.1929253339767456
outputs_pos.loss : 1.3194636106491089
outputs_pos.loss : 1.7928638458251953
Epoch 00252: adjusting learning rate of group 0 to 2.4961e-06.
outputs_pos.loss : 1.2544918060302734
outputs_pos.loss : 1.0618352890014648
outputs_pos.loss : 1.577352523803711
outputs_pos.loss : 0.8523240089416504
outputs_pos.loss : 1.187289834022522
outputs_pos.loss : 1.0004088878631592
outputs_pos.loss : 1.1617223024368286
outputs_pos.loss : 1.5528134107589722
Epoch 00253: adjusting learning rate of group 0 to 2.4961e-06.
outputs_pos.loss : 0.877116858959198
outputs_pos.loss : 1.2445135116577148
outputs_pos.loss : 1.137935757637024
outputs_pos.loss : 1.6136468648910522
outputs_pos.loss : 0.9687219858169556
outputs_pos.loss : 1.0714069604873657
outputs_pos.loss : 1.4745547771453857
outputs_pos.loss : 0.8307144045829773
Epoch 00254: adjusting learning rate of group 0 to 2.4960e-06.
outputs_pos.loss : 1.181864619255066
outputs_pos.loss : 1.2674555778503418
outputs_pos.loss : 1.3805968761444092
outputs_pos.loss : 1.5236390829086304
outputs_pos.loss : 0.941484272480011
outputs_pos.loss : 1.0492992401123047
outputs_pos.loss : 1.3719408512115479
outputs_pos.loss : 1.3939841985702515
Epoch 00255: adjusting learning rate of group 0 to 2.4960e-06.
outputs_pos.loss : 1.2854163646697998
outputs_pos.loss : 1.5988407135009766
outputs_pos.loss : 1.3702322244644165
outputs_pos.loss : 0.9215512871742249
outputs_pos.loss : 1.7468616962432861
outputs_pos.loss : 1.1300230026245117
outputs_pos.loss : 1.6268258094787598
outputs_pos.loss : 1.6337729692459106
Epoch 00256: adjusting learning rate of group 0 to 2.4960e-06.
outputs_pos.loss : 1.1108323335647583
outputs_pos.loss : 0.843567967414856
outputs_pos.loss : 1.3653830289840698
outputs_pos.loss : 1.288509726524353
outputs_pos.loss : 1.370010495185852
outputs_pos.loss : 1.5689510107040405
outputs_pos.loss : 1.2200039625167847
outputs_pos.loss : 0.9782122373580933
Epoch 00257: adjusting learning rate of group 0 to 2.4959e-06.
outputs_pos.loss : 1.2255735397338867
outputs_pos.loss : 1.308383822441101
outputs_pos.loss : 2.3619213104248047
outputs_pos.loss : 1.2629179954528809
outputs_pos.loss : 1.2660815715789795
outputs_pos.loss : 1.0603057146072388
outputs_pos.loss : 1.5007139444351196
outputs_pos.loss : 0.7777548432350159
Epoch 00258: adjusting learning rate of group 0 to 2.4959e-06.
outputs_pos.loss : 1.0340814590454102
outputs_pos.loss : 0.7886137962341309
outputs_pos.loss : 1.711089015007019
outputs_pos.loss : 1.0157674551010132
outputs_pos.loss : 1.1961793899536133
outputs_pos.loss : 1.0297250747680664
outputs_pos.loss : 1.1921800374984741
outputs_pos.loss : 1.2096762657165527
Epoch 00259: adjusting learning rate of group 0 to 2.4959e-06.
outputs_pos.loss : 1.0677695274353027
outputs_pos.loss : 1.1409862041473389
outputs_pos.loss : 1.4142169952392578
outputs_pos.loss : 1.3163126707077026
outputs_pos.loss : 1.5293112993240356
outputs_pos.loss : 1.2503795623779297
outputs_pos.loss : 1.2028604745864868
outputs_pos.loss : 1.5131502151489258
Epoch 00260: adjusting learning rate of group 0 to 2.4958e-06.
outputs_pos.loss : 1.351858377456665
outputs_pos.loss : 1.9086716175079346
outputs_pos.loss : 1.1134308576583862
outputs_pos.loss : 0.9358228445053101
outputs_pos.loss : 1.4749209880828857
outputs_pos.loss : 1.1162395477294922
outputs_pos.loss : 1.1248815059661865
outputs_pos.loss : 1.1103392839431763
Epoch 00261: adjusting learning rate of group 0 to 2.4958e-06.
outputs_pos.loss : 1.0269283056259155
outputs_pos.loss : 1.3370462656021118
outputs_pos.loss : 1.1672766208648682
outputs_pos.loss : 1.0598082542419434
outputs_pos.loss : 1.2688915729522705
outputs_pos.loss : 1.2028056383132935
outputs_pos.loss : 1.2741471529006958
outputs_pos.loss : 1.2256746292114258
Epoch 00262: adjusting learning rate of group 0 to 2.4958e-06.
outputs_pos.loss : 1.0291558504104614
outputs_pos.loss : 1.830117106437683
outputs_pos.loss : 1.3273180723190308
outputs_pos.loss : 1.097252368927002
outputs_pos.loss : 1.030745267868042
outputs_pos.loss : 1.3080246448516846
outputs_pos.loss : 1.1819038391113281
outputs_pos.loss : 1.684509515762329
Epoch 00263: adjusting learning rate of group 0 to 2.4957e-06.
outputs_pos.loss : 1.0697660446166992
outputs_pos.loss : 1.1852796077728271
outputs_pos.loss : 1.3807978630065918
outputs_pos.loss : 1.4358758926391602
outputs_pos.loss : 1.2564494609832764
outputs_pos.loss : 1.3466514348983765
outputs_pos.loss : 1.1630910634994507
outputs_pos.loss : 1.238271713256836
Epoch 00264: adjusting learning rate of group 0 to 2.4957e-06.
outputs_pos.loss : 1.6013224124908447
outputs_pos.loss : 0.4390927255153656
outputs_pos.loss : 1.0104594230651855
outputs_pos.loss : 1.4260302782058716
outputs_pos.loss : 0.9341267943382263
outputs_pos.loss : 1.1618611812591553
outputs_pos.loss : 1.4469197988510132
outputs_pos.loss : 1.2431535720825195
Epoch 00265: adjusting learning rate of group 0 to 2.4957e-06.
outputs_pos.loss : 1.1660678386688232
outputs_pos.loss : 1.4639922380447388
outputs_pos.loss : 1.0170037746429443
outputs_pos.loss : 0.7250518202781677
outputs_pos.loss : 1.2399345636367798
outputs_pos.loss : 1.1293805837631226
outputs_pos.loss : 1.2024976015090942
outputs_pos.loss : 0.9212066531181335
Epoch 00266: adjusting learning rate of group 0 to 2.4956e-06.
outputs_pos.loss : 1.4097661972045898
outputs_pos.loss : 1.0756264925003052
outputs_pos.loss : 1.036582350730896
outputs_pos.loss : 2.0220906734466553
outputs_pos.loss : 1.2719627618789673
outputs_pos.loss : 2.0473990440368652
outputs_pos.loss : 1.4941891431808472
outputs_pos.loss : 1.656214714050293
Epoch 00267: adjusting learning rate of group 0 to 2.4956e-06.
outputs_pos.loss : 1.2357321977615356
outputs_pos.loss : 1.6819225549697876
outputs_pos.loss : 1.0945216417312622
outputs_pos.loss : 1.3839282989501953
outputs_pos.loss : 1.3290228843688965
outputs_pos.loss : 1.6359751224517822
outputs_pos.loss : 1.1378086805343628
outputs_pos.loss : 0.8605297803878784
Epoch 00268: adjusting learning rate of group 0 to 2.4956e-06.
outputs_pos.loss : 0.9442571997642517
outputs_pos.loss : 1.3746540546417236
outputs_pos.loss : 1.4381242990493774
outputs_pos.loss : 1.0576039552688599
outputs_pos.loss : 1.4373292922973633
outputs_pos.loss : 1.0326534509658813
outputs_pos.loss : 1.1294913291931152
outputs_pos.loss : 1.3606308698654175
Epoch 00269: adjusting learning rate of group 0 to 2.4955e-06.
outputs_pos.loss : 1.1433342695236206
outputs_pos.loss : 0.9440849423408508
outputs_pos.loss : 1.2165166139602661
outputs_pos.loss : 1.0849251747131348
outputs_pos.loss : 0.9060285687446594
outputs_pos.loss : 1.5476322174072266
outputs_pos.loss : 1.0638329982757568
outputs_pos.loss : 1.5153063535690308
Epoch 00270: adjusting learning rate of group 0 to 2.4955e-06.
outputs_pos.loss : 0.973079264163971
outputs_pos.loss : 1.601081371307373
outputs_pos.loss : 1.5198698043823242
outputs_pos.loss : 1.1490660905838013
outputs_pos.loss : 1.2696348428726196
outputs_pos.loss : 1.5491528511047363
outputs_pos.loss : 1.0778087377548218
outputs_pos.loss : 0.7410767078399658
Epoch 00271: adjusting learning rate of group 0 to 2.4955e-06.
outputs_pos.loss : 1.3161407709121704
outputs_pos.loss : 1.2956792116165161
outputs_pos.loss : 1.1905864477157593
outputs_pos.loss : 1.2632660865783691
outputs_pos.loss : 0.9349068999290466
outputs_pos.loss : 1.2637441158294678
outputs_pos.loss : 1.2815141677856445
outputs_pos.loss : 1.4765284061431885
Epoch 00272: adjusting learning rate of group 0 to 2.4954e-06.
outputs_pos.loss : 1.2527267932891846
outputs_pos.loss : 1.0678808689117432
outputs_pos.loss : 1.133675217628479
outputs_pos.loss : 0.8553568720817566
outputs_pos.loss : 1.4250506162643433
outputs_pos.loss : 0.9681209325790405
outputs_pos.loss : 1.6352128982543945
outputs_pos.loss : 1.0516856908798218
Epoch 00273: adjusting learning rate of group 0 to 2.4954e-06.
outputs_pos.loss : 1.7746027708053589
outputs_pos.loss : 1.0453674793243408
outputs_pos.loss : 1.3161816596984863
outputs_pos.loss : 1.3574978113174438
outputs_pos.loss : 1.3421710729599
outputs_pos.loss : 1.538648009300232
outputs_pos.loss : 1.4039292335510254
outputs_pos.loss : 1.5294995307922363
Epoch 00274: adjusting learning rate of group 0 to 2.4954e-06.
outputs_pos.loss : 1.2772479057312012
outputs_pos.loss : 1.2117384672164917
outputs_pos.loss : 1.1644461154937744
outputs_pos.loss : 0.9705519676208496
outputs_pos.loss : 1.6080961227416992
outputs_pos.loss : 1.030911922454834
outputs_pos.loss : 1.3224698305130005
outputs_pos.loss : 1.679909110069275
Epoch 00275: adjusting learning rate of group 0 to 2.4953e-06.
outputs_pos.loss : 1.4419233798980713
outputs_pos.loss : 1.410501480102539
outputs_pos.loss : 1.1292577981948853
outputs_pos.loss : 1.1488423347473145
outputs_pos.loss : 0.947346568107605
outputs_pos.loss : 1.643720030784607
outputs_pos.loss : 1.2058829069137573
outputs_pos.loss : 1.275486707687378
Epoch 00276: adjusting learning rate of group 0 to 2.4953e-06.
outputs_pos.loss : 1.464392066001892
outputs_pos.loss : 0.6340956687927246
outputs_pos.loss : 1.2726751565933228
outputs_pos.loss : 1.354730486869812
outputs_pos.loss : 1.100716471672058
outputs_pos.loss : 1.438300609588623
outputs_pos.loss : 1.4571713209152222
outputs_pos.loss : 1.3471128940582275
Epoch 00277: adjusting learning rate of group 0 to 2.4953e-06.
outputs_pos.loss : 1.21454656124115
outputs_pos.loss : 0.9327563643455505
outputs_pos.loss : 1.8473711013793945
outputs_pos.loss : 1.0740599632263184
outputs_pos.loss : 1.039313793182373
outputs_pos.loss : 1.3269670009613037
outputs_pos.loss : 1.2997242212295532
outputs_pos.loss : 1.1848543882369995
Epoch 00278: adjusting learning rate of group 0 to 2.4952e-06.
outputs_pos.loss : 1.2495583295822144
outputs_pos.loss : 1.450635313987732
outputs_pos.loss : 1.2510387897491455
outputs_pos.loss : 1.5280839204788208
outputs_pos.loss : 1.1559886932373047
outputs_pos.loss : 1.019371747970581
outputs_pos.loss : 1.237412929534912
outputs_pos.loss : 1.1561740636825562
Epoch 00279: adjusting learning rate of group 0 to 2.4952e-06.
outputs_pos.loss : 1.3157778978347778
outputs_pos.loss : 1.8774282932281494
outputs_pos.loss : 1.380610466003418
outputs_pos.loss : 1.668476939201355
outputs_pos.loss : 1.096673607826233
outputs_pos.loss : 1.2987829446792603
outputs_pos.loss : 0.9813641905784607
outputs_pos.loss : 1.300256609916687
Epoch 00280: adjusting learning rate of group 0 to 2.4952e-06.
outputs_pos.loss : 1.2693331241607666
outputs_pos.loss : 1.4811313152313232
outputs_pos.loss : 1.405714750289917
outputs_pos.loss : 1.296676754951477
outputs_pos.loss : 0.9379250407218933
outputs_pos.loss : 1.7270009517669678
outputs_pos.loss : 1.2116832733154297
outputs_pos.loss : 1.2969220876693726
Epoch 00281: adjusting learning rate of group 0 to 2.4951e-06.
outputs_pos.loss : 1.1640729904174805
outputs_pos.loss : 1.2682992219924927
outputs_pos.loss : 1.3897777795791626
outputs_pos.loss : 1.39681077003479
outputs_pos.loss : 2.004999876022339
outputs_pos.loss : 1.2693862915039062
outputs_pos.loss : 0.8756861686706543
outputs_pos.loss : 1.013665795326233
Epoch 00282: adjusting learning rate of group 0 to 2.4951e-06.
outputs_pos.loss : 1.0823639631271362
outputs_pos.loss : 1.1131993532180786
outputs_pos.loss : 1.3078389167785645
outputs_pos.loss : 1.3150569200515747
outputs_pos.loss : 1.3611564636230469
outputs_pos.loss : 1.226178765296936
outputs_pos.loss : 0.9537995457649231
outputs_pos.loss : 1.593477725982666
Epoch 00283: adjusting learning rate of group 0 to 2.4951e-06.
outputs_pos.loss : 1.1731245517730713
outputs_pos.loss : 0.8224397897720337
outputs_pos.loss : 1.339662790298462
outputs_pos.loss : 1.403913140296936
outputs_pos.loss : 1.321354627609253
outputs_pos.loss : 1.1733665466308594
outputs_pos.loss : 0.7412950992584229
outputs_pos.loss : 1.148127794265747
Epoch 00284: adjusting learning rate of group 0 to 2.4950e-06.
outputs_pos.loss : 1.3733494281768799
outputs_pos.loss : 1.464096188545227
outputs_pos.loss : 1.1619360446929932
outputs_pos.loss : 1.0311473608016968
outputs_pos.loss : 1.2475411891937256
outputs_pos.loss : 1.4981738328933716
outputs_pos.loss : 1.660803198814392
outputs_pos.loss : 1.2774887084960938
Epoch 00285: adjusting learning rate of group 0 to 2.4950e-06.
outputs_pos.loss : 0.9481880068778992
outputs_pos.loss : 1.2246172428131104
outputs_pos.loss : 0.9848778247833252
outputs_pos.loss : 1.1853879690170288
outputs_pos.loss : 0.9182537198066711
outputs_pos.loss : 1.1915843486785889
outputs_pos.loss : 1.182413935661316
outputs_pos.loss : 1.0258766412734985
Epoch 00286: adjusting learning rate of group 0 to 2.4950e-06.
outputs_pos.loss : 1.7437121868133545
outputs_pos.loss : 1.058598279953003
outputs_pos.loss : 1.79204523563385
outputs_pos.loss : 0.976803719997406
outputs_pos.loss : 1.3430200815200806
outputs_pos.loss : 1.3756061792373657
outputs_pos.loss : 1.4175809621810913
outputs_pos.loss : 1.1201483011245728
Epoch 00287: adjusting learning rate of group 0 to 2.4949e-06.
outputs_pos.loss : 1.0452609062194824
outputs_pos.loss : 1.2058271169662476
outputs_pos.loss : 0.9432411789894104
outputs_pos.loss : 1.674768090248108
outputs_pos.loss : 0.883269727230072
outputs_pos.loss : 1.119473934173584
outputs_pos.loss : 1.4830025434494019
outputs_pos.loss : 1.2542120218276978
Epoch 00288: adjusting learning rate of group 0 to 2.4949e-06.
outputs_pos.loss : 1.0918225049972534
outputs_pos.loss : 1.0650306940078735
outputs_pos.loss : 1.3980578184127808
outputs_pos.loss : 0.958747386932373
outputs_pos.loss : 1.0374428033828735
outputs_pos.loss : 1.520158052444458
outputs_pos.loss : 1.144379734992981
outputs_pos.loss : 1.5835884809494019
Epoch 00289: adjusting learning rate of group 0 to 2.4949e-06.
outputs_pos.loss : 1.1075851917266846
outputs_pos.loss : 1.0691862106323242
outputs_pos.loss : 0.9448782205581665
outputs_pos.loss : 1.5203793048858643
outputs_pos.loss : 1.5681226253509521
outputs_pos.loss : 1.453556776046753
outputs_pos.loss : 0.8612881302833557
outputs_pos.loss : 0.8670388460159302
Epoch 00290: adjusting learning rate of group 0 to 2.4948e-06.
outputs_pos.loss : 0.8251804113388062
outputs_pos.loss : 1.1821213960647583
outputs_pos.loss : 1.5499308109283447
outputs_pos.loss : 1.1030402183532715
outputs_pos.loss : 1.4001004695892334
outputs_pos.loss : 0.7650982737541199
outputs_pos.loss : 1.1458148956298828
outputs_pos.loss : 1.2809209823608398
Epoch 00291: adjusting learning rate of group 0 to 2.4948e-06.
outputs_pos.loss : 1.0901442766189575
outputs_pos.loss : 1.639451503753662
outputs_pos.loss : 1.5844355821609497
outputs_pos.loss : 1.5312951803207397
outputs_pos.loss : 0.997431218624115
outputs_pos.loss : 1.0743924379348755
outputs_pos.loss : 1.7345961332321167
outputs_pos.loss : 1.1614739894866943
Epoch 00292: adjusting learning rate of group 0 to 2.4947e-06.
outputs_pos.loss : 1.3276678323745728
outputs_pos.loss : 1.5064131021499634
outputs_pos.loss : 1.5628339052200317
outputs_pos.loss : 1.5850796699523926
outputs_pos.loss : 1.4059690237045288
outputs_pos.loss : 1.3473650217056274
outputs_pos.loss : 0.8607548475265503
outputs_pos.loss : 0.9533870816230774
Epoch 00293: adjusting learning rate of group 0 to 2.4947e-06.
outputs_pos.loss : 1.376185655593872
outputs_pos.loss : 1.8651819229125977
outputs_pos.loss : 1.359327793121338
outputs_pos.loss : 1.1631377935409546
outputs_pos.loss : 1.4793442487716675
outputs_pos.loss : 1.4353649616241455
outputs_pos.loss : 1.0152007341384888
outputs_pos.loss : 0.953392505645752
Epoch 00294: adjusting learning rate of group 0 to 2.4947e-06.
outputs_pos.loss : 1.5312649011611938
outputs_pos.loss : 1.8525044918060303
outputs_pos.loss : 1.257254958152771
outputs_pos.loss : 1.506681203842163
outputs_pos.loss : 0.8967410326004028
outputs_pos.loss : 0.8274471759796143
outputs_pos.loss : 1.3743380308151245
outputs_pos.loss : 1.09425950050354
Epoch 00295: adjusting learning rate of group 0 to 2.4946e-06.
outputs_pos.loss : 1.0625735521316528
outputs_pos.loss : 1.439874291419983
outputs_pos.loss : 1.1256762742996216
outputs_pos.loss : 1.427664041519165
outputs_pos.loss : 1.0271573066711426
outputs_pos.loss : 1.007204532623291
outputs_pos.loss : 1.9559638500213623
outputs_pos.loss : 1.3862427473068237
Epoch 00296: adjusting learning rate of group 0 to 2.4946e-06.
outputs_pos.loss : 1.5826573371887207
outputs_pos.loss : 1.5775376558303833
outputs_pos.loss : 0.7058197855949402
outputs_pos.loss : 1.0938913822174072
outputs_pos.loss : 0.947555422782898
outputs_pos.loss : 0.8859878182411194
outputs_pos.loss : 1.2743251323699951
outputs_pos.loss : 1.3502695560455322
Epoch 00297: adjusting learning rate of group 0 to 2.4946e-06.
outputs_pos.loss : 0.9952844381332397
outputs_pos.loss : 1.1897395849227905
outputs_pos.loss : 1.1839631795883179
outputs_pos.loss : 1.54087233543396
outputs_pos.loss : 1.190293788909912
outputs_pos.loss : 1.0303001403808594
outputs_pos.loss : 1.2915430068969727
outputs_pos.loss : 1.570041537284851
Epoch 00298: adjusting learning rate of group 0 to 2.4945e-06.
outputs_pos.loss : 1.4144132137298584
outputs_pos.loss : 0.6953920722007751
outputs_pos.loss : 1.2727208137512207
outputs_pos.loss : 1.14067542552948
outputs_pos.loss : 0.7733621597290039
outputs_pos.loss : 1.4232568740844727
outputs_pos.loss : 1.387804388999939
outputs_pos.loss : 1.400733232498169
Epoch 00299: adjusting learning rate of group 0 to 2.4945e-06.
outputs_pos.loss : 0.8557463884353638
outputs_pos.loss : 1.0124543905258179
outputs_pos.loss : 1.2150870561599731
outputs_pos.loss : 1.1054141521453857
outputs_pos.loss : 1.4551979303359985
outputs_pos.loss : 1.0021835565567017
outputs_pos.loss : 0.8321747183799744
outputs_pos.loss : 1.1604101657867432
Epoch 00300: adjusting learning rate of group 0 to 2.4945e-06.
outputs_pos.loss : 1.3763501644134521
outputs_pos.loss : 1.139588475227356
outputs_pos.loss : 1.317211627960205
outputs_pos.loss : 1.3058240413665771
outputs_pos.loss : 1.4345577955245972
outputs_pos.loss : 1.2118079662322998
outputs_pos.loss : 1.3552250862121582
outputs_pos.loss : 1.354735255241394
Epoch 00301: adjusting learning rate of group 0 to 2.4944e-06.
outputs_pos.loss : 1.2396490573883057
outputs_pos.loss : 1.7962408065795898
outputs_pos.loss : 1.6003096103668213
outputs_pos.loss : 1.2802150249481201
outputs_pos.loss : 1.8329280614852905
outputs_pos.loss : 1.4832782745361328
outputs_pos.loss : 1.3772445917129517
outputs_pos.loss : 1.1053625345230103
Epoch 00302: adjusting learning rate of group 0 to 2.4944e-06.
outputs_pos.loss : 1.0871652364730835
outputs_pos.loss : 1.0576082468032837
outputs_pos.loss : 1.0316746234893799
outputs_pos.loss : 1.2734423875808716
outputs_pos.loss : 1.3791871070861816
outputs_pos.loss : 1.1538597345352173
outputs_pos.loss : 1.4731194972991943
outputs_pos.loss : 1.636182427406311
Epoch 00303: adjusting learning rate of group 0 to 2.4943e-06.
outputs_pos.loss : 1.3388032913208008
outputs_pos.loss : 1.097190022468567
outputs_pos.loss : 1.119240641593933
outputs_pos.loss : 1.01173996925354
outputs_pos.loss : 1.1816480159759521
outputs_pos.loss : 1.2874327898025513
outputs_pos.loss : 0.7751003503799438
outputs_pos.loss : 1.4237276315689087
Epoch 00304: adjusting learning rate of group 0 to 2.4943e-06.
outputs_pos.loss : 1.166211485862732
outputs_pos.loss : 1.1931736469268799
outputs_pos.loss : 1.7118291854858398
outputs_pos.loss : 1.1371873617172241
outputs_pos.loss : 1.7521917819976807
outputs_pos.loss : 1.243104338645935
outputs_pos.loss : 1.3642255067825317
outputs_pos.loss : 0.9887667894363403
Epoch 00305: adjusting learning rate of group 0 to 2.4943e-06.
outputs_pos.loss : 1.362220048904419
outputs_pos.loss : 1.7217401266098022
outputs_pos.loss : 1.1078135967254639
outputs_pos.loss : 0.9803870916366577
outputs_pos.loss : 0.9361157417297363
outputs_pos.loss : 1.118280291557312
outputs_pos.loss : 0.8730922341346741
outputs_pos.loss : 1.078037142753601
Epoch 00306: adjusting learning rate of group 0 to 2.4942e-06.
outputs_pos.loss : 1.3585920333862305
outputs_pos.loss : 1.2313227653503418
outputs_pos.loss : 1.6865546703338623
outputs_pos.loss : 0.8441997766494751
outputs_pos.loss : 1.7959442138671875
outputs_pos.loss : 1.4072175025939941
outputs_pos.loss : 1.2935717105865479
outputs_pos.loss : 1.1088742017745972
Epoch 00307: adjusting learning rate of group 0 to 2.4942e-06.
outputs_pos.loss : 1.169269323348999
outputs_pos.loss : 0.9251235723495483
outputs_pos.loss : 1.215661883354187
outputs_pos.loss : 1.399692177772522
outputs_pos.loss : 1.186771273612976
outputs_pos.loss : 1.073458194732666
outputs_pos.loss : 1.3397048711776733
outputs_pos.loss : 1.3114773035049438
Epoch 00308: adjusting learning rate of group 0 to 2.4942e-06.
outputs_pos.loss : 1.6248153448104858
outputs_pos.loss : 1.5683045387268066
outputs_pos.loss : 1.3496928215026855
outputs_pos.loss : 0.832664966583252
outputs_pos.loss : 1.1485265493392944
outputs_pos.loss : 1.3064228296279907
outputs_pos.loss : 1.031931757926941
outputs_pos.loss : 1.2409477233886719
Epoch 00309: adjusting learning rate of group 0 to 2.4941e-06.
outputs_pos.loss : 1.4341278076171875
outputs_pos.loss : 1.1613434553146362
outputs_pos.loss : 1.4564803838729858
outputs_pos.loss : 1.3272314071655273
outputs_pos.loss : 1.5083744525909424
outputs_pos.loss : 1.1463931798934937
outputs_pos.loss : 1.614942193031311
outputs_pos.loss : 1.146181583404541
Epoch 00310: adjusting learning rate of group 0 to 2.4941e-06.
outputs_pos.loss : 1.1023094654083252
outputs_pos.loss : 0.9219012260437012
outputs_pos.loss : 1.2556350231170654
outputs_pos.loss : 1.270784854888916
outputs_pos.loss : 1.5512856245040894
outputs_pos.loss : 1.6461461782455444
outputs_pos.loss : 1.1668375730514526
outputs_pos.loss : 1.5713378190994263
Epoch 00311: adjusting learning rate of group 0 to 2.4940e-06.
outputs_pos.loss : 1.4119302034378052
outputs_pos.loss : 1.6262949705123901
outputs_pos.loss : 1.0297958850860596
outputs_pos.loss : 1.5083314180374146
outputs_pos.loss : 0.9635857343673706
outputs_pos.loss : 1.1084603071212769
outputs_pos.loss : 0.9987847208976746
outputs_pos.loss : 1.336366891860962
Epoch 00312: adjusting learning rate of group 0 to 2.4940e-06.
outputs_pos.loss : 1.7079411745071411
outputs_pos.loss : 0.9195337891578674
outputs_pos.loss : 1.6502028703689575
outputs_pos.loss : 1.3299460411071777
outputs_pos.loss : 1.0175305604934692
outputs_pos.loss : 1.327942967414856
outputs_pos.loss : 1.2245463132858276
outputs_pos.loss : 1.9608346223831177
Epoch 00313: adjusting learning rate of group 0 to 2.4940e-06.
outputs_pos.loss : 1.4072614908218384
outputs_pos.loss : 1.4705586433410645
outputs_pos.loss : 1.499104380607605
outputs_pos.loss : 0.886512041091919
outputs_pos.loss : 2.3296432495117188
outputs_pos.loss : 1.1143383979797363
outputs_pos.loss : 1.295244812965393
outputs_pos.loss : 0.9107147455215454
Epoch 00314: adjusting learning rate of group 0 to 2.4939e-06.
outputs_pos.loss : 1.0132060050964355
outputs_pos.loss : 1.1814279556274414
outputs_pos.loss : 1.4467905759811401
outputs_pos.loss : 1.307374358177185
outputs_pos.loss : 1.0419659614562988
outputs_pos.loss : 1.4726617336273193
outputs_pos.loss : 1.229193925857544
outputs_pos.loss : 1.4089291095733643
Epoch 00315: adjusting learning rate of group 0 to 2.4939e-06.
outputs_pos.loss : 0.7355708479881287
outputs_pos.loss : 1.1475352048873901
outputs_pos.loss : 0.5732884407043457
outputs_pos.loss : 1.5816446542739868
outputs_pos.loss : 1.667608380317688
outputs_pos.loss : 1.5479596853256226
outputs_pos.loss : 2.0242578983306885
outputs_pos.loss : 1.321816086769104
Epoch 00316: adjusting learning rate of group 0 to 2.4938e-06.
outputs_pos.loss : 1.5569024085998535
outputs_pos.loss : 1.0077345371246338
outputs_pos.loss : 1.2886300086975098
outputs_pos.loss : 1.3032053709030151
outputs_pos.loss : 1.5800385475158691
outputs_pos.loss : 1.2897876501083374
outputs_pos.loss : 1.0147428512573242
outputs_pos.loss : 2.0642361640930176
Epoch 00317: adjusting learning rate of group 0 to 2.4938e-06.
outputs_pos.loss : 0.8953452110290527
outputs_pos.loss : 0.8264822959899902
outputs_pos.loss : 1.2438180446624756
outputs_pos.loss : 1.5497487783432007
outputs_pos.loss : 1.4210646152496338
outputs_pos.loss : 1.5926283597946167
outputs_pos.loss : 1.3028337955474854
outputs_pos.loss : 1.1718744039535522
Epoch 00318: adjusting learning rate of group 0 to 2.4938e-06.
outputs_pos.loss : 0.9612271189689636
outputs_pos.loss : 1.2446939945220947
outputs_pos.loss : 1.47992742061615
outputs_pos.loss : 1.5187437534332275
outputs_pos.loss : 1.1692110300064087
outputs_pos.loss : 1.2447128295898438
outputs_pos.loss : 1.038274884223938
outputs_pos.loss : 1.229882836341858
Epoch 00319: adjusting learning rate of group 0 to 2.4937e-06.
outputs_pos.loss : 1.314131736755371
outputs_pos.loss : 1.2203636169433594
outputs_pos.loss : 0.82662034034729
outputs_pos.loss : 1.1875245571136475
outputs_pos.loss : 0.8485649228096008
outputs_pos.loss : 1.1530479192733765
outputs_pos.loss : 0.9503641724586487
outputs_pos.loss : 0.9969509243965149
Epoch 00320: adjusting learning rate of group 0 to 2.4937e-06.
outputs_pos.loss : 0.8661059141159058
outputs_pos.loss : 1.1935880184173584
outputs_pos.loss : 1.0341668128967285
outputs_pos.loss : 1.119905710220337
outputs_pos.loss : 1.0897390842437744
outputs_pos.loss : 1.2814466953277588
outputs_pos.loss : 1.2264612913131714
outputs_pos.loss : 1.1259032487869263
Epoch 00321: adjusting learning rate of group 0 to 2.4936e-06.
outputs_pos.loss : 1.1312123537063599
outputs_pos.loss : 1.550468921661377
outputs_pos.loss : 1.9198676347732544
outputs_pos.loss : 1.1055508852005005
outputs_pos.loss : 1.1693799495697021
outputs_pos.loss : 1.3744474649429321
outputs_pos.loss : 1.5184963941574097
outputs_pos.loss : 1.1585769653320312
Epoch 00322: adjusting learning rate of group 0 to 2.4936e-06.
outputs_pos.loss : 1.0631667375564575
outputs_pos.loss : 1.0963271856307983
outputs_pos.loss : 0.9604832530021667
outputs_pos.loss : 1.1634619235992432
outputs_pos.loss : 1.256544589996338
outputs_pos.loss : 1.1364421844482422
outputs_pos.loss : 1.016172170639038
outputs_pos.loss : 0.9359766244888306
Epoch 00323: adjusting learning rate of group 0 to 2.4936e-06.
outputs_pos.loss : 1.313757300376892
outputs_pos.loss : 1.8566460609436035
outputs_pos.loss : 1.260799765586853
outputs_pos.loss : 1.5265672206878662
outputs_pos.loss : 1.0884742736816406
outputs_pos.loss : 1.9337637424468994
outputs_pos.loss : 1.1389080286026
outputs_pos.loss : 0.7824504375457764
Epoch 00324: adjusting learning rate of group 0 to 2.4935e-06.
outputs_pos.loss : 2.565333366394043
outputs_pos.loss : 1.030434489250183
outputs_pos.loss : 1.1584389209747314
outputs_pos.loss : 1.1345914602279663
outputs_pos.loss : 1.1057510375976562
outputs_pos.loss : 1.2417597770690918
outputs_pos.loss : 0.9307547807693481
outputs_pos.loss : 1.0214650630950928
Epoch 00325: adjusting learning rate of group 0 to 2.4935e-06.
outputs_pos.loss : 1.5343244075775146
outputs_pos.loss : 0.9657175540924072
outputs_pos.loss : 1.4330174922943115
outputs_pos.loss : 1.2476955652236938
outputs_pos.loss : 1.2173068523406982
outputs_pos.loss : 0.8381706476211548
outputs_pos.loss : 1.3910362720489502
outputs_pos.loss : 1.1955385208129883
Epoch 00326: adjusting learning rate of group 0 to 2.4935e-06.
outputs_pos.loss : 1.4628386497497559
outputs_pos.loss : 1.012383222579956
outputs_pos.loss : 1.0401601791381836
outputs_pos.loss : 1.3508636951446533
outputs_pos.loss : 1.1825976371765137
outputs_pos.loss : 1.9195407629013062
outputs_pos.loss : 1.27324640750885
outputs_pos.loss : 1.3441134691238403
Epoch 00327: adjusting learning rate of group 0 to 2.4934e-06.
outputs_pos.loss : 1.1694153547286987
outputs_pos.loss : 0.9409220218658447
outputs_pos.loss : 1.1905326843261719
outputs_pos.loss : 1.5149327516555786
outputs_pos.loss : 1.2688382863998413
outputs_pos.loss : 1.1018095016479492
outputs_pos.loss : 1.5571575164794922
outputs_pos.loss : 1.503857135772705
Epoch 00328: adjusting learning rate of group 0 to 2.4934e-06.
outputs_pos.loss : 1.2893866300582886
outputs_pos.loss : 1.2220934629440308
outputs_pos.loss : 0.9213474988937378
outputs_pos.loss : 1.1217793226242065
outputs_pos.loss : 0.8428952097892761
outputs_pos.loss : 1.8250653743743896
outputs_pos.loss : 1.5823818445205688
outputs_pos.loss : 1.2589386701583862
Epoch 00329: adjusting learning rate of group 0 to 2.4933e-06.
outputs_pos.loss : 1.1393009424209595
outputs_pos.loss : 1.1775226593017578
outputs_pos.loss : 1.0765386819839478
outputs_pos.loss : 1.5770618915557861
outputs_pos.loss : 0.9844128489494324
outputs_pos.loss : 0.9333410263061523
outputs_pos.loss : 2.0366175174713135
outputs_pos.loss : 1.3707493543624878
Epoch 00330: adjusting learning rate of group 0 to 2.4933e-06.
outputs_pos.loss : 0.8899807929992676
outputs_pos.loss : 1.0909700393676758
outputs_pos.loss : 0.9374921917915344
outputs_pos.loss : 1.1368334293365479
outputs_pos.loss : 1.1372795104980469
outputs_pos.loss : 1.3843705654144287
outputs_pos.loss : 1.5205614566802979
outputs_pos.loss : 0.9341275691986084
Epoch 00331: adjusting learning rate of group 0 to 2.4932e-06.
outputs_pos.loss : 1.1516501903533936
outputs_pos.loss : 1.2787803411483765
outputs_pos.loss : 1.313542127609253
outputs_pos.loss : 1.4037278890609741
outputs_pos.loss : 1.1739646196365356
outputs_pos.loss : 1.3323665857315063
outputs_pos.loss : 1.398980975151062
outputs_pos.loss : 1.255828619003296
Epoch 00332: adjusting learning rate of group 0 to 2.4932e-06.
outputs_pos.loss : 1.3136534690856934
outputs_pos.loss : 1.010618805885315
outputs_pos.loss : 1.2097282409667969
outputs_pos.loss : 1.511944055557251
outputs_pos.loss : 1.7198803424835205
outputs_pos.loss : 1.28414785861969
outputs_pos.loss : 1.4346823692321777
outputs_pos.loss : 1.0547301769256592
Epoch 00333: adjusting learning rate of group 0 to 2.4932e-06.
outputs_pos.loss : 1.9046558141708374
outputs_pos.loss : 0.9748110771179199
outputs_pos.loss : 1.4361047744750977
outputs_pos.loss : 1.1492912769317627
outputs_pos.loss : 1.0432074069976807
outputs_pos.loss : 0.8986771106719971
outputs_pos.loss : 0.8049483299255371
outputs_pos.loss : 0.8896874785423279
Epoch 00334: adjusting learning rate of group 0 to 2.4931e-06.
outputs_pos.loss : 1.4295538663864136
outputs_pos.loss : 1.3407536745071411
outputs_pos.loss : 1.2033205032348633
outputs_pos.loss : 1.4777988195419312
outputs_pos.loss : 1.0644372701644897
outputs_pos.loss : 1.0045579671859741
outputs_pos.loss : 1.093034029006958
outputs_pos.loss : 1.3747280836105347
Epoch 00335: adjusting learning rate of group 0 to 2.4931e-06.
outputs_pos.loss : 1.3898299932479858
outputs_pos.loss : 1.1891207695007324
outputs_pos.loss : 1.155478835105896
outputs_pos.loss : 2.2759673595428467
outputs_pos.loss : 1.3204970359802246
outputs_pos.loss : 0.7881618142127991
outputs_pos.loss : 1.1814966201782227
outputs_pos.loss : 1.4242734909057617
Epoch 00336: adjusting learning rate of group 0 to 2.4930e-06.
outputs_pos.loss : 1.0991212129592896
outputs_pos.loss : 0.7498204708099365
outputs_pos.loss : 1.0037473440170288
outputs_pos.loss : 1.8227201700210571
outputs_pos.loss : 1.4018248319625854
outputs_pos.loss : 0.7568901181221008
outputs_pos.loss : 1.1631447076797485
outputs_pos.loss : 0.7871567606925964
Epoch 00337: adjusting learning rate of group 0 to 2.4930e-06.
outputs_pos.loss : 0.9794623851776123
outputs_pos.loss : 1.2054059505462646
outputs_pos.loss : 1.960349678993225
outputs_pos.loss : 1.0024609565734863
outputs_pos.loss : 1.396544098854065
outputs_pos.loss : 1.364595890045166
outputs_pos.loss : 1.092628002166748
outputs_pos.loss : 1.3748512268066406
Epoch 00338: adjusting learning rate of group 0 to 2.4930e-06.
outputs_pos.loss : 1.2330963611602783
outputs_pos.loss : 1.9041932821273804
outputs_pos.loss : 1.0704045295715332
outputs_pos.loss : 1.3251276016235352
outputs_pos.loss : 1.174631953239441
outputs_pos.loss : 1.2397427558898926
outputs_pos.loss : 1.2348830699920654
outputs_pos.loss : 1.3840161561965942
Epoch 00339: adjusting learning rate of group 0 to 2.4929e-06.
outputs_pos.loss : 1.4821733236312866
outputs_pos.loss : 1.6654912233352661
outputs_pos.loss : 1.0266295671463013
outputs_pos.loss : 1.0744997262954712
outputs_pos.loss : 1.1967283487319946
outputs_pos.loss : 0.75919109582901
outputs_pos.loss : 1.4098552465438843
outputs_pos.loss : 0.9768309593200684
Epoch 00340: adjusting learning rate of group 0 to 2.4929e-06.
outputs_pos.loss : 1.692400574684143
outputs_pos.loss : 1.2217730283737183
outputs_pos.loss : 0.9716259241104126
outputs_pos.loss : 1.0216165781021118
outputs_pos.loss : 1.1651122570037842
outputs_pos.loss : 1.524174690246582
outputs_pos.loss : 0.9262935519218445
outputs_pos.loss : 1.308913230895996
Epoch 00341: adjusting learning rate of group 0 to 2.4928e-06.
outputs_pos.loss : 0.9690466523170471
outputs_pos.loss : 1.180083155632019
outputs_pos.loss : 1.5491303205490112
outputs_pos.loss : 0.8479593992233276
outputs_pos.loss : 1.6599730253219604
outputs_pos.loss : 1.6548017263412476
outputs_pos.loss : 1.0560330152511597
outputs_pos.loss : 0.8547019362449646
Epoch 00342: adjusting learning rate of group 0 to 2.4928e-06.
outputs_pos.loss : 1.1734976768493652
outputs_pos.loss : 0.6039304137229919
outputs_pos.loss : 1.317790150642395
outputs_pos.loss : 1.4922634363174438
outputs_pos.loss : 1.4797329902648926
outputs_pos.loss : 0.9382253885269165
outputs_pos.loss : 1.3216339349746704
outputs_pos.loss : 1.625471591949463
Epoch 00343: adjusting learning rate of group 0 to 2.4927e-06.
outputs_pos.loss : 0.8976278901100159
outputs_pos.loss : 1.363630771636963
outputs_pos.loss : 0.9921489953994751
outputs_pos.loss : 0.8696129322052002
outputs_pos.loss : 1.2219314575195312
outputs_pos.loss : 2.084031343460083
outputs_pos.loss : 1.2508106231689453
outputs_pos.loss : 1.3940098285675049
Epoch 00344: adjusting learning rate of group 0 to 2.4927e-06.
outputs_pos.loss : 1.1178332567214966
outputs_pos.loss : 1.6993988752365112
outputs_pos.loss : 1.3253178596496582
outputs_pos.loss : 1.2187175750732422
outputs_pos.loss : 1.0871602296829224
outputs_pos.loss : 1.1397441625595093
outputs_pos.loss : 1.3119685649871826
outputs_pos.loss : 1.153254508972168
Epoch 00345: adjusting learning rate of group 0 to 2.4927e-06.
outputs_pos.loss : 1.139585018157959
outputs_pos.loss : 1.2002949714660645
outputs_pos.loss : 1.2583513259887695
outputs_pos.loss : 0.8345041275024414
outputs_pos.loss : 1.3182709217071533
outputs_pos.loss : 1.2373250722885132
outputs_pos.loss : 1.277050495147705
outputs_pos.loss : 0.9700412154197693
Epoch 00346: adjusting learning rate of group 0 to 2.4926e-06.
outputs_pos.loss : 1.3310149908065796
outputs_pos.loss : 1.2140311002731323
outputs_pos.loss : 1.0237332582473755
outputs_pos.loss : 1.5158699750900269
outputs_pos.loss : 1.7672953605651855
outputs_pos.loss : 1.6163936853408813
outputs_pos.loss : 1.5071806907653809
outputs_pos.loss : 1.5587645769119263
Epoch 00347: adjusting learning rate of group 0 to 2.4926e-06.
outputs_pos.loss : 0.8449893593788147
outputs_pos.loss : 1.038124442100525
outputs_pos.loss : 1.4097843170166016
outputs_pos.loss : 1.1722655296325684
outputs_pos.loss : 1.0284099578857422
outputs_pos.loss : 1.0049911737442017
outputs_pos.loss : 1.1191712617874146
outputs_pos.loss : 1.0125625133514404
Epoch 00348: adjusting learning rate of group 0 to 2.4925e-06.
outputs_pos.loss : 1.4640871286392212
outputs_pos.loss : 1.8825852870941162
outputs_pos.loss : 1.1034494638442993
outputs_pos.loss : 1.1999592781066895
outputs_pos.loss : 1.6969389915466309
outputs_pos.loss : 1.301390528678894
outputs_pos.loss : 1.4542826414108276
outputs_pos.loss : 1.2413105964660645
Epoch 00349: adjusting learning rate of group 0 to 2.4925e-06.
outputs_pos.loss : 1.496654987335205
outputs_pos.loss : 1.324393630027771
outputs_pos.loss : 1.6386728286743164
outputs_pos.loss : 1.7739049196243286
outputs_pos.loss : 2.048138380050659
outputs_pos.loss : 1.3812756538391113
outputs_pos.loss : 1.3964556455612183
outputs_pos.loss : 1.2418826818466187
Epoch 00350: adjusting learning rate of group 0 to 2.4925e-06.
outputs_pos.loss : 1.1137678623199463
outputs_pos.loss : 1.3764467239379883
outputs_pos.loss : 1.150793433189392
outputs_pos.loss : 1.410776972770691
outputs_pos.loss : 1.7907663583755493
outputs_pos.loss : 1.0210683345794678
outputs_pos.loss : 1.0496994256973267
outputs_pos.loss : 0.9797399640083313
Epoch 00351: adjusting learning rate of group 0 to 2.4924e-06.
outputs_pos.loss : 1.538274884223938
outputs_pos.loss : 0.9549515843391418
outputs_pos.loss : 0.8091473579406738
outputs_pos.loss : 1.3081684112548828
outputs_pos.loss : 1.4052118062973022
outputs_pos.loss : 1.4378713369369507
outputs_pos.loss : 1.2235865592956543
outputs_pos.loss : 1.1245836019515991
Epoch 00352: adjusting learning rate of group 0 to 2.4924e-06.
outputs_pos.loss : 1.1385010480880737
outputs_pos.loss : 1.0454527139663696
outputs_pos.loss : 1.0141067504882812
outputs_pos.loss : 0.9289186596870422
outputs_pos.loss : 1.3145049810409546
outputs_pos.loss : 1.1912003755569458
outputs_pos.loss : 1.24904203414917
outputs_pos.loss : 1.7191373109817505
Epoch 00353: adjusting learning rate of group 0 to 2.4923e-06.
outputs_pos.loss : 1.2529921531677246
outputs_pos.loss : 1.4756742715835571
outputs_pos.loss : 1.326036810874939
outputs_pos.loss : 0.9030075073242188
outputs_pos.loss : 1.1489177942276
outputs_pos.loss : 1.0559258460998535
outputs_pos.loss : 1.126938819885254
outputs_pos.loss : 1.1890009641647339
Epoch 00354: adjusting learning rate of group 0 to 2.4923e-06.
outputs_pos.loss : 1.075691819190979
outputs_pos.loss : 1.4267715215682983
outputs_pos.loss : 1.5567702054977417
outputs_pos.loss : 1.336692452430725
outputs_pos.loss : 1.342774510383606
outputs_pos.loss : 0.47569286823272705
outputs_pos.loss : 1.4506880044937134
outputs_pos.loss : 0.9884681701660156
Epoch 00355: adjusting learning rate of group 0 to 2.4922e-06.
outputs_pos.loss : 0.9897817969322205
outputs_pos.loss : 1.006762146949768
outputs_pos.loss : 1.4009283781051636
outputs_pos.loss : 1.10880708694458
outputs_pos.loss : 1.2804977893829346
outputs_pos.loss : 1.4081392288208008
outputs_pos.loss : 0.9959290623664856
outputs_pos.loss : 1.1505863666534424
Epoch 00356: adjusting learning rate of group 0 to 2.4922e-06.
outputs_pos.loss : 1.607681393623352
outputs_pos.loss : 1.401206612586975
outputs_pos.loss : 0.8206512928009033
outputs_pos.loss : 1.2802177667617798
outputs_pos.loss : 0.9634230732917786
outputs_pos.loss : 1.1924331188201904
outputs_pos.loss : 1.8136440515518188
outputs_pos.loss : 1.0101510286331177
Epoch 00357: adjusting learning rate of group 0 to 2.4921e-06.
outputs_pos.loss : 1.2722855806350708
outputs_pos.loss : 2.637068271636963
outputs_pos.loss : 1.1233161687850952
outputs_pos.loss : 1.359933853149414
outputs_pos.loss : 0.9648638963699341
outputs_pos.loss : 1.086449146270752
outputs_pos.loss : 1.191077709197998
outputs_pos.loss : 1.3750096559524536
Epoch 00358: adjusting learning rate of group 0 to 2.4921e-06.
outputs_pos.loss : 1.0333970785140991
outputs_pos.loss : 1.287514090538025
outputs_pos.loss : 1.3347070217132568
outputs_pos.loss : 1.121911883354187
outputs_pos.loss : 1.3990787267684937
outputs_pos.loss : 0.9982618093490601
outputs_pos.loss : 0.9463107585906982
outputs_pos.loss : 1.3977898359298706
Epoch 00359: adjusting learning rate of group 0 to 2.4921e-06.
outputs_pos.loss : 1.7280924320220947
outputs_pos.loss : 1.3073045015335083
outputs_pos.loss : 1.7599180936813354
outputs_pos.loss : 1.2100485563278198
outputs_pos.loss : 1.2119742631912231
outputs_pos.loss : 1.3758269548416138
outputs_pos.loss : 1.0448338985443115
outputs_pos.loss : 0.7561020255088806
Epoch 00360: adjusting learning rate of group 0 to 2.4920e-06.
outputs_pos.loss : 0.823174238204956
outputs_pos.loss : 1.17824387550354
outputs_pos.loss : 1.3048874139785767
outputs_pos.loss : 1.2020505666732788
outputs_pos.loss : 1.182724952697754
outputs_pos.loss : 0.5686326622962952
outputs_pos.loss : 1.1310769319534302
outputs_pos.loss : 1.0234242677688599
Epoch 00361: adjusting learning rate of group 0 to 2.4920e-06.
outputs_pos.loss : 1.5458111763000488
outputs_pos.loss : 1.6723233461380005
outputs_pos.loss : 0.7961307764053345
outputs_pos.loss : 1.3199328184127808
outputs_pos.loss : 1.1249958276748657
outputs_pos.loss : 1.3220927715301514
outputs_pos.loss : 0.7137910723686218
outputs_pos.loss : 1.3633421659469604
Epoch 00362: adjusting learning rate of group 0 to 2.4919e-06.
outputs_pos.loss : 1.4477427005767822
outputs_pos.loss : 1.136290431022644
outputs_pos.loss : 1.7328064441680908
outputs_pos.loss : 0.6842467784881592
outputs_pos.loss : 1.0295573472976685
outputs_pos.loss : 1.395739197731018
outputs_pos.loss : 0.8564602732658386
outputs_pos.loss : 1.0727016925811768
Epoch 00363: adjusting learning rate of group 0 to 2.4919e-06.
outputs_pos.loss : 1.7900186777114868
outputs_pos.loss : 1.394946813583374
outputs_pos.loss : 1.4275251626968384
outputs_pos.loss : 1.3457818031311035
outputs_pos.loss : 1.924155592918396
outputs_pos.loss : 1.5034703016281128
outputs_pos.loss : 1.3440971374511719
outputs_pos.loss : 0.978935956954956
Epoch 00364: adjusting learning rate of group 0 to 2.4918e-06.
outputs_pos.loss : 1.7937872409820557
outputs_pos.loss : 1.2077898979187012
outputs_pos.loss : 1.4069935083389282
outputs_pos.loss : 0.8399562239646912
outputs_pos.loss : 1.1589598655700684
outputs_pos.loss : 0.8425298929214478
outputs_pos.loss : 0.9346339106559753
outputs_pos.loss : 0.6958825588226318
Epoch 00365: adjusting learning rate of group 0 to 2.4918e-06.
outputs_pos.loss : 1.3856441974639893
outputs_pos.loss : 1.2103468179702759
outputs_pos.loss : 1.1346698999404907
outputs_pos.loss : 1.1327924728393555
outputs_pos.loss : 1.1493818759918213
outputs_pos.loss : 1.2925726175308228
outputs_pos.loss : 0.9181749820709229
outputs_pos.loss : 0.9196043014526367
Epoch 00366: adjusting learning rate of group 0 to 2.4917e-06.
outputs_pos.loss : 1.424071192741394
outputs_pos.loss : 1.1843767166137695
outputs_pos.loss : 0.8811777830123901
outputs_pos.loss : 1.3294085264205933
outputs_pos.loss : 1.0645674467086792
outputs_pos.loss : 1.5928137302398682
outputs_pos.loss : 1.365111231803894
outputs_pos.loss : 1.3743821382522583
Epoch 00367: adjusting learning rate of group 0 to 2.4917e-06.
outputs_pos.loss : 1.376399278640747
outputs_pos.loss : 1.6828210353851318
outputs_pos.loss : 1.2800604104995728
outputs_pos.loss : 1.2094333171844482
outputs_pos.loss : 1.0366145372390747
outputs_pos.loss : 1.1553295850753784
outputs_pos.loss : 1.2407029867172241
outputs_pos.loss : 0.9143262505531311
Epoch 00368: adjusting learning rate of group 0 to 2.4917e-06.
outputs_pos.loss : 1.0292458534240723
outputs_pos.loss : 1.756714105606079
outputs_pos.loss : 1.0531009435653687
outputs_pos.loss : 1.2264516353607178
outputs_pos.loss : 1.1572375297546387
outputs_pos.loss : 1.8013741970062256
outputs_pos.loss : 1.0086075067520142
outputs_pos.loss : 1.2505481243133545
Epoch 00369: adjusting learning rate of group 0 to 2.4916e-06.
outputs_pos.loss : 1.2972238063812256
outputs_pos.loss : 1.3112928867340088
outputs_pos.loss : 1.1110942363739014
outputs_pos.loss : 1.198448657989502
outputs_pos.loss : 1.205415964126587
outputs_pos.loss : 1.0916669368743896
outputs_pos.loss : 1.311163306236267
outputs_pos.loss : 1.3834210634231567
Epoch 00370: adjusting learning rate of group 0 to 2.4916e-06.
outputs_pos.loss : 1.239955186843872
outputs_pos.loss : 1.2819942235946655
outputs_pos.loss : 1.304755449295044
outputs_pos.loss : 1.3088696002960205
outputs_pos.loss : 1.342104196548462
outputs_pos.loss : 1.6064873933792114
outputs_pos.loss : 1.4399553537368774
outputs_pos.loss : 0.8147248029708862
Epoch 00371: adjusting learning rate of group 0 to 2.4915e-06.
outputs_pos.loss : 1.2217426300048828
outputs_pos.loss : 1.2722395658493042
outputs_pos.loss : 1.641401767730713
outputs_pos.loss : 1.3012404441833496
outputs_pos.loss : 1.1911801099777222
outputs_pos.loss : 1.3325386047363281
outputs_pos.loss : 1.0484963655471802
outputs_pos.loss : 1.9648240804672241
Epoch 00372: adjusting learning rate of group 0 to 2.4915e-06.
outputs_pos.loss : 1.1474430561065674
outputs_pos.loss : 1.3215316534042358
outputs_pos.loss : 1.592317819595337
outputs_pos.loss : 1.2329977750778198
outputs_pos.loss : 0.8936418294906616
outputs_pos.loss : 1.5208877325057983
outputs_pos.loss : 1.4456108808517456
outputs_pos.loss : 1.3779247999191284
Epoch 00373: adjusting learning rate of group 0 to 2.4914e-06.
outputs_pos.loss : 1.590011477470398
outputs_pos.loss : 1.239115834236145
outputs_pos.loss : 1.6674147844314575
outputs_pos.loss : 1.319919228553772
outputs_pos.loss : 0.8704608082771301
outputs_pos.loss : 1.124176263809204
outputs_pos.loss : 1.7076441049575806
outputs_pos.loss : 1.0230766534805298
Epoch 00374: adjusting learning rate of group 0 to 2.4914e-06.
outputs_pos.loss : 1.6917510032653809
outputs_pos.loss : 1.3425861597061157
outputs_pos.loss : 1.0264065265655518
outputs_pos.loss : 1.4886702299118042
outputs_pos.loss : 0.9029424786567688
outputs_pos.loss : 1.3890783786773682
outputs_pos.loss : 0.9707802534103394
outputs_pos.loss : 0.8410075306892395
Epoch 00375: adjusting learning rate of group 0 to 2.4913e-06.
outputs_pos.loss : 1.264385461807251
outputs_pos.loss : 1.6051045656204224
outputs_pos.loss : 1.0032975673675537
outputs_pos.loss : 1.0563899278640747
outputs_pos.loss : 1.0769811868667603
outputs_pos.loss : 1.436692476272583
outputs_pos.loss : 1.7706573009490967
outputs_pos.loss : 1.0453966856002808
Epoch 00376: adjusting learning rate of group 0 to 2.4913e-06.
outputs_pos.loss : 1.174296259880066
outputs_pos.loss : 1.2139933109283447
outputs_pos.loss : 1.0872846841812134
outputs_pos.loss : 1.3984285593032837
outputs_pos.loss : 1.3265111446380615
outputs_pos.loss : 1.7438780069351196
outputs_pos.loss : 1.3308746814727783
outputs_pos.loss : 1.0212041139602661
Epoch 00377: adjusting learning rate of group 0 to 2.4912e-06.
outputs_pos.loss : 2.0158588886260986
outputs_pos.loss : 1.0568060874938965
outputs_pos.loss : 3.2145442962646484
outputs_pos.loss : 1.1446069478988647
outputs_pos.loss : 0.7821093797683716
outputs_pos.loss : 1.3695155382156372
outputs_pos.loss : 1.3950364589691162
outputs_pos.loss : 1.4487695693969727
Epoch 00378: adjusting learning rate of group 0 to 2.4912e-06.
outputs_pos.loss : 1.7755953073501587
outputs_pos.loss : 1.057366132736206
outputs_pos.loss : 1.2108025550842285
outputs_pos.loss : 0.9048511981964111
outputs_pos.loss : 0.7712384462356567
outputs_pos.loss : 1.9887782335281372
outputs_pos.loss : 1.2349146604537964
outputs_pos.loss : 1.0350499153137207
Epoch 00379: adjusting learning rate of group 0 to 2.4911e-06.
outputs_pos.loss : 1.1354527473449707
outputs_pos.loss : 1.449506163597107
outputs_pos.loss : 1.120736002922058
outputs_pos.loss : 1.2508130073547363
outputs_pos.loss : 1.3905975818634033
outputs_pos.loss : 1.1432642936706543
outputs_pos.loss : 1.116650938987732
outputs_pos.loss : 0.9275045394897461
Epoch 00380: adjusting learning rate of group 0 to 2.4911e-06.
outputs_pos.loss : 1.5785025358200073
outputs_pos.loss : 1.6333388090133667
outputs_pos.loss : 1.2489458322525024
outputs_pos.loss : 2.0851385593414307
outputs_pos.loss : 1.4678767919540405
outputs_pos.loss : 1.0865615606307983
outputs_pos.loss : 1.0654653310775757
outputs_pos.loss : 1.2482483386993408
Epoch 00381: adjusting learning rate of group 0 to 2.4911e-06.
outputs_pos.loss : 1.7697503566741943
outputs_pos.loss : 1.2871805429458618
outputs_pos.loss : 1.1663503646850586
outputs_pos.loss : 1.1013073921203613
outputs_pos.loss : 1.2795789241790771
outputs_pos.loss : 0.7129747271537781
outputs_pos.loss : 1.3676402568817139
outputs_pos.loss : 1.3968321084976196
Epoch 00382: adjusting learning rate of group 0 to 2.4910e-06.
outputs_pos.loss : 1.218509554862976
outputs_pos.loss : 1.1914199590682983
outputs_pos.loss : 1.301114559173584
outputs_pos.loss : 1.2577601671218872
outputs_pos.loss : 1.288131833076477
outputs_pos.loss : 1.0802334547042847
outputs_pos.loss : 1.5878880023956299
outputs_pos.loss : 1.3999497890472412
Epoch 00383: adjusting learning rate of group 0 to 2.4910e-06.
outputs_pos.loss : 1.303736686706543
outputs_pos.loss : 1.3424961566925049
outputs_pos.loss : 1.261911153793335
outputs_pos.loss : 1.1493030786514282
outputs_pos.loss : 1.3499704599380493
outputs_pos.loss : 0.6109529733657837
outputs_pos.loss : 0.9543356895446777
outputs_pos.loss : 1.0142481327056885
Epoch 00384: adjusting learning rate of group 0 to 2.4909e-06.
outputs_pos.loss : 1.3995839357376099
outputs_pos.loss : 1.483952522277832
outputs_pos.loss : 1.046515941619873
outputs_pos.loss : 1.6068800687789917
outputs_pos.loss : 1.0697579383850098
outputs_pos.loss : 1.7185076475143433
outputs_pos.loss : 1.320739507675171
outputs_pos.loss : 1.2279722690582275
Epoch 00385: adjusting learning rate of group 0 to 2.4909e-06.
outputs_pos.loss : 0.9725778698921204
outputs_pos.loss : 1.0120580196380615
outputs_pos.loss : 0.8965082168579102
outputs_pos.loss : 1.140830159187317
outputs_pos.loss : 0.8800624012947083
outputs_pos.loss : 0.9665704965591431
outputs_pos.loss : 1.5235788822174072
outputs_pos.loss : 1.3222354650497437
Epoch 00386: adjusting learning rate of group 0 to 2.4908e-06.
outputs_pos.loss : 1.0519870519638062
outputs_pos.loss : 1.3550472259521484
outputs_pos.loss : 1.2526507377624512
outputs_pos.loss : 0.9455756545066833
outputs_pos.loss : 1.1363774538040161
outputs_pos.loss : 1.0077998638153076
outputs_pos.loss : 1.8171130418777466
outputs_pos.loss : 1.2158329486846924
Epoch 00387: adjusting learning rate of group 0 to 2.4908e-06.
outputs_pos.loss : 1.165820837020874
outputs_pos.loss : 0.740764856338501
outputs_pos.loss : 1.2957228422164917
outputs_pos.loss : 1.6082894802093506
outputs_pos.loss : 1.052713394165039
outputs_pos.loss : 1.0300469398498535
outputs_pos.loss : 1.0357427597045898
outputs_pos.loss : 1.0976672172546387
Epoch 00388: adjusting learning rate of group 0 to 2.4907e-06.
outputs_pos.loss : 1.4234962463378906
outputs_pos.loss : 0.918294370174408
outputs_pos.loss : 0.9048511385917664
outputs_pos.loss : 1.238427996635437
outputs_pos.loss : 1.2236686944961548
outputs_pos.loss : 1.567520022392273
outputs_pos.loss : 1.0972293615341187
outputs_pos.loss : 0.5674005746841431
Epoch 00389: adjusting learning rate of group 0 to 2.4907e-06.
outputs_pos.loss : 1.175168514251709
outputs_pos.loss : 1.7332278490066528
outputs_pos.loss : 1.0765352249145508
outputs_pos.loss : 1.2119152545928955
outputs_pos.loss : 1.3785533905029297
outputs_pos.loss : 1.2501108646392822
outputs_pos.loss : 0.7773054242134094
outputs_pos.loss : 1.226723074913025
Epoch 00390: adjusting learning rate of group 0 to 2.4906e-06.
outputs_pos.loss : 1.1238752603530884
outputs_pos.loss : 1.4846625328063965
outputs_pos.loss : 1.2198467254638672
outputs_pos.loss : 1.1374582052230835
outputs_pos.loss : 1.3699580430984497
outputs_pos.loss : 0.925693154335022
outputs_pos.loss : 1.3139078617095947
outputs_pos.loss : 1.3276011943817139
Epoch 00391: adjusting learning rate of group 0 to 2.4906e-06.
outputs_pos.loss : 1.1758464574813843
outputs_pos.loss : 1.1505827903747559
outputs_pos.loss : 1.1502463817596436
outputs_pos.loss : 1.5052040815353394
outputs_pos.loss : 1.2537261247634888
outputs_pos.loss : 1.1861369609832764
outputs_pos.loss : 2.178635358810425
outputs_pos.loss : 1.336567997932434
Epoch 00392: adjusting learning rate of group 0 to 2.4905e-06.
outputs_pos.loss : 0.9384351372718811
outputs_pos.loss : 1.3229460716247559
outputs_pos.loss : 1.4423818588256836
outputs_pos.loss : 1.3248672485351562
outputs_pos.loss : 1.4712105989456177
outputs_pos.loss : 0.8738666772842407
outputs_pos.loss : 1.8749445676803589
outputs_pos.loss : 1.161176085472107
Epoch 00393: adjusting learning rate of group 0 to 2.4905e-06.
outputs_pos.loss : 0.8085143566131592
outputs_pos.loss : 1.143548607826233
outputs_pos.loss : 1.0996886491775513
outputs_pos.loss : 1.141825795173645
outputs_pos.loss : 1.0050979852676392
outputs_pos.loss : 1.2530757188796997
outputs_pos.loss : 0.9891486763954163
outputs_pos.loss : 1.5913422107696533
Epoch 00394: adjusting learning rate of group 0 to 2.4904e-06.
outputs_pos.loss : 1.185251235961914
outputs_pos.loss : 1.66056489944458
outputs_pos.loss : 1.3267027139663696
outputs_pos.loss : 1.3991221189498901
outputs_pos.loss : 1.184609293937683
outputs_pos.loss : 0.978321373462677
outputs_pos.loss : 1.2069320678710938
outputs_pos.loss : 0.9318084716796875
Epoch 00395: adjusting learning rate of group 0 to 2.4904e-06.
outputs_pos.loss : 1.40888249874115
outputs_pos.loss : 1.78871488571167
outputs_pos.loss : 1.1403249502182007
outputs_pos.loss : 1.392619252204895
outputs_pos.loss : 1.1209439039230347
outputs_pos.loss : 0.8792021870613098
outputs_pos.loss : 1.1176496744155884
outputs_pos.loss : 1.283064603805542
Epoch 00396: adjusting learning rate of group 0 to 2.4903e-06.
outputs_pos.loss : 1.2871750593185425
outputs_pos.loss : 1.7365895509719849
outputs_pos.loss : 1.1043645143508911
outputs_pos.loss : 1.391158938407898
outputs_pos.loss : 1.201482892036438
outputs_pos.loss : 1.2597402334213257
outputs_pos.loss : 1.175217628479004
outputs_pos.loss : 0.6787773966789246
Epoch 00397: adjusting learning rate of group 0 to 2.4903e-06.
outputs_pos.loss : 1.2211707830429077
outputs_pos.loss : 1.1025832891464233
outputs_pos.loss : 1.3008323907852173
outputs_pos.loss : 1.2151968479156494
outputs_pos.loss : 1.3634803295135498
outputs_pos.loss : 1.1581811904907227
outputs_pos.loss : 1.4579052925109863
outputs_pos.loss : 0.8725938200950623
Epoch 00398: adjusting learning rate of group 0 to 2.4902e-06.
outputs_pos.loss : 1.8303112983703613
outputs_pos.loss : 1.407995581626892
outputs_pos.loss : 1.1087301969528198
outputs_pos.loss : 1.5924736261367798
outputs_pos.loss : 1.5865790843963623
outputs_pos.loss : 1.5484859943389893
outputs_pos.loss : 1.0614440441131592
outputs_pos.loss : 1.5805381536483765
Epoch 00399: adjusting learning rate of group 0 to 2.4902e-06.
outputs_pos.loss : 1.093315839767456
outputs_pos.loss : 1.1478369235992432
outputs_pos.loss : 1.3160759210586548
outputs_pos.loss : 0.7094268798828125
outputs_pos.loss : 1.496924877166748
outputs_pos.loss : 1.3299026489257812
outputs_pos.loss : 1.3779070377349854
outputs_pos.loss : 1.2054405212402344
Epoch 00400: adjusting learning rate of group 0 to 2.4901e-06.
outputs_pos.loss : 1.3652621507644653
outputs_pos.loss : 1.0649840831756592
outputs_pos.loss : 1.7627785205841064
outputs_pos.loss : 0.7407774925231934
outputs_pos.loss : 1.4675332307815552
outputs_pos.loss : 1.165428876876831
outputs_pos.loss : 1.3102128505706787
outputs_pos.loss : 1.8264206647872925
Epoch 00401: adjusting learning rate of group 0 to 2.4901e-06.
outputs_pos.loss : 0.9543607831001282
outputs_pos.loss : 1.9432318210601807
outputs_pos.loss : 2.0090198516845703
outputs_pos.loss : 1.3776029348373413
outputs_pos.loss : 1.0483134984970093
outputs_pos.loss : 1.189822793006897
outputs_pos.loss : 0.7084261178970337
outputs_pos.loss : 0.9527933597564697
Epoch 00402: adjusting learning rate of group 0 to 2.4900e-06.
outputs_pos.loss : 1.1372171640396118
outputs_pos.loss : 1.475748896598816
outputs_pos.loss : 1.0578888654708862
outputs_pos.loss : 1.4546047449111938
outputs_pos.loss : 1.2765109539031982
outputs_pos.loss : 1.2363992929458618
outputs_pos.loss : 1.338344931602478
outputs_pos.loss : 1.232245683670044
Epoch 00403: adjusting learning rate of group 0 to 2.4900e-06.
outputs_pos.loss : 0.7532601952552795
outputs_pos.loss : 1.6555900573730469
outputs_pos.loss : 1.5094696283340454
outputs_pos.loss : 1.0680850744247437
outputs_pos.loss : 0.9217039942741394
outputs_pos.loss : 1.7856719493865967
outputs_pos.loss : 1.5115621089935303
outputs_pos.loss : 1.4154560565948486
Epoch 00404: adjusting learning rate of group 0 to 2.4899e-06.
outputs_pos.loss : 1.27280855178833
outputs_pos.loss : 1.2142987251281738
outputs_pos.loss : 1.32086980342865
outputs_pos.loss : 1.132530927658081
outputs_pos.loss : 1.4366341829299927
outputs_pos.loss : 0.8341067433357239
outputs_pos.loss : 1.3538694381713867
outputs_pos.loss : 1.830603003501892
Epoch 00405: adjusting learning rate of group 0 to 2.4899e-06.
outputs_pos.loss : 1.632895588874817
outputs_pos.loss : 1.3681849241256714
outputs_pos.loss : 0.9777156114578247
outputs_pos.loss : 1.2132827043533325
outputs_pos.loss : 1.2484583854675293
outputs_pos.loss : 0.9871668219566345
outputs_pos.loss : 0.9157276749610901
outputs_pos.loss : 1.1263818740844727
Epoch 00406: adjusting learning rate of group 0 to 2.4898e-06.
outputs_pos.loss : 1.4999868869781494
outputs_pos.loss : 1.2200143337249756
outputs_pos.loss : 1.2152810096740723
outputs_pos.loss : 1.339647650718689
outputs_pos.loss : 0.7627980709075928
outputs_pos.loss : 1.364662528038025
outputs_pos.loss : 1.5613397359848022
outputs_pos.loss : 1.0460283756256104
Epoch 00407: adjusting learning rate of group 0 to 2.4898e-06.
outputs_pos.loss : 1.1258984804153442
outputs_pos.loss : 1.0162382125854492
outputs_pos.loss : 0.9845120906829834
outputs_pos.loss : 1.074548363685608
outputs_pos.loss : 0.8897923827171326
outputs_pos.loss : 1.0546207427978516
outputs_pos.loss : 1.1076263189315796
outputs_pos.loss : 1.221486210823059
Epoch 00408: adjusting learning rate of group 0 to 2.4897e-06.
outputs_pos.loss : 1.0968133211135864
outputs_pos.loss : 1.1064318418502808
outputs_pos.loss : 1.7527202367782593
outputs_pos.loss : 1.008794903755188
outputs_pos.loss : 1.3604642152786255
outputs_pos.loss : 0.9721081256866455
outputs_pos.loss : 1.3554301261901855
outputs_pos.loss : 0.816900372505188
Epoch 00409: adjusting learning rate of group 0 to 2.4897e-06.
outputs_pos.loss : 1.1992565393447876
outputs_pos.loss : 1.1651431322097778
outputs_pos.loss : 1.2522422075271606
outputs_pos.loss : 1.2348220348358154
outputs_pos.loss : 1.3477429151535034
outputs_pos.loss : 1.275092601776123
outputs_pos.loss : 1.3053447008132935
outputs_pos.loss : 1.2920869588851929
Epoch 00410: adjusting learning rate of group 0 to 2.4896e-06.
outputs_pos.loss : 0.9953517913818359
outputs_pos.loss : 1.0092309713363647
outputs_pos.loss : 1.3564233779907227
outputs_pos.loss : 1.429696798324585
outputs_pos.loss : 1.014818549156189
outputs_pos.loss : 1.0702086687088013
outputs_pos.loss : 1.1673448085784912
outputs_pos.loss : 1.1529483795166016
Epoch 00411: adjusting learning rate of group 0 to 2.4896e-06.
outputs_pos.loss : 0.9146345853805542
outputs_pos.loss : 1.3662782907485962
outputs_pos.loss : 1.4186111688613892
outputs_pos.loss : 1.3986916542053223
outputs_pos.loss : 1.396703839302063
outputs_pos.loss : 1.0082036256790161
outputs_pos.loss : 1.2353101968765259
outputs_pos.loss : 1.1622121334075928
Epoch 00412: adjusting learning rate of group 0 to 2.4895e-06.
outputs_pos.loss : 1.08231520652771
outputs_pos.loss : 1.0276522636413574
outputs_pos.loss : 1.9312376976013184
outputs_pos.loss : 1.338173508644104
outputs_pos.loss : 0.9153118133544922
outputs_pos.loss : 1.212489128112793
outputs_pos.loss : 0.8671689629554749
outputs_pos.loss : 1.1725841760635376
Epoch 00413: adjusting learning rate of group 0 to 2.4895e-06.
outputs_pos.loss : 1.396301507949829
outputs_pos.loss : 1.3281947374343872
outputs_pos.loss : 1.8731863498687744
outputs_pos.loss : 0.947948694229126
outputs_pos.loss : 1.3148285150527954
outputs_pos.loss : 1.3554013967514038
outputs_pos.loss : 0.8867508769035339
outputs_pos.loss : 1.5245022773742676
Epoch 00414: adjusting learning rate of group 0 to 2.4894e-06.
outputs_pos.loss : 1.4879084825515747
outputs_pos.loss : 0.7878915071487427
outputs_pos.loss : 1.327967643737793
outputs_pos.loss : 1.1094716787338257
outputs_pos.loss : 1.4067384004592896
outputs_pos.loss : 0.9761984944343567
outputs_pos.loss : 1.2116997241973877
outputs_pos.loss : 1.3135415315628052
Epoch 00415: adjusting learning rate of group 0 to 2.4894e-06.
outputs_pos.loss : 1.8514540195465088
outputs_pos.loss : 1.2395719289779663
outputs_pos.loss : 1.0331940650939941
outputs_pos.loss : 0.9437759518623352
outputs_pos.loss : 1.2179346084594727
outputs_pos.loss : 1.2350164651870728
outputs_pos.loss : 1.8748366832733154
outputs_pos.loss : 1.2725398540496826
Epoch 00416: adjusting learning rate of group 0 to 2.4893e-06.
outputs_pos.loss : 1.6141360998153687
outputs_pos.loss : 1.519101858139038
outputs_pos.loss : 1.1870779991149902
outputs_pos.loss : 1.321342945098877
outputs_pos.loss : 1.345703363418579
outputs_pos.loss : 0.8109959363937378
outputs_pos.loss : 1.053281307220459
outputs_pos.loss : 0.7968848943710327
Epoch 00417: adjusting learning rate of group 0 to 2.4893e-06.
outputs_pos.loss : 1.2438864707946777
outputs_pos.loss : 0.8338183164596558
outputs_pos.loss : 1.3879202604293823
outputs_pos.loss : 1.2977179288864136
outputs_pos.loss : 1.2138904333114624
outputs_pos.loss : 1.6002017259597778
outputs_pos.loss : 1.5095669031143188
outputs_pos.loss : 1.4566913843154907
Epoch 00418: adjusting learning rate of group 0 to 2.4892e-06.
outputs_pos.loss : 1.5027410984039307
outputs_pos.loss : 1.216132402420044
outputs_pos.loss : 1.2082265615463257
outputs_pos.loss : 0.8313297033309937
outputs_pos.loss : 1.6114248037338257
outputs_pos.loss : 0.8794512152671814
outputs_pos.loss : 1.351715326309204
outputs_pos.loss : 1.1872721910476685
Epoch 00419: adjusting learning rate of group 0 to 2.4892e-06.
outputs_pos.loss : 0.7911978960037231
outputs_pos.loss : 1.2718249559402466
outputs_pos.loss : 1.2499072551727295
outputs_pos.loss : 1.0623995065689087
outputs_pos.loss : 1.2213324308395386
outputs_pos.loss : 1.2406702041625977
outputs_pos.loss : 1.0389130115509033
outputs_pos.loss : 1.926912546157837
Epoch 00420: adjusting learning rate of group 0 to 2.4891e-06.
outputs_pos.loss : 1.2689132690429688
outputs_pos.loss : 1.644138216972351
outputs_pos.loss : 1.813546061515808
outputs_pos.loss : 1.103042721748352
outputs_pos.loss : 1.1084779500961304
outputs_pos.loss : 1.451625108718872
outputs_pos.loss : 1.2215938568115234
outputs_pos.loss : 1.16056227684021
Epoch 00421: adjusting learning rate of group 0 to 2.4891e-06.
outputs_pos.loss : 1.1565276384353638
outputs_pos.loss : 1.1203439235687256
outputs_pos.loss : 0.947295069694519
outputs_pos.loss : 1.1807286739349365
outputs_pos.loss : 1.1653627157211304
outputs_pos.loss : 2.2100412845611572
outputs_pos.loss : 2.0260162353515625
outputs_pos.loss : 1.4657015800476074
Epoch 00422: adjusting learning rate of group 0 to 2.4890e-06.
outputs_pos.loss : 1.8964858055114746
outputs_pos.loss : 1.2306792736053467
outputs_pos.loss : 0.9181686639785767
outputs_pos.loss : 1.2920857667922974
outputs_pos.loss : 0.8667804002761841
outputs_pos.loss : 1.2449895143508911
outputs_pos.loss : 1.033782720565796
outputs_pos.loss : 1.4250702857971191
Epoch 00423: adjusting learning rate of group 0 to 2.4890e-06.
outputs_pos.loss : 2.128183603286743
outputs_pos.loss : 1.5910465717315674
outputs_pos.loss : 0.8777618408203125
outputs_pos.loss : 2.07448410987854
outputs_pos.loss : 1.9587266445159912
outputs_pos.loss : 1.2458256483078003
outputs_pos.loss : 1.082078456878662
outputs_pos.loss : 0.5364173650741577
Epoch 00424: adjusting learning rate of group 0 to 2.4889e-06.
outputs_pos.loss : 1.5765429735183716
outputs_pos.loss : 1.4759137630462646
outputs_pos.loss : 1.091296672821045
outputs_pos.loss : 1.263213038444519
outputs_pos.loss : 1.518462061882019
outputs_pos.loss : 0.8005712032318115
outputs_pos.loss : 1.3311867713928223
outputs_pos.loss : 1.1200106143951416
Epoch 00425: adjusting learning rate of group 0 to 2.4889e-06.
outputs_pos.loss : 1.1252552270889282
outputs_pos.loss : 1.1711772680282593
outputs_pos.loss : 0.8528108596801758
outputs_pos.loss : 1.5080821514129639
outputs_pos.loss : 1.2419058084487915
outputs_pos.loss : 1.2353594303131104
outputs_pos.loss : 1.3494452238082886
outputs_pos.loss : 1.4648953676223755
Epoch 00426: adjusting learning rate of group 0 to 2.4888e-06.
outputs_pos.loss : 1.4820514917373657
outputs_pos.loss : 0.8988115191459656
outputs_pos.loss : 1.080626368522644
outputs_pos.loss : 1.2756285667419434
outputs_pos.loss : 1.0386792421340942
outputs_pos.loss : 1.3886722326278687
outputs_pos.loss : 1.1214845180511475
outputs_pos.loss : 1.269744634628296
Epoch 00427: adjusting learning rate of group 0 to 2.4888e-06.
outputs_pos.loss : 1.4181045293807983
outputs_pos.loss : 1.147394061088562
outputs_pos.loss : 0.7523640990257263
outputs_pos.loss : 1.197792649269104
outputs_pos.loss : 1.07659113407135
outputs_pos.loss : 1.6874587535858154
outputs_pos.loss : 1.1021718978881836
outputs_pos.loss : 1.3533776998519897
Epoch 00428: adjusting learning rate of group 0 to 2.4887e-06.
outputs_pos.loss : 1.2943729162216187
outputs_pos.loss : 1.4271212816238403
outputs_pos.loss : 1.3045661449432373
outputs_pos.loss : 1.2368091344833374
outputs_pos.loss : 1.1011091470718384
outputs_pos.loss : 1.4770382642745972
outputs_pos.loss : 1.1846903562545776
outputs_pos.loss : 1.0790187120437622
Epoch 00429: adjusting learning rate of group 0 to 2.4887e-06.
outputs_pos.loss : 1.024916410446167
outputs_pos.loss : 1.3558107614517212
outputs_pos.loss : 1.244240403175354
outputs_pos.loss : 1.3343573808670044
outputs_pos.loss : 1.1273515224456787
outputs_pos.loss : 1.3669971227645874
outputs_pos.loss : 0.9117733836174011
outputs_pos.loss : 1.1050184965133667
Epoch 00430: adjusting learning rate of group 0 to 2.4886e-06.
outputs_pos.loss : 1.3919191360473633
outputs_pos.loss : 1.063925862312317
outputs_pos.loss : 1.3701635599136353
outputs_pos.loss : 2.0106492042541504
outputs_pos.loss : 1.6648067235946655
outputs_pos.loss : 1.1528654098510742
outputs_pos.loss : 1.51918625831604
outputs_pos.loss : 0.696811854839325
Epoch 00431: adjusting learning rate of group 0 to 2.4886e-06.
outputs_pos.loss : 1.3080737590789795
outputs_pos.loss : 1.1102632284164429
outputs_pos.loss : 1.4080231189727783
outputs_pos.loss : 1.6858837604522705
outputs_pos.loss : 1.0549589395523071
outputs_pos.loss : 1.3509540557861328
outputs_pos.loss : 0.8557644486427307
outputs_pos.loss : 0.9108272194862366
Epoch 00432: adjusting learning rate of group 0 to 2.4885e-06.
outputs_pos.loss : 1.6002933979034424
outputs_pos.loss : 1.5764344930648804
outputs_pos.loss : 0.8927064538002014
outputs_pos.loss : 1.2499487400054932
outputs_pos.loss : 1.4646369218826294
outputs_pos.loss : 1.4361695051193237
outputs_pos.loss : 0.9092127680778503
outputs_pos.loss : 1.5370821952819824
Epoch 00433: adjusting learning rate of group 0 to 2.4885e-06.
outputs_pos.loss : 1.1642776727676392
outputs_pos.loss : 1.121490478515625
outputs_pos.loss : 1.0998778343200684
outputs_pos.loss : 1.3459457159042358
outputs_pos.loss : 1.055290699005127
outputs_pos.loss : 1.2927560806274414
outputs_pos.loss : 0.96497642993927
outputs_pos.loss : 1.2575639486312866
Epoch 00434: adjusting learning rate of group 0 to 2.4884e-06.
outputs_pos.loss : 1.0112825632095337
outputs_pos.loss : 0.9296100735664368
outputs_pos.loss : 1.4201796054840088
outputs_pos.loss : 2.033220052719116
outputs_pos.loss : 1.5312120914459229
outputs_pos.loss : 1.2383019924163818
outputs_pos.loss : 1.4690302610397339
outputs_pos.loss : 1.3479375839233398
Epoch 00435: adjusting learning rate of group 0 to 2.4883e-06.
outputs_pos.loss : 1.7693465948104858
outputs_pos.loss : 1.3803961277008057
outputs_pos.loss : 0.9970961809158325
outputs_pos.loss : 0.6725651621818542
outputs_pos.loss : 1.2146738767623901
outputs_pos.loss : 1.0309131145477295
outputs_pos.loss : 1.211242914199829
outputs_pos.loss : 0.9145671725273132
Epoch 00436: adjusting learning rate of group 0 to 2.4883e-06.
outputs_pos.loss : 1.5589476823806763
outputs_pos.loss : 1.195329189300537
outputs_pos.loss : 1.3709087371826172
outputs_pos.loss : 1.040717363357544
outputs_pos.loss : 1.0565686225891113
outputs_pos.loss : 0.6608229875564575
outputs_pos.loss : 1.152877688407898
outputs_pos.loss : 1.037088394165039
Epoch 00437: adjusting learning rate of group 0 to 2.4882e-06.
outputs_pos.loss : 1.0577906370162964
outputs_pos.loss : 1.4178073406219482
outputs_pos.loss : 1.0782158374786377
outputs_pos.loss : 1.1239619255065918
outputs_pos.loss : 1.1920733451843262
outputs_pos.loss : 1.1680757999420166
outputs_pos.loss : 0.7785096168518066
outputs_pos.loss : 1.6233144998550415
Epoch 00438: adjusting learning rate of group 0 to 2.4882e-06.
outputs_pos.loss : 1.0511900186538696
outputs_pos.loss : 0.9414275288581848
outputs_pos.loss : 1.621712565422058
outputs_pos.loss : 0.9883464574813843
outputs_pos.loss : 1.4600917100906372
outputs_pos.loss : 0.588987410068512
outputs_pos.loss : 1.0810202360153198
outputs_pos.loss : 1.196205735206604
Epoch 00439: adjusting learning rate of group 0 to 2.4881e-06.
outputs_pos.loss : 1.3220926523208618
outputs_pos.loss : 1.3572971820831299
outputs_pos.loss : 0.8986043334007263
outputs_pos.loss : 0.9969914555549622
outputs_pos.loss : 1.2117421627044678
outputs_pos.loss : 1.102927327156067
outputs_pos.loss : 1.0584803819656372
outputs_pos.loss : 1.1963289976119995
Epoch 00440: adjusting learning rate of group 0 to 2.4881e-06.
outputs_pos.loss : 1.1134511232376099
outputs_pos.loss : 1.3126980066299438
outputs_pos.loss : 1.4903311729431152
outputs_pos.loss : 1.0061463117599487
outputs_pos.loss : 1.5968838930130005
outputs_pos.loss : 1.0500741004943848
outputs_pos.loss : 0.9751250743865967
outputs_pos.loss : 0.9457076191902161
Epoch 00441: adjusting learning rate of group 0 to 2.4880e-06.
outputs_pos.loss : 1.2850277423858643
outputs_pos.loss : 1.254137635231018
outputs_pos.loss : 1.0794552564620972
outputs_pos.loss : 0.9981021881103516
outputs_pos.loss : 1.3153648376464844
outputs_pos.loss : 0.91611248254776
outputs_pos.loss : 1.1655938625335693
outputs_pos.loss : 1.291694164276123
Epoch 00442: adjusting learning rate of group 0 to 2.4880e-06.
outputs_pos.loss : 1.319253921508789
outputs_pos.loss : 0.9337213635444641
outputs_pos.loss : 1.4237786531448364
outputs_pos.loss : 1.2876315116882324
outputs_pos.loss : 1.199400782585144
outputs_pos.loss : 1.1120659112930298
outputs_pos.loss : 1.4975587129592896
outputs_pos.loss : 1.5477161407470703
Epoch 00443: adjusting learning rate of group 0 to 2.4879e-06.
outputs_pos.loss : 2.063703775405884
outputs_pos.loss : 1.4023317098617554
outputs_pos.loss : 1.6156882047653198
outputs_pos.loss : 1.3585542440414429
outputs_pos.loss : 1.0687860250473022
outputs_pos.loss : 1.2106996774673462
outputs_pos.loss : 1.2210066318511963
outputs_pos.loss : 1.31047785282135
Epoch 00444: adjusting learning rate of group 0 to 2.4879e-06.
outputs_pos.loss : 1.250173568725586
outputs_pos.loss : 0.8807212114334106
outputs_pos.loss : 1.6300817728042603
outputs_pos.loss : 1.1755077838897705
outputs_pos.loss : 0.923600435256958
outputs_pos.loss : 0.9776420593261719
outputs_pos.loss : 0.9631449580192566
outputs_pos.loss : 1.3890492916107178
Epoch 00445: adjusting learning rate of group 0 to 2.4878e-06.
outputs_pos.loss : 1.0590736865997314
outputs_pos.loss : 1.071587324142456
outputs_pos.loss : 1.983279824256897
outputs_pos.loss : 0.7328109741210938
outputs_pos.loss : 1.2679437398910522
outputs_pos.loss : 1.6130599975585938
outputs_pos.loss : 0.9863064885139465
outputs_pos.loss : 1.000697135925293
Epoch 00446: adjusting learning rate of group 0 to 2.4877e-06.
outputs_pos.loss : 1.5050667524337769
outputs_pos.loss : 1.7273268699645996
outputs_pos.loss : 1.013804316520691
outputs_pos.loss : 1.2304701805114746
outputs_pos.loss : 1.1218690872192383
outputs_pos.loss : 1.511047601699829
outputs_pos.loss : 1.6096227169036865
outputs_pos.loss : 0.811588704586029
Epoch 00447: adjusting learning rate of group 0 to 2.4877e-06.
outputs_pos.loss : 1.3811607360839844
outputs_pos.loss : 1.0957494974136353
outputs_pos.loss : 1.152327537536621
outputs_pos.loss : 1.8279470205307007
outputs_pos.loss : 1.240687370300293
outputs_pos.loss : 1.3204256296157837
outputs_pos.loss : 1.0433121919631958
outputs_pos.loss : 0.6771106123924255
Epoch 00448: adjusting learning rate of group 0 to 2.4876e-06.
outputs_pos.loss : 1.408387541770935
outputs_pos.loss : 1.0223290920257568
outputs_pos.loss : 0.9693372845649719
outputs_pos.loss : 1.0066874027252197
outputs_pos.loss : 1.2655247449874878
outputs_pos.loss : 1.7185194492340088
outputs_pos.loss : 1.1574934720993042
outputs_pos.loss : 1.192446231842041
Epoch 00449: adjusting learning rate of group 0 to 2.4876e-06.
outputs_pos.loss : 1.0217034816741943
outputs_pos.loss : 1.4190095663070679
outputs_pos.loss : 1.069366455078125
outputs_pos.loss : 1.039205551147461
outputs_pos.loss : 1.5995224714279175
outputs_pos.loss : 1.3548693656921387
outputs_pos.loss : 1.6568725109100342
outputs_pos.loss : 1.1268161535263062
Epoch 00450: adjusting learning rate of group 0 to 2.4875e-06.
outputs_pos.loss : 0.8478215336799622
outputs_pos.loss : 0.868394136428833
outputs_pos.loss : 2.003769874572754
outputs_pos.loss : 0.9863892197608948
outputs_pos.loss : 1.3877002000808716
outputs_pos.loss : 1.2793617248535156
outputs_pos.loss : 1.1492726802825928
outputs_pos.loss : 1.2989038228988647
Epoch 00451: adjusting learning rate of group 0 to 2.4875e-06.
outputs_pos.loss : 1.357753872871399
outputs_pos.loss : 1.1772651672363281
outputs_pos.loss : 1.5022311210632324
outputs_pos.loss : 1.0311980247497559
outputs_pos.loss : 1.1903331279754639
outputs_pos.loss : 0.9439535737037659
outputs_pos.loss : 1.4260519742965698
outputs_pos.loss : 1.3280524015426636
Epoch 00452: adjusting learning rate of group 0 to 2.4874e-06.
outputs_pos.loss : 1.4684689044952393
outputs_pos.loss : 1.0097544193267822
outputs_pos.loss : 1.1478261947631836
outputs_pos.loss : 1.5175304412841797
outputs_pos.loss : 1.2742078304290771
outputs_pos.loss : 1.0198099613189697
outputs_pos.loss : 1.0593558549880981
outputs_pos.loss : 0.8609728217124939
Epoch 00453: adjusting learning rate of group 0 to 2.4874e-06.
outputs_pos.loss : 1.3983384370803833
outputs_pos.loss : 1.3265994787216187
outputs_pos.loss : 1.366667628288269
outputs_pos.loss : 1.4383094310760498
outputs_pos.loss : 1.182221531867981
outputs_pos.loss : 1.256880283355713
outputs_pos.loss : 1.7371611595153809
outputs_pos.loss : 1.294910192489624
Epoch 00454: adjusting learning rate of group 0 to 2.4873e-06.
outputs_pos.loss : 1.0952235460281372
outputs_pos.loss : 1.2677923440933228
outputs_pos.loss : 0.8225130438804626
outputs_pos.loss : 1.5619748830795288
outputs_pos.loss : 0.9716313481330872
outputs_pos.loss : 1.0051320791244507
outputs_pos.loss : 1.5932296514511108
outputs_pos.loss : 1.049142599105835
Epoch 00455: adjusting learning rate of group 0 to 2.4873e-06.
outputs_pos.loss : 1.6685189008712769
outputs_pos.loss : 1.263075590133667
outputs_pos.loss : 1.380920648574829
outputs_pos.loss : 0.8774154782295227
outputs_pos.loss : 0.9791277647018433
outputs_pos.loss : 1.022747278213501
outputs_pos.loss : 0.937079906463623
outputs_pos.loss : 1.5442633628845215
Epoch 00456: adjusting learning rate of group 0 to 2.4872e-06.
outputs_pos.loss : 1.02186119556427
outputs_pos.loss : 1.2603298425674438
outputs_pos.loss : 0.989496648311615
outputs_pos.loss : 0.9120597839355469
outputs_pos.loss : 0.7253730893135071
outputs_pos.loss : 0.9405331015586853
outputs_pos.loss : 1.6551990509033203
outputs_pos.loss : 1.960303783416748
Epoch 00457: adjusting learning rate of group 0 to 2.4871e-06.
outputs_pos.loss : 1.2274816036224365
outputs_pos.loss : 1.102931022644043
outputs_pos.loss : 1.3976200819015503
outputs_pos.loss : 1.2183202505111694
outputs_pos.loss : 1.2733863592147827
outputs_pos.loss : 1.4258359670639038
outputs_pos.loss : 1.598952054977417
outputs_pos.loss : 1.4282212257385254
Epoch 00458: adjusting learning rate of group 0 to 2.4871e-06.
outputs_pos.loss : 1.5560088157653809
outputs_pos.loss : 1.3046865463256836
outputs_pos.loss : 0.934168815612793
outputs_pos.loss : 1.0588892698287964
outputs_pos.loss : 1.4483702182769775
outputs_pos.loss : 1.3526618480682373
outputs_pos.loss : 1.194898247718811
outputs_pos.loss : 0.7822665572166443
Epoch 00459: adjusting learning rate of group 0 to 2.4870e-06.
outputs_pos.loss : 0.8385610580444336
outputs_pos.loss : 1.2038112878799438
outputs_pos.loss : 1.3122851848602295
outputs_pos.loss : 1.4458630084991455
outputs_pos.loss : 1.1108428239822388
outputs_pos.loss : 1.1248677968978882
outputs_pos.loss : 1.2870557308197021
outputs_pos.loss : 0.9742581844329834
Epoch 00460: adjusting learning rate of group 0 to 2.4870e-06.
outputs_pos.loss : 1.1441516876220703
outputs_pos.loss : 1.2555968761444092
outputs_pos.loss : 1.0811704397201538
outputs_pos.loss : 1.2516734600067139
outputs_pos.loss : 1.0159945487976074
outputs_pos.loss : 1.749963641166687
outputs_pos.loss : 1.0519806146621704
outputs_pos.loss : 1.338869571685791
Epoch 00461: adjusting learning rate of group 0 to 2.4869e-06.
outputs_pos.loss : 1.4388388395309448
outputs_pos.loss : 2.025296926498413
outputs_pos.loss : 1.2598824501037598
outputs_pos.loss : 0.988982081413269
outputs_pos.loss : 0.9271330237388611
outputs_pos.loss : 1.218169927597046
outputs_pos.loss : 0.9982485771179199
outputs_pos.loss : 1.176898717880249
Epoch 00462: adjusting learning rate of group 0 to 2.4869e-06.
outputs_pos.loss : 1.6241687536239624
outputs_pos.loss : 1.373108148574829
outputs_pos.loss : 1.1446712017059326
outputs_pos.loss : 1.2804065942764282
outputs_pos.loss : 1.1314716339111328
outputs_pos.loss : 1.132869005203247
outputs_pos.loss : 1.8990271091461182
outputs_pos.loss : 1.0914413928985596
Epoch 00463: adjusting learning rate of group 0 to 2.4868e-06.
outputs_pos.loss : 1.0393555164337158
outputs_pos.loss : 1.6161772012710571
outputs_pos.loss : 1.2375575304031372
outputs_pos.loss : 1.2320098876953125
outputs_pos.loss : 1.1004197597503662
outputs_pos.loss : 1.7143607139587402
outputs_pos.loss : 1.4978708028793335
outputs_pos.loss : 0.754819393157959
Epoch 00464: adjusting learning rate of group 0 to 2.4867e-06.
outputs_pos.loss : 1.4185843467712402
outputs_pos.loss : 0.9511323571205139
outputs_pos.loss : 1.3163434267044067
outputs_pos.loss : 1.4631129503250122
outputs_pos.loss : 0.7231995463371277
outputs_pos.loss : 0.90627521276474
outputs_pos.loss : 0.8091487288475037
outputs_pos.loss : 1.023863673210144
Epoch 00465: adjusting learning rate of group 0 to 2.4867e-06.
outputs_pos.loss : 1.511939525604248
outputs_pos.loss : 1.048365592956543
outputs_pos.loss : 1.145499348640442
outputs_pos.loss : 0.9947717189788818
outputs_pos.loss : 1.4018807411193848
outputs_pos.loss : 1.3218071460723877
outputs_pos.loss : 1.5606228113174438
outputs_pos.loss : 0.8473377823829651
Epoch 00466: adjusting learning rate of group 0 to 2.4866e-06.
outputs_pos.loss : 0.8632042407989502
outputs_pos.loss : 1.0748196840286255
outputs_pos.loss : 1.4708772897720337
outputs_pos.loss : 1.5260893106460571
outputs_pos.loss : 1.3365610837936401
outputs_pos.loss : 1.161190390586853
outputs_pos.loss : 1.0588243007659912
outputs_pos.loss : 1.2415757179260254
Epoch 00467: adjusting learning rate of group 0 to 2.4866e-06.
outputs_pos.loss : 1.4312251806259155
outputs_pos.loss : 1.3583016395568848
outputs_pos.loss : 0.9103059768676758
outputs_pos.loss : 1.5137916803359985
outputs_pos.loss : 1.3913730382919312
outputs_pos.loss : 1.2647294998168945
outputs_pos.loss : 1.5405317544937134
outputs_pos.loss : 1.159741997718811
Epoch 00468: adjusting learning rate of group 0 to 2.4865e-06.
outputs_pos.loss : 1.539718747138977
outputs_pos.loss : 1.4669733047485352
outputs_pos.loss : 1.9320019483566284
outputs_pos.loss : 1.6585451364517212
outputs_pos.loss : 0.9987170100212097
outputs_pos.loss : 1.0605719089508057
outputs_pos.loss : 1.1207711696624756
outputs_pos.loss : 1.0403972864151
Epoch 00469: adjusting learning rate of group 0 to 2.4865e-06.
outputs_pos.loss : 0.9945595264434814
outputs_pos.loss : 1.1400648355484009
outputs_pos.loss : 1.5595372915267944
outputs_pos.loss : 1.3923083543777466
outputs_pos.loss : 1.1751360893249512
outputs_pos.loss : 1.8880726099014282
outputs_pos.loss : 1.657148003578186
outputs_pos.loss : 1.091224193572998
Epoch 00470: adjusting learning rate of group 0 to 2.4864e-06.
outputs_pos.loss : 0.9139301776885986
outputs_pos.loss : 1.794506549835205
outputs_pos.loss : 0.8768391013145447
outputs_pos.loss : 1.0370038747787476
outputs_pos.loss : 1.6332954168319702
outputs_pos.loss : 1.1592702865600586
outputs_pos.loss : 1.2039821147918701
outputs_pos.loss : 1.1730773448944092
Epoch 00471: adjusting learning rate of group 0 to 2.4863e-06.
outputs_pos.loss : 1.5829501152038574
outputs_pos.loss : 1.3321971893310547
outputs_pos.loss : 1.0615206956863403
outputs_pos.loss : 1.4847153425216675
outputs_pos.loss : 1.0599242448806763
outputs_pos.loss : 1.0004020929336548
outputs_pos.loss : 0.9491159319877625
outputs_pos.loss : 1.4229155778884888
Epoch 00472: adjusting learning rate of group 0 to 2.4863e-06.
outputs_pos.loss : 1.1441279649734497
outputs_pos.loss : 1.3509067296981812
outputs_pos.loss : 1.5575165748596191
outputs_pos.loss : 1.460111141204834
outputs_pos.loss : 0.9247798919677734
outputs_pos.loss : 1.4183695316314697
outputs_pos.loss : 0.9608705043792725
outputs_pos.loss : 0.9645017385482788
Epoch 00473: adjusting learning rate of group 0 to 2.4862e-06.
outputs_pos.loss : 1.1788486242294312
outputs_pos.loss : 1.6253923177719116
outputs_pos.loss : 0.7549348473548889
outputs_pos.loss : 1.070936918258667
outputs_pos.loss : 1.288246512413025
outputs_pos.loss : 1.147964596748352
outputs_pos.loss : 1.3293344974517822
outputs_pos.loss : 1.0109981298446655
Epoch 00474: adjusting learning rate of group 0 to 2.4862e-06.
outputs_pos.loss : 1.1623085737228394
outputs_pos.loss : 1.2064094543457031
outputs_pos.loss : 2.0464797019958496
outputs_pos.loss : 0.9654669761657715
outputs_pos.loss : 1.0500291585922241
outputs_pos.loss : 0.9278464317321777
outputs_pos.loss : 1.4476197957992554
outputs_pos.loss : 1.520206332206726
Epoch 00475: adjusting learning rate of group 0 to 2.4861e-06.
outputs_pos.loss : 0.9260088801383972
outputs_pos.loss : 0.747591495513916
outputs_pos.loss : 1.2903075218200684
outputs_pos.loss : 1.5294268131256104
outputs_pos.loss : 1.403441309928894
outputs_pos.loss : 0.9809322953224182
outputs_pos.loss : 1.3303335905075073
outputs_pos.loss : 1.0815051794052124
Epoch 00476: adjusting learning rate of group 0 to 2.4860e-06.
outputs_pos.loss : 1.3207460641860962
outputs_pos.loss : 0.8892173171043396
outputs_pos.loss : 1.24495267868042
outputs_pos.loss : 1.6034958362579346
outputs_pos.loss : 1.0841567516326904
outputs_pos.loss : 1.1908164024353027
outputs_pos.loss : 1.2729507684707642
outputs_pos.loss : 0.8328171968460083
Epoch 00477: adjusting learning rate of group 0 to 2.4860e-06.
outputs_pos.loss : 1.126948356628418
outputs_pos.loss : 1.4944825172424316
outputs_pos.loss : 1.5030503273010254
outputs_pos.loss : 1.4036033153533936
outputs_pos.loss : 1.4310168027877808
outputs_pos.loss : 1.3013978004455566
outputs_pos.loss : 1.1096218824386597
outputs_pos.loss : 1.0616694688796997
Epoch 00478: adjusting learning rate of group 0 to 2.4859e-06.
outputs_pos.loss : 1.2547067403793335
outputs_pos.loss : 1.3450541496276855
outputs_pos.loss : 1.1213279962539673
outputs_pos.loss : 1.1810991764068604
outputs_pos.loss : 1.2127373218536377
outputs_pos.loss : 1.2527786493301392
outputs_pos.loss : 1.087348461151123
outputs_pos.loss : 0.6472803950309753
Epoch 00479: adjusting learning rate of group 0 to 2.4859e-06.
outputs_pos.loss : 1.0656265020370483
outputs_pos.loss : 1.0783112049102783
outputs_pos.loss : 1.3669288158416748
outputs_pos.loss : 1.3603349924087524
outputs_pos.loss : 1.1347419023513794
outputs_pos.loss : 1.1918842792510986
outputs_pos.loss : 1.1991530656814575
outputs_pos.loss : 1.2104051113128662
Epoch 00480: adjusting learning rate of group 0 to 2.4858e-06.
outputs_pos.loss : 1.1281925439834595
outputs_pos.loss : 1.2837153673171997
outputs_pos.loss : 0.6973990797996521
outputs_pos.loss : 1.0875827074050903
outputs_pos.loss : 1.511082410812378
outputs_pos.loss : 1.0248204469680786
outputs_pos.loss : 0.9744025468826294
outputs_pos.loss : 1.1158994436264038
Epoch 00481: adjusting learning rate of group 0 to 2.4858e-06.
outputs_pos.loss : 1.3925751447677612
outputs_pos.loss : 1.3581196069717407
outputs_pos.loss : 1.483190655708313
outputs_pos.loss : 1.5659105777740479
outputs_pos.loss : 2.055237054824829
outputs_pos.loss : 1.4621200561523438
outputs_pos.loss : 0.767063319683075
outputs_pos.loss : 1.7180856466293335
Epoch 00482: adjusting learning rate of group 0 to 2.4857e-06.
outputs_pos.loss : 0.9475091099739075
outputs_pos.loss : 0.7962120175361633
outputs_pos.loss : 1.4077001810073853
outputs_pos.loss : 1.2863761186599731
outputs_pos.loss : 1.719752550125122
outputs_pos.loss : 1.150339126586914
outputs_pos.loss : 1.2950547933578491
outputs_pos.loss : 1.5850659608840942
Epoch 00483: adjusting learning rate of group 0 to 2.4856e-06.
outputs_pos.loss : 1.6975914239883423
outputs_pos.loss : 1.421584963798523
outputs_pos.loss : 0.9480876326560974
outputs_pos.loss : 0.8615288734436035
outputs_pos.loss : 1.1780223846435547
outputs_pos.loss : 2.013577938079834
outputs_pos.loss : 1.4023898839950562
outputs_pos.loss : 1.0859731435775757
Epoch 00484: adjusting learning rate of group 0 to 2.4856e-06.
outputs_pos.loss : 1.382177472114563
outputs_pos.loss : 1.7155382633209229
outputs_pos.loss : 1.1290720701217651
outputs_pos.loss : 0.837627649307251
outputs_pos.loss : 1.226340889930725
outputs_pos.loss : 1.338552474975586
outputs_pos.loss : 1.1420598030090332
outputs_pos.loss : 1.1203265190124512
Epoch 00485: adjusting learning rate of group 0 to 2.4855e-06.
outputs_pos.loss : 1.809111475944519
outputs_pos.loss : 1.1945762634277344
outputs_pos.loss : 1.2737427949905396
outputs_pos.loss : 1.0941370725631714
outputs_pos.loss : 1.3498135805130005
outputs_pos.loss : 1.0186262130737305
outputs_pos.loss : 1.1783164739608765
outputs_pos.loss : 1.4145091772079468
Epoch 00486: adjusting learning rate of group 0 to 2.4855e-06.
outputs_pos.loss : 1.1607387065887451
outputs_pos.loss : 1.0670747756958008
outputs_pos.loss : 1.29459810256958
outputs_pos.loss : 1.7294024229049683
outputs_pos.loss : 1.1447508335113525
outputs_pos.loss : 1.1391983032226562
outputs_pos.loss : 1.0338362455368042
outputs_pos.loss : 1.0420784950256348
Epoch 00487: adjusting learning rate of group 0 to 2.4854e-06.
outputs_pos.loss : 1.375230312347412
outputs_pos.loss : 1.2841893434524536
outputs_pos.loss : 1.5988619327545166
outputs_pos.loss : 1.1163887977600098
outputs_pos.loss : 1.063654899597168
outputs_pos.loss : 1.4923542737960815
outputs_pos.loss : 1.0300847291946411
outputs_pos.loss : 1.3793882131576538
Epoch 00488: adjusting learning rate of group 0 to 2.4853e-06.
outputs_pos.loss : 0.8253488540649414
outputs_pos.loss : 1.1767290830612183
outputs_pos.loss : 1.2886087894439697
outputs_pos.loss : 1.7412173748016357
outputs_pos.loss : 1.5141777992248535
outputs_pos.loss : 1.271173357963562
outputs_pos.loss : 0.9122341871261597
outputs_pos.loss : 1.2635442018508911
Epoch 00489: adjusting learning rate of group 0 to 2.4853e-06.
outputs_pos.loss : 1.0323222875595093
outputs_pos.loss : 1.408134937286377
outputs_pos.loss : 1.346012830734253
outputs_pos.loss : 1.1130744218826294
outputs_pos.loss : 1.3103957176208496
outputs_pos.loss : 1.0312716960906982
outputs_pos.loss : 1.3181703090667725
outputs_pos.loss : 0.9317269325256348
Epoch 00490: adjusting learning rate of group 0 to 2.4852e-06.
outputs_pos.loss : 1.0056347846984863
outputs_pos.loss : 1.3026238679885864
outputs_pos.loss : 1.2984609603881836
outputs_pos.loss : 1.512191653251648
outputs_pos.loss : 1.0044389963150024
outputs_pos.loss : 1.213714838027954
outputs_pos.loss : 1.1266758441925049
outputs_pos.loss : 0.7130576372146606
Epoch 00491: adjusting learning rate of group 0 to 2.4852e-06.
outputs_pos.loss : 1.6409966945648193
outputs_pos.loss : 1.6999268531799316
outputs_pos.loss : 0.9708699584007263
outputs_pos.loss : 0.802159309387207
outputs_pos.loss : 1.0055787563323975
outputs_pos.loss : 1.67405366897583
outputs_pos.loss : 0.977773129940033
outputs_pos.loss : 1.3279640674591064
Epoch 00492: adjusting learning rate of group 0 to 2.4851e-06.
outputs_pos.loss : 0.8912760019302368
outputs_pos.loss : 0.8833631277084351
outputs_pos.loss : 2.643866777420044
outputs_pos.loss : 1.4702212810516357
outputs_pos.loss : 1.1955338716506958
outputs_pos.loss : 1.016683578491211
outputs_pos.loss : 1.3848638534545898
outputs_pos.loss : 1.3998786211013794
Epoch 00493: adjusting learning rate of group 0 to 2.4850e-06.
outputs_pos.loss : 1.0420490503311157
outputs_pos.loss : 0.9418188333511353
outputs_pos.loss : 1.3566464185714722
outputs_pos.loss : 1.4692397117614746
outputs_pos.loss : 1.4317927360534668
outputs_pos.loss : 1.2945340871810913
outputs_pos.loss : 1.467858910560608
outputs_pos.loss : 1.159038782119751
Epoch 00494: adjusting learning rate of group 0 to 2.4850e-06.
outputs_pos.loss : 1.1460732221603394
outputs_pos.loss : 1.398737907409668
outputs_pos.loss : 2.267522096633911
outputs_pos.loss : 1.535582423210144
outputs_pos.loss : 1.1333829164505005
outputs_pos.loss : 1.7297718524932861
outputs_pos.loss : 0.8060818314552307
outputs_pos.loss : 0.9422371983528137
Epoch 00495: adjusting learning rate of group 0 to 2.4849e-06.
outputs_pos.loss : 0.9286239743232727
outputs_pos.loss : 1.3258942365646362
outputs_pos.loss : 0.8918042182922363
outputs_pos.loss : 0.969267725944519
outputs_pos.loss : 0.9302641749382019
outputs_pos.loss : 1.0634407997131348
outputs_pos.loss : 1.1690080165863037
outputs_pos.loss : 1.2992689609527588
Epoch 00496: adjusting learning rate of group 0 to 2.4849e-06.
outputs_pos.loss : 1.3700125217437744
outputs_pos.loss : 1.3166872262954712
outputs_pos.loss : 0.8688852787017822
outputs_pos.loss : 1.2590227127075195
outputs_pos.loss : 1.0735743045806885
outputs_pos.loss : 1.2836053371429443
outputs_pos.loss : 1.421248435974121
outputs_pos.loss : 1.2540346384048462
Epoch 00497: adjusting learning rate of group 0 to 2.4848e-06.
outputs_pos.loss : 0.9872854948043823
outputs_pos.loss : 0.9381243586540222
outputs_pos.loss : 1.165058970451355
outputs_pos.loss : 1.0785315036773682
outputs_pos.loss : 1.711919903755188
outputs_pos.loss : 1.4626646041870117
outputs_pos.loss : 0.91009920835495
outputs_pos.loss : 1.2280479669570923
Epoch 00498: adjusting learning rate of group 0 to 2.4847e-06.
outputs_pos.loss : 0.9066140055656433
outputs_pos.loss : 1.3583403825759888
outputs_pos.loss : 1.0142521858215332
outputs_pos.loss : 1.500059723854065
outputs_pos.loss : 1.1872072219848633
outputs_pos.loss : 1.0754982233047485
outputs_pos.loss : 1.34087336063385
outputs_pos.loss : 1.5376660823822021
Epoch 00499: adjusting learning rate of group 0 to 2.4847e-06.
outputs_pos.loss : 1.1150802373886108
outputs_pos.loss : 1.0312068462371826
outputs_pos.loss : 1.0766208171844482
outputs_pos.loss : 1.0327204465866089
outputs_pos.loss : 1.2073506116867065
outputs_pos.loss : 0.8827939033508301
outputs_pos.loss : 1.1792771816253662
outputs_pos.loss : 1.2445471286773682
Epoch 00500: adjusting learning rate of group 0 to 2.4846e-06.
outputs_pos.loss : 1.2613518238067627
outputs_pos.loss : 1.0228973627090454
outputs_pos.loss : 1.0991493463516235
outputs_pos.loss : 0.6370533108711243
outputs_pos.loss : 0.7196860909461975
outputs_pos.loss : 0.8332939743995667
outputs_pos.loss : 0.81507408618927
outputs_pos.loss : 1.0766671895980835
Epoch 00501: adjusting learning rate of group 0 to 2.4845e-06.
outputs_pos.loss : 1.3835145235061646
outputs_pos.loss : 0.7488248348236084
outputs_pos.loss : 1.1201883554458618
outputs_pos.loss : 1.5758675336837769
outputs_pos.loss : 1.1254994869232178
outputs_pos.loss : 1.9168020486831665
outputs_pos.loss : 1.5181186199188232
outputs_pos.loss : 1.2811765670776367
Epoch 00502: adjusting learning rate of group 0 to 2.4845e-06.
outputs_pos.loss : 0.9854018688201904
outputs_pos.loss : 1.0415867567062378
outputs_pos.loss : 1.353717565536499
outputs_pos.loss : 1.2910656929016113
outputs_pos.loss : 1.2067476511001587
outputs_pos.loss : 1.2999330759048462
outputs_pos.loss : 1.2294044494628906
outputs_pos.loss : 0.9883142113685608
Epoch 00503: adjusting learning rate of group 0 to 2.4844e-06.
outputs_pos.loss : 1.3915683031082153
outputs_pos.loss : 1.1310055255889893
outputs_pos.loss : 0.8123365640640259
outputs_pos.loss : 0.9304803609848022
outputs_pos.loss : 0.9471139907836914
outputs_pos.loss : 1.1010558605194092
outputs_pos.loss : 1.0524277687072754
outputs_pos.loss : 1.9372742176055908
Epoch 00504: adjusting learning rate of group 0 to 2.4844e-06.
outputs_pos.loss : 0.9753943085670471
outputs_pos.loss : 1.1818304061889648
outputs_pos.loss : 1.1120028495788574
outputs_pos.loss : 1.3350287675857544
outputs_pos.loss : 0.5907009243965149
outputs_pos.loss : 1.0201135873794556
outputs_pos.loss : 1.2876158952713013
outputs_pos.loss : 1.030272126197815
Epoch 00505: adjusting learning rate of group 0 to 2.4843e-06.
outputs_pos.loss : 1.2949693202972412
outputs_pos.loss : 1.206488847732544
outputs_pos.loss : 1.5156100988388062
outputs_pos.loss : 1.0339688062667847
outputs_pos.loss : 1.5621448755264282
outputs_pos.loss : 1.2507011890411377
outputs_pos.loss : 0.5948564410209656
outputs_pos.loss : 0.8613736629486084
Epoch 00506: adjusting learning rate of group 0 to 2.4842e-06.
outputs_pos.loss : 1.566355586051941
outputs_pos.loss : 1.5417931079864502
outputs_pos.loss : 1.2040975093841553
outputs_pos.loss : 1.1697255373001099
outputs_pos.loss : 1.1186156272888184
outputs_pos.loss : 1.6391037702560425
outputs_pos.loss : 1.1208828687667847
outputs_pos.loss : 1.1883450746536255
Epoch 00507: adjusting learning rate of group 0 to 2.4842e-06.
outputs_pos.loss : 1.0030838251113892
outputs_pos.loss : 1.0755376815795898
outputs_pos.loss : 1.2221710681915283
outputs_pos.loss : 1.02304208278656
outputs_pos.loss : 0.7273969054222107
outputs_pos.loss : 1.9408745765686035
outputs_pos.loss : 1.4894977807998657
outputs_pos.loss : 1.617790699005127
Epoch 00508: adjusting learning rate of group 0 to 2.4841e-06.
outputs_pos.loss : 1.1095565557479858
outputs_pos.loss : 1.6354862451553345
outputs_pos.loss : 1.32125723361969
outputs_pos.loss : 1.1741914749145508
outputs_pos.loss : 1.791284441947937
outputs_pos.loss : 1.4987545013427734
outputs_pos.loss : 0.7250609397888184
outputs_pos.loss : 1.5409576892852783
Epoch 00509: adjusting learning rate of group 0 to 2.4841e-06.
outputs_pos.loss : 1.0535582304000854
outputs_pos.loss : 1.3960877656936646
outputs_pos.loss : 1.3881562948226929
outputs_pos.loss : 1.1868051290512085
outputs_pos.loss : 1.382367730140686
outputs_pos.loss : 1.3496448993682861
outputs_pos.loss : 1.2132526636123657
outputs_pos.loss : 1.354476809501648
Epoch 00510: adjusting learning rate of group 0 to 2.4840e-06.
outputs_pos.loss : 1.2278189659118652
outputs_pos.loss : 1.2372463941574097
outputs_pos.loss : 1.281640648841858
outputs_pos.loss : 1.7256499528884888
outputs_pos.loss : 1.3934320211410522
outputs_pos.loss : 0.762743353843689
outputs_pos.loss : 1.444157600402832
outputs_pos.loss : 1.3436528444290161
Epoch 00511: adjusting learning rate of group 0 to 2.4839e-06.
outputs_pos.loss : 1.2215205430984497
outputs_pos.loss : 1.1165962219238281
outputs_pos.loss : 2.920966386795044
outputs_pos.loss : 2.0802621841430664
outputs_pos.loss : 0.6534470915794373
outputs_pos.loss : 0.7262555360794067
outputs_pos.loss : 1.130690336227417
outputs_pos.loss : 0.73600172996521
Epoch 00512: adjusting learning rate of group 0 to 2.4839e-06.
outputs_pos.loss : 0.9016838073730469
outputs_pos.loss : 1.4057823419570923
outputs_pos.loss : 1.1474790573120117
outputs_pos.loss : 0.9599887132644653
outputs_pos.loss : 1.0317838191986084
outputs_pos.loss : 1.4533783197402954
outputs_pos.loss : 1.3438626527786255
outputs_pos.loss : 1.2595548629760742
Epoch 00513: adjusting learning rate of group 0 to 2.4838e-06.
outputs_pos.loss : 1.113871455192566
outputs_pos.loss : 1.0242637395858765
outputs_pos.loss : 0.8487975597381592
outputs_pos.loss : 1.4086979627609253
outputs_pos.loss : 0.9860195517539978
outputs_pos.loss : 1.0679600238800049
outputs_pos.loss : 1.4781996011734009
outputs_pos.loss : 1.648360013961792
Epoch 00514: adjusting learning rate of group 0 to 2.4837e-06.
outputs_pos.loss : 0.9245816469192505
outputs_pos.loss : 0.9341506361961365
outputs_pos.loss : 1.3018916845321655
outputs_pos.loss : 1.1852492094039917
outputs_pos.loss : 1.3352638483047485
outputs_pos.loss : 1.0454319715499878
outputs_pos.loss : 0.4143681824207306
outputs_pos.loss : 1.365797758102417
Epoch 00515: adjusting learning rate of group 0 to 2.4837e-06.
outputs_pos.loss : 1.0474977493286133
outputs_pos.loss : 0.9890800714492798
outputs_pos.loss : 0.7388855218887329
outputs_pos.loss : 1.130676031112671
outputs_pos.loss : 1.0920437574386597
outputs_pos.loss : 1.0830847024917603
outputs_pos.loss : 0.8890840411186218
outputs_pos.loss : 1.2770955562591553
Epoch 00516: adjusting learning rate of group 0 to 2.4836e-06.
outputs_pos.loss : 1.0111182928085327
outputs_pos.loss : 1.578484296798706
outputs_pos.loss : 1.0736510753631592
outputs_pos.loss : 0.9539937376976013
outputs_pos.loss : 1.5074025392532349
outputs_pos.loss : 1.9344186782836914
outputs_pos.loss : 1.4837605953216553
outputs_pos.loss : 1.3842679262161255
Epoch 00517: adjusting learning rate of group 0 to 2.4835e-06.
outputs_pos.loss : 1.2409905195236206
outputs_pos.loss : 1.2544786930084229
outputs_pos.loss : 1.5187430381774902
outputs_pos.loss : 0.8895363211631775
outputs_pos.loss : 1.179699420928955
outputs_pos.loss : 0.7949399948120117
outputs_pos.loss : 1.5565197467803955
outputs_pos.loss : 1.2101092338562012
Epoch 00518: adjusting learning rate of group 0 to 2.4835e-06.
outputs_pos.loss : 1.2613664865493774
outputs_pos.loss : 0.9203656911849976
outputs_pos.loss : 1.3122769594192505
outputs_pos.loss : 1.1975125074386597
outputs_pos.loss : 1.197381854057312
outputs_pos.loss : 0.9503501057624817
outputs_pos.loss : 0.9110508561134338
outputs_pos.loss : 1.2938493490219116
Epoch 00519: adjusting learning rate of group 0 to 2.4834e-06.
outputs_pos.loss : 1.149309515953064
outputs_pos.loss : 0.9066444039344788
outputs_pos.loss : 0.9642986059188843
outputs_pos.loss : 1.5149165391921997
outputs_pos.loss : 0.8105148673057556
outputs_pos.loss : 1.1498799324035645
outputs_pos.loss : 0.8168268203735352
outputs_pos.loss : 1.6388484239578247
Epoch 00520: adjusting learning rate of group 0 to 2.4834e-06.
outputs_pos.loss : 1.4046825170516968
outputs_pos.loss : 0.8180584907531738
outputs_pos.loss : 1.0594803094863892
outputs_pos.loss : 1.056071162223816
outputs_pos.loss : 1.271570086479187
outputs_pos.loss : 1.1735973358154297
outputs_pos.loss : 0.6117812395095825
outputs_pos.loss : 1.1644673347473145
Epoch 00521: adjusting learning rate of group 0 to 2.4833e-06.
outputs_pos.loss : 1.0260370969772339
outputs_pos.loss : 1.7721213102340698
outputs_pos.loss : 0.8790347576141357
outputs_pos.loss : 1.1197785139083862
outputs_pos.loss : 1.501285433769226
outputs_pos.loss : 0.9787323474884033
outputs_pos.loss : 0.9966694116592407
outputs_pos.loss : 1.6572823524475098
Epoch 00522: adjusting learning rate of group 0 to 2.4832e-06.
outputs_pos.loss : 1.7467228174209595
outputs_pos.loss : 1.4612897634506226
outputs_pos.loss : 1.3154678344726562
outputs_pos.loss : 1.1364424228668213
outputs_pos.loss : 1.107800841331482
outputs_pos.loss : 1.1204860210418701
outputs_pos.loss : 1.8489701747894287
outputs_pos.loss : 1.2790979146957397
Epoch 00523: adjusting learning rate of group 0 to 2.4832e-06.
outputs_pos.loss : 1.209664225578308
outputs_pos.loss : 1.0423377752304077
outputs_pos.loss : 1.498539924621582
outputs_pos.loss : 1.2414275407791138
outputs_pos.loss : 1.184971570968628
outputs_pos.loss : 1.1798710823059082
outputs_pos.loss : 1.26730477809906
outputs_pos.loss : 0.717017650604248
Epoch 00524: adjusting learning rate of group 0 to 2.4831e-06.
outputs_pos.loss : 0.9285611510276794
outputs_pos.loss : 1.0656828880310059
outputs_pos.loss : 0.9535118341445923
outputs_pos.loss : 1.952547311782837
outputs_pos.loss : 1.1691832542419434
outputs_pos.loss : 1.210161566734314
outputs_pos.loss : 1.2898130416870117
outputs_pos.loss : 1.2966521978378296
Epoch 00525: adjusting learning rate of group 0 to 2.4830e-06.
outputs_pos.loss : 1.342360019683838
outputs_pos.loss : 1.7128310203552246
outputs_pos.loss : 1.105481743812561
outputs_pos.loss : 0.6884711384773254
outputs_pos.loss : 1.134249210357666
outputs_pos.loss : 1.8738404512405396
outputs_pos.loss : 1.2238818407058716
outputs_pos.loss : 1.0788137912750244
Epoch 00526: adjusting learning rate of group 0 to 2.4830e-06.
outputs_pos.loss : 1.0319297313690186
outputs_pos.loss : 1.1696761846542358
outputs_pos.loss : 0.9679661989212036
outputs_pos.loss : 1.1030683517456055
outputs_pos.loss : 1.0692061185836792
outputs_pos.loss : 1.2517000436782837
outputs_pos.loss : 1.1946760416030884
outputs_pos.loss : 0.8271282911300659
Epoch 00527: adjusting learning rate of group 0 to 2.4829e-06.
outputs_pos.loss : 1.079610824584961
outputs_pos.loss : 1.296534538269043
outputs_pos.loss : 1.3418331146240234
outputs_pos.loss : 1.2986574172973633
outputs_pos.loss : 1.4701474905014038
outputs_pos.loss : 1.3609731197357178
outputs_pos.loss : 1.1415950059890747
outputs_pos.loss : 0.9210729598999023
Epoch 00528: adjusting learning rate of group 0 to 2.4828e-06.
outputs_pos.loss : 1.124036192893982
outputs_pos.loss : 1.4554153680801392
outputs_pos.loss : 1.0094107389450073
outputs_pos.loss : 0.920712411403656
outputs_pos.loss : 1.4514530897140503
outputs_pos.loss : 0.7937886714935303
outputs_pos.loss : 1.114935040473938
outputs_pos.loss : 0.9001542925834656
Epoch 00529: adjusting learning rate of group 0 to 2.4828e-06.
outputs_pos.loss : 0.9851313829421997
outputs_pos.loss : 1.4595708847045898
outputs_pos.loss : 1.1463459730148315
outputs_pos.loss : 1.1486483812332153
outputs_pos.loss : 1.5438828468322754
outputs_pos.loss : 1.2583317756652832
outputs_pos.loss : 1.5257750749588013
outputs_pos.loss : 1.0015615224838257
Epoch 00530: adjusting learning rate of group 0 to 2.4827e-06.
outputs_pos.loss : 1.6778321266174316
outputs_pos.loss : 1.350360631942749
outputs_pos.loss : 0.9054034352302551
outputs_pos.loss : 1.1758275032043457
outputs_pos.loss : 0.9318940043449402
outputs_pos.loss : 1.5723698139190674
outputs_pos.loss : 1.543674349784851
outputs_pos.loss : 1.0588487386703491
Epoch 00531: adjusting learning rate of group 0 to 2.4826e-06.
outputs_pos.loss : 1.0835038423538208
outputs_pos.loss : 1.0676538944244385
outputs_pos.loss : 0.934687614440918
outputs_pos.loss : 1.048856496810913
outputs_pos.loss : 1.3460278511047363
outputs_pos.loss : 1.2343544960021973
outputs_pos.loss : 1.7668483257293701
outputs_pos.loss : 1.0257163047790527
Epoch 00532: adjusting learning rate of group 0 to 2.4826e-06.
outputs_pos.loss : 1.542197346687317
outputs_pos.loss : 1.489180564880371
outputs_pos.loss : 0.9528148770332336
outputs_pos.loss : 1.6028602123260498
outputs_pos.loss : 1.083545446395874
outputs_pos.loss : 1.399338722229004
outputs_pos.loss : 1.5991913080215454
outputs_pos.loss : 1.1941133737564087
Epoch 00533: adjusting learning rate of group 0 to 2.4825e-06.
outputs_pos.loss : 1.0416806936264038
outputs_pos.loss : 0.9748914241790771
outputs_pos.loss : 0.8149949312210083
outputs_pos.loss : 1.0577143430709839
outputs_pos.loss : 1.1574307680130005
outputs_pos.loss : 1.3810747861862183
outputs_pos.loss : 1.2008049488067627
outputs_pos.loss : 1.328455924987793
Epoch 00534: adjusting learning rate of group 0 to 2.4825e-06.
outputs_pos.loss : 0.9367698431015015
outputs_pos.loss : 1.459198236465454
outputs_pos.loss : 0.966886043548584
outputs_pos.loss : 1.5608545541763306
outputs_pos.loss : 1.3621350526809692
outputs_pos.loss : 0.9671134948730469
outputs_pos.loss : 0.9296053647994995
outputs_pos.loss : 1.3860433101654053
Epoch 00535: adjusting learning rate of group 0 to 2.4824e-06.
outputs_pos.loss : 1.4688348770141602
outputs_pos.loss : 1.3922107219696045
outputs_pos.loss : 1.042421817779541
outputs_pos.loss : 1.7501598596572876
outputs_pos.loss : 1.2647234201431274
outputs_pos.loss : 1.5165386199951172
outputs_pos.loss : 1.0630531311035156
outputs_pos.loss : 1.4955291748046875
Epoch 00536: adjusting learning rate of group 0 to 2.4823e-06.
outputs_pos.loss : 1.14309561252594
outputs_pos.loss : 0.9876249432563782
outputs_pos.loss : 1.5650080442428589
outputs_pos.loss : 1.1553512811660767
outputs_pos.loss : 0.8369247913360596
outputs_pos.loss : 1.3192219734191895
outputs_pos.loss : 0.9818016886711121
outputs_pos.loss : 0.9593503475189209
Epoch 00537: adjusting learning rate of group 0 to 2.4823e-06.
outputs_pos.loss : 0.8665309548377991
outputs_pos.loss : 1.0666818618774414
outputs_pos.loss : 1.3636832237243652
outputs_pos.loss : 1.1662671566009521
outputs_pos.loss : 1.0684916973114014
outputs_pos.loss : 1.002413272857666
outputs_pos.loss : 1.3609013557434082
outputs_pos.loss : 0.688084602355957
Epoch 00538: adjusting learning rate of group 0 to 2.4822e-06.
outputs_pos.loss : 1.9870543479919434
outputs_pos.loss : 0.9062030911445618
outputs_pos.loss : 1.6871752738952637
outputs_pos.loss : 1.1495883464813232
outputs_pos.loss : 1.2803146839141846
outputs_pos.loss : 1.4901249408721924
outputs_pos.loss : 1.494718074798584
outputs_pos.loss : 1.3815333843231201
Epoch 00539: adjusting learning rate of group 0 to 2.4821e-06.
outputs_pos.loss : 1.2049604654312134
outputs_pos.loss : 0.7456197142601013
outputs_pos.loss : 1.057185411453247
outputs_pos.loss : 1.0591633319854736
outputs_pos.loss : 1.1384592056274414
outputs_pos.loss : 1.5935739278793335
outputs_pos.loss : 1.0215345621109009
outputs_pos.loss : 1.146094560623169
Epoch 00540: adjusting learning rate of group 0 to 2.4821e-06.
outputs_pos.loss : 1.0611118078231812
outputs_pos.loss : 1.9401026964187622
outputs_pos.loss : 0.9254179000854492
outputs_pos.loss : 1.8288737535476685
outputs_pos.loss : 1.2286832332611084
outputs_pos.loss : 0.9908865690231323
outputs_pos.loss : 1.2806931734085083
outputs_pos.loss : 0.9305235743522644
Epoch 00541: adjusting learning rate of group 0 to 2.4820e-06.
outputs_pos.loss : 1.3784643411636353
outputs_pos.loss : 0.8103087544441223
outputs_pos.loss : 1.275397777557373
outputs_pos.loss : 1.664473056793213
outputs_pos.loss : 1.5394405126571655
outputs_pos.loss : 1.7044633626937866
outputs_pos.loss : 1.1968178749084473
outputs_pos.loss : 1.0690841674804688
Epoch 00542: adjusting learning rate of group 0 to 2.4819e-06.
outputs_pos.loss : 1.137168288230896
outputs_pos.loss : 0.938618540763855
outputs_pos.loss : 0.8401232361793518
outputs_pos.loss : 1.4753167629241943
outputs_pos.loss : 1.1979954242706299
outputs_pos.loss : 1.4608925580978394
outputs_pos.loss : 1.079727292060852
outputs_pos.loss : 1.245324730873108
Epoch 00543: adjusting learning rate of group 0 to 2.4819e-06.
outputs_pos.loss : 1.2516151666641235
outputs_pos.loss : 1.0834792852401733
outputs_pos.loss : 1.1962414979934692
outputs_pos.loss : 1.9450968503952026
outputs_pos.loss : 1.0216567516326904
outputs_pos.loss : 1.0941046476364136
outputs_pos.loss : 0.9739540815353394
outputs_pos.loss : 1.2331293821334839
Epoch 00544: adjusting learning rate of group 0 to 2.4818e-06.
outputs_pos.loss : 0.6965430974960327
outputs_pos.loss : 1.4419796466827393
outputs_pos.loss : 1.1348893642425537
outputs_pos.loss : 1.0031324625015259
outputs_pos.loss : 1.5485190153121948
outputs_pos.loss : 1.3383841514587402
outputs_pos.loss : 1.6529717445373535
outputs_pos.loss : 1.5733405351638794
Epoch 00545: adjusting learning rate of group 0 to 2.4817e-06.
outputs_pos.loss : 1.8239496946334839
outputs_pos.loss : 1.1972459554672241
outputs_pos.loss : 1.0426732301712036
outputs_pos.loss : 1.5327253341674805
outputs_pos.loss : 1.2550455331802368
outputs_pos.loss : 1.3195112943649292
outputs_pos.loss : 1.121332049369812
outputs_pos.loss : 1.3423644304275513
Epoch 00546: adjusting learning rate of group 0 to 2.4817e-06.
outputs_pos.loss : 1.0205425024032593
outputs_pos.loss : 1.036797046661377
outputs_pos.loss : 1.041664958000183
outputs_pos.loss : 1.130420446395874
outputs_pos.loss : 1.2808537483215332
outputs_pos.loss : 1.45689058303833
outputs_pos.loss : 0.9314086437225342
outputs_pos.loss : 0.8486822247505188
Epoch 00547: adjusting learning rate of group 0 to 2.4816e-06.
outputs_pos.loss : 1.3416335582733154
outputs_pos.loss : 1.2995420694351196
outputs_pos.loss : 1.0861371755599976
outputs_pos.loss : 0.9856603741645813
outputs_pos.loss : 1.4927093982696533
outputs_pos.loss : 0.7259857058525085
outputs_pos.loss : 1.0765374898910522
outputs_pos.loss : 1.0317935943603516
Epoch 00548: adjusting learning rate of group 0 to 2.4815e-06.
outputs_pos.loss : 1.228830099105835
outputs_pos.loss : 1.2730424404144287
outputs_pos.loss : 1.2473492622375488
outputs_pos.loss : 1.216834306716919
outputs_pos.loss : 1.154902696609497
outputs_pos.loss : 1.4224646091461182
outputs_pos.loss : 1.7811130285263062
outputs_pos.loss : 1.0933648347854614
Epoch 00549: adjusting learning rate of group 0 to 2.4815e-06.
outputs_pos.loss : 1.168076515197754
outputs_pos.loss : 1.4654595851898193
outputs_pos.loss : 1.2350876331329346
outputs_pos.loss : 1.3715393543243408
outputs_pos.loss : 1.1086409091949463
outputs_pos.loss : 1.3341954946517944
outputs_pos.loss : 1.1853725910186768
outputs_pos.loss : 1.2228809595108032
Epoch 00550: adjusting learning rate of group 0 to 2.4814e-06.
outputs_pos.loss : 1.1781824827194214
outputs_pos.loss : 1.1860617399215698
outputs_pos.loss : 1.1270276308059692
outputs_pos.loss : 1.359917163848877
outputs_pos.loss : 0.7973973751068115
outputs_pos.loss : 1.2625479698181152
outputs_pos.loss : 0.6765817403793335
outputs_pos.loss : 1.358994960784912
Epoch 00551: adjusting learning rate of group 0 to 2.4813e-06.
outputs_pos.loss : 1.1905429363250732
outputs_pos.loss : 0.9776641726493835
outputs_pos.loss : 1.2614521980285645
outputs_pos.loss : 0.8819422125816345
outputs_pos.loss : 1.40402090549469
outputs_pos.loss : 1.2914012670516968
outputs_pos.loss : 0.8871647119522095
outputs_pos.loss : 1.1434510946273804
Epoch 00552: adjusting learning rate of group 0 to 2.4813e-06.
outputs_pos.loss : 0.926861047744751
outputs_pos.loss : 1.4673265218734741
outputs_pos.loss : 0.8491840362548828
outputs_pos.loss : 1.4773272275924683
outputs_pos.loss : 1.2800588607788086
outputs_pos.loss : 1.100816249847412
outputs_pos.loss : 1.2391712665557861
outputs_pos.loss : 1.2494131326675415
Epoch 00553: adjusting learning rate of group 0 to 2.4812e-06.
outputs_pos.loss : 1.4935953617095947
outputs_pos.loss : 1.7458921670913696
outputs_pos.loss : 1.3975777626037598
outputs_pos.loss : 1.5430046319961548
outputs_pos.loss : 0.9654616117477417
outputs_pos.loss : 0.9056441783905029
outputs_pos.loss : 1.1139178276062012
outputs_pos.loss : 1.3353469371795654
Epoch 00554: adjusting learning rate of group 0 to 2.4811e-06.
outputs_pos.loss : 0.853326141834259
outputs_pos.loss : 1.1082829236984253
outputs_pos.loss : 1.4389697313308716
outputs_pos.loss : 1.9222559928894043
outputs_pos.loss : 1.2651901245117188
outputs_pos.loss : 1.1761399507522583
outputs_pos.loss : 1.1212496757507324
outputs_pos.loss : 1.6464438438415527
Epoch 00555: adjusting learning rate of group 0 to 2.4810e-06.
outputs_pos.loss : 1.6122255325317383
outputs_pos.loss : 1.183057427406311
outputs_pos.loss : 1.5998198986053467
outputs_pos.loss : 1.3680124282836914
outputs_pos.loss : 0.7289201021194458
outputs_pos.loss : 0.8090572357177734
outputs_pos.loss : 1.3081303834915161
outputs_pos.loss : 0.8641127347946167
Epoch 00556: adjusting learning rate of group 0 to 2.4810e-06.
outputs_pos.loss : 1.6087003946304321
outputs_pos.loss : 1.6478993892669678
outputs_pos.loss : 1.8352973461151123
outputs_pos.loss : 1.181333303451538
outputs_pos.loss : 1.308924913406372
outputs_pos.loss : 1.242926001548767
outputs_pos.loss : 1.4137037992477417
outputs_pos.loss : 1.2566783428192139
Epoch 00557: adjusting learning rate of group 0 to 2.4809e-06.
outputs_pos.loss : 1.7197613716125488
outputs_pos.loss : 1.020970344543457
outputs_pos.loss : 1.3391642570495605
outputs_pos.loss : 1.3905751705169678
outputs_pos.loss : 1.0811161994934082
outputs_pos.loss : 1.1548539400100708
outputs_pos.loss : 1.5755900144577026
outputs_pos.loss : 1.2465431690216064
Epoch 00558: adjusting learning rate of group 0 to 2.4808e-06.
outputs_pos.loss : 1.438031792640686
outputs_pos.loss : 1.4149425029754639
outputs_pos.loss : 1.5877795219421387
outputs_pos.loss : 1.5403157472610474
outputs_pos.loss : 1.0038937330245972
outputs_pos.loss : 1.1677131652832031
outputs_pos.loss : 1.0242983102798462
outputs_pos.loss : 0.938598096370697
Epoch 00559: adjusting learning rate of group 0 to 2.4808e-06.
outputs_pos.loss : 1.1685553789138794
outputs_pos.loss : 0.9229645133018494
outputs_pos.loss : 1.123964786529541
outputs_pos.loss : 1.6765244007110596
outputs_pos.loss : 2.0201892852783203
outputs_pos.loss : 1.0291376113891602
outputs_pos.loss : 1.1309857368469238
outputs_pos.loss : 1.1525477170944214
Epoch 00560: adjusting learning rate of group 0 to 2.4807e-06.
outputs_pos.loss : 1.7158119678497314
outputs_pos.loss : 1.5104056596755981
outputs_pos.loss : 1.0431100130081177
outputs_pos.loss : 1.7645357847213745
outputs_pos.loss : 1.0648025274276733
outputs_pos.loss : 1.2427440881729126
outputs_pos.loss : 1.5802661180496216
outputs_pos.loss : 1.0865236520767212
Epoch 00561: adjusting learning rate of group 0 to 2.4806e-06.
outputs_pos.loss : 1.6027706861495972
outputs_pos.loss : 1.8508180379867554
outputs_pos.loss : 1.1860663890838623
outputs_pos.loss : 1.4556169509887695
outputs_pos.loss : 1.3912506103515625
outputs_pos.loss : 1.6028701066970825
outputs_pos.loss : 1.0528461933135986
outputs_pos.loss : 1.138879418373108
Epoch 00562: adjusting learning rate of group 0 to 2.4806e-06.
outputs_pos.loss : 0.8290600180625916
outputs_pos.loss : 1.1796255111694336
outputs_pos.loss : 1.2628962993621826
outputs_pos.loss : 0.8700240850448608
outputs_pos.loss : 0.7474180459976196
outputs_pos.loss : 0.9519895315170288
outputs_pos.loss : 1.2586380243301392
outputs_pos.loss : 0.7323929667472839
Epoch 00563: adjusting learning rate of group 0 to 2.4805e-06.
outputs_pos.loss : 2.11529278755188
outputs_pos.loss : 1.006962776184082
outputs_pos.loss : 0.8542501330375671
outputs_pos.loss : 1.1982520818710327
outputs_pos.loss : 1.1222307682037354
outputs_pos.loss : 1.2201114892959595
outputs_pos.loss : 1.1174010038375854
outputs_pos.loss : 1.2493150234222412
Epoch 00564: adjusting learning rate of group 0 to 2.4804e-06.
outputs_pos.loss : 1.3513507843017578
outputs_pos.loss : 1.3368459939956665
outputs_pos.loss : 0.9965276122093201
outputs_pos.loss : 1.1148205995559692
outputs_pos.loss : 1.4362660646438599
outputs_pos.loss : 1.3118927478790283
outputs_pos.loss : 1.267878770828247
outputs_pos.loss : 0.9233941435813904
Epoch 00565: adjusting learning rate of group 0 to 2.4804e-06.
outputs_pos.loss : 1.0950744152069092
outputs_pos.loss : 2.1858150959014893
outputs_pos.loss : 1.5660018920898438
outputs_pos.loss : 1.1145402193069458
outputs_pos.loss : 1.4816689491271973
outputs_pos.loss : 1.6505684852600098
outputs_pos.loss : 1.5809937715530396
outputs_pos.loss : 0.962273359298706
Epoch 00566: adjusting learning rate of group 0 to 2.4803e-06.
outputs_pos.loss : 1.0764222145080566
outputs_pos.loss : 1.4939613342285156
outputs_pos.loss : 1.8429592847824097
outputs_pos.loss : 1.1037482023239136
outputs_pos.loss : 0.791783332824707
outputs_pos.loss : 0.9460997581481934
outputs_pos.loss : 1.257578730583191
outputs_pos.loss : 0.9356465935707092
Epoch 00567: adjusting learning rate of group 0 to 2.4802e-06.
outputs_pos.loss : 1.2905610799789429
outputs_pos.loss : 1.1340585947036743
outputs_pos.loss : 1.0144037008285522
outputs_pos.loss : 1.5474516153335571
outputs_pos.loss : 1.095848798751831
outputs_pos.loss : 1.3004673719406128
outputs_pos.loss : 1.1186292171478271
outputs_pos.loss : 1.0525416135787964
Epoch 00568: adjusting learning rate of group 0 to 2.4802e-06.
outputs_pos.loss : 1.1713471412658691
outputs_pos.loss : 1.9695936441421509
outputs_pos.loss : 1.19804048538208
outputs_pos.loss : 1.041722059249878
outputs_pos.loss : 0.9737300276756287
outputs_pos.loss : 1.3654628992080688
outputs_pos.loss : 1.6335018873214722
outputs_pos.loss : 1.4420589208602905
Epoch 00569: adjusting learning rate of group 0 to 2.4801e-06.
outputs_pos.loss : 1.4274052381515503
outputs_pos.loss : 1.0100414752960205
outputs_pos.loss : 1.0206453800201416
outputs_pos.loss : 1.0734989643096924
outputs_pos.loss : 1.4664807319641113
outputs_pos.loss : 1.6075385808944702
outputs_pos.loss : 1.0962021350860596
outputs_pos.loss : 1.1564409732818604
Epoch 00570: adjusting learning rate of group 0 to 2.4800e-06.
outputs_pos.loss : 0.9685186743736267
outputs_pos.loss : 0.9437301754951477
outputs_pos.loss : 1.2487050294876099
outputs_pos.loss : 1.4144940376281738
outputs_pos.loss : 0.9185060262680054
outputs_pos.loss : 1.5335842370986938
outputs_pos.loss : 1.4808346033096313
outputs_pos.loss : 0.829167366027832
Epoch 00571: adjusting learning rate of group 0 to 2.4799e-06.
outputs_pos.loss : 2.1193909645080566
outputs_pos.loss : 0.8827544450759888
outputs_pos.loss : 1.2131694555282593
outputs_pos.loss : 1.2432526350021362
outputs_pos.loss : 1.0391650199890137
outputs_pos.loss : 0.8337300419807434
outputs_pos.loss : 0.9156298041343689
outputs_pos.loss : 1.2893807888031006
Epoch 00572: adjusting learning rate of group 0 to 2.4799e-06.
outputs_pos.loss : 1.3729327917099
outputs_pos.loss : 1.245267391204834
outputs_pos.loss : 0.8779096603393555
outputs_pos.loss : 0.9982687830924988
outputs_pos.loss : 1.3564213514328003
outputs_pos.loss : 1.9859158992767334
outputs_pos.loss : 1.3620305061340332
outputs_pos.loss : 0.8553658127784729
Epoch 00573: adjusting learning rate of group 0 to 2.4798e-06.
outputs_pos.loss : 0.9309330582618713
outputs_pos.loss : 1.078343391418457
outputs_pos.loss : 0.970899224281311
outputs_pos.loss : 1.2170836925506592
outputs_pos.loss : 1.4555411338806152
outputs_pos.loss : 1.4689160585403442
outputs_pos.loss : 1.244166374206543
outputs_pos.loss : 1.079084873199463
Epoch 00574: adjusting learning rate of group 0 to 2.4797e-06.
outputs_pos.loss : 0.9991939663887024
outputs_pos.loss : 1.4843631982803345
outputs_pos.loss : 1.2032749652862549
outputs_pos.loss : 1.1057987213134766
outputs_pos.loss : 1.293031096458435
outputs_pos.loss : 1.46587073802948
outputs_pos.loss : 1.4150502681732178
outputs_pos.loss : 1.563073992729187
Epoch 00575: adjusting learning rate of group 0 to 2.4797e-06.
outputs_pos.loss : 1.112505555152893
outputs_pos.loss : 1.2851943969726562
outputs_pos.loss : 1.4569507837295532
outputs_pos.loss : 0.8504071235656738
outputs_pos.loss : 1.5343518257141113
outputs_pos.loss : 1.2204338312149048
outputs_pos.loss : 1.3245466947555542
outputs_pos.loss : 1.2066659927368164
Epoch 00576: adjusting learning rate of group 0 to 2.4796e-06.
outputs_pos.loss : 1.2314000129699707
outputs_pos.loss : 1.6441068649291992
outputs_pos.loss : 1.1101738214492798
outputs_pos.loss : 1.1615605354309082
outputs_pos.loss : 1.7142736911773682
outputs_pos.loss : 2.1580960750579834
outputs_pos.loss : 1.2865709066390991
outputs_pos.loss : 1.27126145362854
Epoch 00577: adjusting learning rate of group 0 to 2.4795e-06.
outputs_pos.loss : 1.016685962677002
outputs_pos.loss : 0.7892921566963196
outputs_pos.loss : 1.146494746208191
outputs_pos.loss : 1.0607943534851074
outputs_pos.loss : 1.007848858833313
outputs_pos.loss : 1.4670014381408691
outputs_pos.loss : 1.5957762002944946
outputs_pos.loss : 1.2331780195236206
Epoch 00578: adjusting learning rate of group 0 to 2.4794e-06.
outputs_pos.loss : 0.9891960024833679
outputs_pos.loss : 1.1175938844680786
outputs_pos.loss : 1.3444666862487793
outputs_pos.loss : 0.7998170852661133
outputs_pos.loss : 1.4106334447860718
outputs_pos.loss : 1.5139427185058594
outputs_pos.loss : 1.275175929069519
outputs_pos.loss : 0.8784818649291992
Epoch 00579: adjusting learning rate of group 0 to 2.4794e-06.
outputs_pos.loss : 1.338986873626709
outputs_pos.loss : 1.1481540203094482
outputs_pos.loss : 1.3645293712615967
outputs_pos.loss : 1.0634173154830933
outputs_pos.loss : 1.2432982921600342
outputs_pos.loss : 1.1756689548492432
outputs_pos.loss : 1.161110281944275
outputs_pos.loss : 1.0840696096420288
Epoch 00580: adjusting learning rate of group 0 to 2.4793e-06.
outputs_pos.loss : 1.0168207883834839
outputs_pos.loss : 1.5020229816436768
outputs_pos.loss : 0.8929889798164368
outputs_pos.loss : 0.916391134262085
outputs_pos.loss : 1.2779197692871094
outputs_pos.loss : 1.027355432510376
outputs_pos.loss : 1.1703071594238281
outputs_pos.loss : 1.0745673179626465
Epoch 00581: adjusting learning rate of group 0 to 2.4792e-06.
outputs_pos.loss : 1.2812870740890503
outputs_pos.loss : 1.2173020839691162
outputs_pos.loss : 1.1151319742202759
outputs_pos.loss : 1.381519079208374
outputs_pos.loss : 1.4110740423202515
outputs_pos.loss : 1.1697630882263184
outputs_pos.loss : 1.1694360971450806
outputs_pos.loss : 1.4775712490081787
Epoch 00582: adjusting learning rate of group 0 to 2.4792e-06.
outputs_pos.loss : 1.4351447820663452
outputs_pos.loss : 1.2190797328948975
outputs_pos.loss : 1.2580939531326294
outputs_pos.loss : 0.8832441568374634
outputs_pos.loss : 1.2432146072387695
outputs_pos.loss : 0.9538333415985107
outputs_pos.loss : 1.2105799913406372
outputs_pos.loss : 1.0165051221847534
Epoch 00583: adjusting learning rate of group 0 to 2.4791e-06.
outputs_pos.loss : 1.0718944072723389
outputs_pos.loss : 0.91834557056427
outputs_pos.loss : 1.3132754564285278
outputs_pos.loss : 1.6211680173873901
outputs_pos.loss : 1.205163598060608
outputs_pos.loss : 1.3098708391189575
outputs_pos.loss : 1.187339186668396
outputs_pos.loss : 1.1272552013397217
Epoch 00584: adjusting learning rate of group 0 to 2.4790e-06.
outputs_pos.loss : 1.2845618724822998
outputs_pos.loss : 1.398991346359253
outputs_pos.loss : 0.9545024633407593
outputs_pos.loss : 0.8102766871452332
outputs_pos.loss : 1.0113693475723267
outputs_pos.loss : 1.4213422536849976
outputs_pos.loss : 1.1487940549850464
outputs_pos.loss : 1.1137278079986572
Epoch 00585: adjusting learning rate of group 0 to 2.4789e-06.
outputs_pos.loss : 0.8008621335029602
outputs_pos.loss : 1.4737331867218018
outputs_pos.loss : 1.4460307359695435
outputs_pos.loss : 1.0041197538375854
outputs_pos.loss : 0.9727776050567627
outputs_pos.loss : 1.2491518259048462
outputs_pos.loss : 1.2712849378585815
outputs_pos.loss : 1.4807699918746948
Epoch 00586: adjusting learning rate of group 0 to 2.4789e-06.
outputs_pos.loss : 1.1653897762298584
outputs_pos.loss : 1.3734941482543945
outputs_pos.loss : 1.1774070262908936
outputs_pos.loss : 1.1349936723709106
outputs_pos.loss : 0.9147025942802429
outputs_pos.loss : 1.0681735277175903
outputs_pos.loss : 1.372855544090271
outputs_pos.loss : 1.0104390382766724
Epoch 00587: adjusting learning rate of group 0 to 2.4788e-06.
outputs_pos.loss : 1.4984230995178223
outputs_pos.loss : 1.00614595413208
outputs_pos.loss : 1.4315420389175415
outputs_pos.loss : 1.7571322917938232
outputs_pos.loss : 0.9858343601226807
outputs_pos.loss : 1.3153297901153564
outputs_pos.loss : 0.924019455909729
outputs_pos.loss : 1.1161235570907593
Epoch 00588: adjusting learning rate of group 0 to 2.4787e-06.
outputs_pos.loss : 1.3475978374481201
outputs_pos.loss : 1.1705716848373413
outputs_pos.loss : 0.8882834911346436
outputs_pos.loss : 1.1927545070648193
outputs_pos.loss : 1.288318395614624
outputs_pos.loss : 1.3664573431015015
outputs_pos.loss : 0.9658173322677612
outputs_pos.loss : 0.9704612493515015
Epoch 00589: adjusting learning rate of group 0 to 2.4787e-06.
outputs_pos.loss : 1.0723588466644287
outputs_pos.loss : 1.7264130115509033
outputs_pos.loss : 1.0846537351608276
outputs_pos.loss : 1.1520028114318848
outputs_pos.loss : 0.9942604303359985
outputs_pos.loss : 1.1529544591903687
outputs_pos.loss : 1.0251941680908203
outputs_pos.loss : 0.8936245441436768
Epoch 00590: adjusting learning rate of group 0 to 2.4786e-06.
outputs_pos.loss : 1.1831645965576172
outputs_pos.loss : 0.8743786215782166
outputs_pos.loss : 1.053889274597168
outputs_pos.loss : 0.784249484539032
outputs_pos.loss : 1.352117896080017
outputs_pos.loss : 1.0491163730621338
outputs_pos.loss : 1.7385612726211548
outputs_pos.loss : 1.1501014232635498
Epoch 00591: adjusting learning rate of group 0 to 2.4785e-06.
outputs_pos.loss : 1.190644383430481
outputs_pos.loss : 1.4803123474121094
outputs_pos.loss : 1.2455052137374878
outputs_pos.loss : 1.404367208480835
outputs_pos.loss : 1.357727289199829
outputs_pos.loss : 0.9651132822036743
outputs_pos.loss : 0.9537814855575562
outputs_pos.loss : 1.2719892263412476
Epoch 00592: adjusting learning rate of group 0 to 2.4784e-06.
outputs_pos.loss : 1.7297451496124268
outputs_pos.loss : 1.0686101913452148
outputs_pos.loss : 1.1516258716583252
outputs_pos.loss : 1.5301510095596313
outputs_pos.loss : 1.1649643182754517
outputs_pos.loss : 1.1399418115615845
outputs_pos.loss : 1.0334759950637817
outputs_pos.loss : 1.2917704582214355
Epoch 00593: adjusting learning rate of group 0 to 2.4784e-06.
outputs_pos.loss : 1.110463261604309
outputs_pos.loss : 1.0148683786392212
outputs_pos.loss : 1.2694658041000366
outputs_pos.loss : 1.1649539470672607
outputs_pos.loss : 1.3101272583007812
outputs_pos.loss : 0.7891827821731567
outputs_pos.loss : 1.3210742473602295
outputs_pos.loss : 1.6288237571716309
Epoch 00594: adjusting learning rate of group 0 to 2.4783e-06.
outputs_pos.loss : 1.9014357328414917
outputs_pos.loss : 1.4399536848068237
outputs_pos.loss : 1.2329702377319336
outputs_pos.loss : 1.1714820861816406
outputs_pos.loss : 1.219159722328186
outputs_pos.loss : 0.9720114469528198
outputs_pos.loss : 0.9082430005073547
outputs_pos.loss : 1.0579389333724976
Epoch 00595: adjusting learning rate of group 0 to 2.4782e-06.
outputs_pos.loss : 1.5420464277267456
outputs_pos.loss : 1.723760724067688
outputs_pos.loss : 1.1594595909118652
outputs_pos.loss : 1.2741719484329224
outputs_pos.loss : 1.054421305656433
outputs_pos.loss : 1.0137128829956055
outputs_pos.loss : 1.2156507968902588
outputs_pos.loss : 1.7137725353240967
Epoch 00596: adjusting learning rate of group 0 to 2.4782e-06.
outputs_pos.loss : 1.1852257251739502
outputs_pos.loss : 0.8726288676261902
outputs_pos.loss : 1.4586811065673828
outputs_pos.loss : 1.0505260229110718
outputs_pos.loss : 0.8887944221496582
outputs_pos.loss : 1.0164769887924194
outputs_pos.loss : 1.2585455179214478
outputs_pos.loss : 1.2957135438919067
Epoch 00597: adjusting learning rate of group 0 to 2.4781e-06.
outputs_pos.loss : 1.0778685808181763
outputs_pos.loss : 1.1018861532211304
outputs_pos.loss : 1.6378042697906494
outputs_pos.loss : 1.3547931909561157
outputs_pos.loss : 1.8275333642959595
outputs_pos.loss : 1.470780372619629
outputs_pos.loss : 1.263029932975769
outputs_pos.loss : 1.8305808305740356
Epoch 00598: adjusting learning rate of group 0 to 2.4780e-06.
outputs_pos.loss : 1.3563799858093262
outputs_pos.loss : 1.3350028991699219
outputs_pos.loss : 1.1248955726623535
outputs_pos.loss : 1.04189932346344
outputs_pos.loss : 1.3189113140106201
outputs_pos.loss : 1.4915931224822998
outputs_pos.loss : 0.9937869310379028
outputs_pos.loss : 1.1874642372131348
Epoch 00599: adjusting learning rate of group 0 to 2.4779e-06.
outputs_pos.loss : 1.3574416637420654
outputs_pos.loss : 1.3210041522979736
outputs_pos.loss : 1.3715314865112305
outputs_pos.loss : 1.3789485692977905
outputs_pos.loss : 1.1963318586349487
outputs_pos.loss : 1.0531708002090454
outputs_pos.loss : 1.0876593589782715
outputs_pos.loss : 2.094517469406128
Epoch 00600: adjusting learning rate of group 0 to 2.4779e-06.
outputs_pos.loss : 1.2806980609893799
outputs_pos.loss : 1.3858487606048584
outputs_pos.loss : 1.5068432092666626
outputs_pos.loss : 1.2961435317993164
outputs_pos.loss : 1.0506268739700317
outputs_pos.loss : 1.1260018348693848
outputs_pos.loss : 1.0287069082260132
outputs_pos.loss : 1.4312715530395508
Epoch 00601: adjusting learning rate of group 0 to 2.4778e-06.
outputs_pos.loss : 0.9615510106086731
outputs_pos.loss : 1.024271845817566
outputs_pos.loss : 1.3652360439300537
outputs_pos.loss : 0.9543903470039368
outputs_pos.loss : 1.3137760162353516
outputs_pos.loss : 1.2909640073776245
outputs_pos.loss : 1.2199286222457886
outputs_pos.loss : 1.2957617044448853
Epoch 00602: adjusting learning rate of group 0 to 2.4777e-06.
outputs_pos.loss : 0.8961833715438843
outputs_pos.loss : 1.553561806678772
outputs_pos.loss : 1.4945341348648071
outputs_pos.loss : 1.2022221088409424
outputs_pos.loss : 1.1144789457321167
outputs_pos.loss : 0.9191529154777527
outputs_pos.loss : 1.2908880710601807
outputs_pos.loss : 1.3789750337600708
Epoch 00603: adjusting learning rate of group 0 to 2.4776e-06.
outputs_pos.loss : 0.9517257809638977
outputs_pos.loss : 0.8719644546508789
outputs_pos.loss : 1.250460147857666
outputs_pos.loss : 1.130497694015503
outputs_pos.loss : 1.9406812191009521
outputs_pos.loss : 1.0237302780151367
outputs_pos.loss : 1.2986960411071777
outputs_pos.loss : 1.3424925804138184
Epoch 00604: adjusting learning rate of group 0 to 2.4776e-06.
outputs_pos.loss : 1.2485021352767944
outputs_pos.loss : 1.0488593578338623
outputs_pos.loss : 0.9930831789970398
outputs_pos.loss : 0.88884437084198
outputs_pos.loss : 1.32383131980896
outputs_pos.loss : 0.9076777100563049
outputs_pos.loss : 1.3665440082550049
outputs_pos.loss : 1.0992512702941895
Epoch 00605: adjusting learning rate of group 0 to 2.4775e-06.
outputs_pos.loss : 1.0223214626312256
outputs_pos.loss : 1.193706750869751
outputs_pos.loss : 0.8131663799285889
outputs_pos.loss : 1.0609827041625977
outputs_pos.loss : 1.2732247114181519
outputs_pos.loss : 1.124225378036499
outputs_pos.loss : 1.4582167863845825
outputs_pos.loss : 1.2882497310638428
Epoch 00606: adjusting learning rate of group 0 to 2.4774e-06.
outputs_pos.loss : 1.371406078338623
outputs_pos.loss : 1.360736608505249
outputs_pos.loss : 1.3508468866348267
outputs_pos.loss : 0.8128547072410583
outputs_pos.loss : 1.4665449857711792
outputs_pos.loss : 1.5390933752059937
outputs_pos.loss : 0.8331887722015381
outputs_pos.loss : 1.666247844696045
Epoch 00607: adjusting learning rate of group 0 to 2.4773e-06.
outputs_pos.loss : 1.2118134498596191
outputs_pos.loss : 0.9750052690505981
outputs_pos.loss : 1.2923492193222046
outputs_pos.loss : 1.1453406810760498
outputs_pos.loss : 1.07083261013031
outputs_pos.loss : 0.8932661414146423
outputs_pos.loss : 1.6840829849243164
outputs_pos.loss : 2.0281670093536377
Epoch 00608: adjusting learning rate of group 0 to 2.4773e-06.
outputs_pos.loss : 1.0288677215576172
outputs_pos.loss : 0.845703125
outputs_pos.loss : 1.1088175773620605
outputs_pos.loss : 1.2948333024978638
outputs_pos.loss : 1.297696590423584
outputs_pos.loss : 1.2266818284988403
outputs_pos.loss : 1.2635693550109863
outputs_pos.loss : 1.062982201576233
Epoch 00609: adjusting learning rate of group 0 to 2.4772e-06.
outputs_pos.loss : 1.394335389137268
outputs_pos.loss : 1.2828664779663086
outputs_pos.loss : 1.0557352304458618
outputs_pos.loss : 1.493826985359192
outputs_pos.loss : 1.5035417079925537
outputs_pos.loss : 0.9453061819076538
outputs_pos.loss : 1.2362806797027588
outputs_pos.loss : 1.1349700689315796
Epoch 00610: adjusting learning rate of group 0 to 2.4771e-06.
outputs_pos.loss : 1.4790445566177368
outputs_pos.loss : 1.1633200645446777
outputs_pos.loss : 1.2827283143997192
outputs_pos.loss : 1.0466197729110718
outputs_pos.loss : 1.4313658475875854
outputs_pos.loss : 1.446834921836853
outputs_pos.loss : 1.1698758602142334
outputs_pos.loss : 2.010368585586548
Epoch 00611: adjusting learning rate of group 0 to 2.4770e-06.
outputs_pos.loss : 1.041598916053772
outputs_pos.loss : 1.3640494346618652
outputs_pos.loss : 1.2502186298370361
outputs_pos.loss : 1.0737370252609253
outputs_pos.loss : 1.0301145315170288
outputs_pos.loss : 1.2872482538223267
outputs_pos.loss : 0.8291820287704468
outputs_pos.loss : 0.9800947904586792
Epoch 00612: adjusting learning rate of group 0 to 2.4770e-06.
outputs_pos.loss : 1.2834652662277222
outputs_pos.loss : 0.7985024452209473
outputs_pos.loss : 1.1487069129943848
outputs_pos.loss : 0.5813124775886536
outputs_pos.loss : 1.125659465789795
outputs_pos.loss : 1.1586748361587524
outputs_pos.loss : 1.3392080068588257
outputs_pos.loss : 1.3124338388442993
Epoch 00613: adjusting learning rate of group 0 to 2.4769e-06.
outputs_pos.loss : 1.150428056716919
outputs_pos.loss : 1.464633584022522
outputs_pos.loss : 1.1188526153564453
outputs_pos.loss : 0.9479883313179016
outputs_pos.loss : 1.98809015750885
outputs_pos.loss : 1.506590723991394
outputs_pos.loss : 1.3148969411849976
outputs_pos.loss : 1.1873465776443481
Epoch 00614: adjusting learning rate of group 0 to 2.4768e-06.
outputs_pos.loss : 1.7005473375320435
outputs_pos.loss : 0.8956146836280823
outputs_pos.loss : 1.0065124034881592
outputs_pos.loss : 1.010716199874878
outputs_pos.loss : 1.1699542999267578
outputs_pos.loss : 0.8947813510894775
outputs_pos.loss : 1.3815498352050781
outputs_pos.loss : 1.279585361480713
Epoch 00615: adjusting learning rate of group 0 to 2.4767e-06.
outputs_pos.loss : 1.2297108173370361
outputs_pos.loss : 1.4811913967132568
outputs_pos.loss : 1.2223131656646729
outputs_pos.loss : 1.1044950485229492
outputs_pos.loss : 1.4706498384475708
outputs_pos.loss : 1.107906460762024
outputs_pos.loss : 1.3754699230194092
outputs_pos.loss : 1.0855292081832886
Epoch 00616: adjusting learning rate of group 0 to 2.4767e-06.
outputs_pos.loss : 1.066362977027893
outputs_pos.loss : 0.9441657066345215
outputs_pos.loss : 1.61356520652771
outputs_pos.loss : 1.0528559684753418
outputs_pos.loss : 1.0910698175430298
outputs_pos.loss : 0.949512779712677
outputs_pos.loss : 1.362558126449585
outputs_pos.loss : 1.0064445734024048
Epoch 00617: adjusting learning rate of group 0 to 2.4766e-06.
outputs_pos.loss : 1.0128710269927979
outputs_pos.loss : 0.8360361456871033
outputs_pos.loss : 1.3404632806777954
outputs_pos.loss : 1.222566843032837
outputs_pos.loss : 0.9328207969665527
outputs_pos.loss : 1.3120158910751343
outputs_pos.loss : 1.0849921703338623
outputs_pos.loss : 0.9869017601013184
Epoch 00618: adjusting learning rate of group 0 to 2.4765e-06.
outputs_pos.loss : 1.3463431596755981
outputs_pos.loss : 0.9739029407501221
outputs_pos.loss : 1.1517082452774048
outputs_pos.loss : 1.119180679321289
outputs_pos.loss : 0.8755624890327454
outputs_pos.loss : 0.9816937446594238
outputs_pos.loss : 1.4121756553649902
outputs_pos.loss : 0.9182643890380859
Epoch 00619: adjusting learning rate of group 0 to 2.4764e-06.
outputs_pos.loss : 1.1153781414031982
outputs_pos.loss : 1.2924773693084717
outputs_pos.loss : 0.8988969922065735
outputs_pos.loss : 1.9917209148406982
outputs_pos.loss : 1.1998790502548218
outputs_pos.loss : 1.1785506010055542
outputs_pos.loss : 1.2726130485534668
outputs_pos.loss : 1.26137375831604
Epoch 00620: adjusting learning rate of group 0 to 2.4764e-06.
outputs_pos.loss : 1.0927091836929321
outputs_pos.loss : 1.0045772790908813
outputs_pos.loss : 1.452765941619873
outputs_pos.loss : 1.6373465061187744
outputs_pos.loss : 0.9668691754341125
outputs_pos.loss : 2.217284917831421
outputs_pos.loss : 1.1210931539535522
outputs_pos.loss : 1.1455382108688354
Epoch 00621: adjusting learning rate of group 0 to 2.4763e-06.
outputs_pos.loss : 0.6650575995445251
outputs_pos.loss : 1.1784567832946777
outputs_pos.loss : 0.9254620671272278
outputs_pos.loss : 0.9500223398208618
outputs_pos.loss : 1.0982729196548462
outputs_pos.loss : 1.224954605102539
outputs_pos.loss : 1.1733026504516602
outputs_pos.loss : 1.2647852897644043
Epoch 00622: adjusting learning rate of group 0 to 2.4762e-06.
outputs_pos.loss : 1.3058844804763794
outputs_pos.loss : 1.1080585718154907
outputs_pos.loss : 1.1805915832519531
outputs_pos.loss : 1.0434986352920532
outputs_pos.loss : 1.1059534549713135
outputs_pos.loss : 1.477142095565796
outputs_pos.loss : 1.2186236381530762
outputs_pos.loss : 1.64000403881073
Epoch 00623: adjusting learning rate of group 0 to 2.4761e-06.
outputs_pos.loss : 1.012582778930664
outputs_pos.loss : 0.9772982597351074
outputs_pos.loss : 1.243237853050232
outputs_pos.loss : 0.8721243739128113
outputs_pos.loss : 1.2433711290359497
outputs_pos.loss : 0.5634637475013733
outputs_pos.loss : 0.9735419750213623
outputs_pos.loss : 0.9180840253829956
Epoch 00624: adjusting learning rate of group 0 to 2.4761e-06.
outputs_pos.loss : 1.1856430768966675
outputs_pos.loss : 1.1053245067596436
outputs_pos.loss : 0.5023462772369385
outputs_pos.loss : 1.3283740282058716
outputs_pos.loss : 1.229091763496399
outputs_pos.loss : 0.9422115087509155
outputs_pos.loss : 1.5029010772705078
outputs_pos.loss : 0.9919970631599426
Epoch 00625: adjusting learning rate of group 0 to 2.4760e-06.
outputs_pos.loss : 2.274799108505249
outputs_pos.loss : 1.3620511293411255
outputs_pos.loss : 1.493174433708191
outputs_pos.loss : 1.0479813814163208
outputs_pos.loss : 1.3696980476379395
outputs_pos.loss : 1.1454554796218872
outputs_pos.loss : 1.317781686782837
outputs_pos.loss : 1.5338165760040283
Epoch 00626: adjusting learning rate of group 0 to 2.4759e-06.
outputs_pos.loss : 1.5779753923416138
outputs_pos.loss : 1.2897148132324219
outputs_pos.loss : 1.0231677293777466
outputs_pos.loss : 1.1106775999069214
outputs_pos.loss : 0.9743083715438843
outputs_pos.loss : 1.011663556098938
outputs_pos.loss : 1.2262606620788574
outputs_pos.loss : 1.0380823612213135
Epoch 00627: adjusting learning rate of group 0 to 2.4758e-06.
outputs_pos.loss : 0.6982353925704956
outputs_pos.loss : 1.0591514110565186
outputs_pos.loss : 1.8165614604949951
outputs_pos.loss : 1.7691477537155151
outputs_pos.loss : 0.9570907950401306
outputs_pos.loss : 1.766814112663269
outputs_pos.loss : 1.0332958698272705
outputs_pos.loss : 1.312554121017456
Epoch 00628: adjusting learning rate of group 0 to 2.4758e-06.
outputs_pos.loss : 1.4005870819091797
outputs_pos.loss : 1.282321572303772
outputs_pos.loss : 1.585654854774475
outputs_pos.loss : 1.2718555927276611
outputs_pos.loss : 1.6831239461898804
outputs_pos.loss : 1.2711278200149536
outputs_pos.loss : 0.7651636600494385
outputs_pos.loss : 1.4664264917373657
Epoch 00629: adjusting learning rate of group 0 to 2.4757e-06.
outputs_pos.loss : 1.5385886430740356
outputs_pos.loss : 1.3722076416015625
outputs_pos.loss : 1.0623741149902344
outputs_pos.loss : 1.5265768766403198
outputs_pos.loss : 1.195277452468872
outputs_pos.loss : 0.988618791103363
outputs_pos.loss : 1.1773850917816162
outputs_pos.loss : 1.13621187210083
Epoch 00630: adjusting learning rate of group 0 to 2.4756e-06.
outputs_pos.loss : 0.7043945789337158
outputs_pos.loss : 1.2077864408493042
outputs_pos.loss : 0.8388196229934692
outputs_pos.loss : 1.3107365369796753
outputs_pos.loss : 1.5178302526474
outputs_pos.loss : 2.0696372985839844
outputs_pos.loss : 0.8211456537246704
outputs_pos.loss : 2.0158371925354004
Epoch 00631: adjusting learning rate of group 0 to 2.4755e-06.
outputs_pos.loss : 1.5029213428497314
outputs_pos.loss : 1.1442375183105469
outputs_pos.loss : 0.662100076675415
outputs_pos.loss : 1.1084614992141724
outputs_pos.loss : 1.3455686569213867
outputs_pos.loss : 1.1721123456954956
outputs_pos.loss : 1.084713101387024
outputs_pos.loss : 0.6562510132789612
Epoch 00632: adjusting learning rate of group 0 to 2.4754e-06.
outputs_pos.loss : 1.7681849002838135
outputs_pos.loss : 1.0067682266235352
outputs_pos.loss : 0.8979908227920532
outputs_pos.loss : 1.0184119939804077
outputs_pos.loss : 1.7583367824554443
outputs_pos.loss : 1.0939377546310425
outputs_pos.loss : 1.010707974433899
outputs_pos.loss : 0.920368492603302
Epoch 00633: adjusting learning rate of group 0 to 2.4754e-06.
outputs_pos.loss : 1.8253827095031738
outputs_pos.loss : 1.0260697603225708
outputs_pos.loss : 0.77696293592453
outputs_pos.loss : 1.3747073411941528
outputs_pos.loss : 1.3629757165908813
outputs_pos.loss : 1.4747906923294067
outputs_pos.loss : 1.3320063352584839
outputs_pos.loss : 1.4145705699920654
Epoch 00634: adjusting learning rate of group 0 to 2.4753e-06.
outputs_pos.loss : 1.1417872905731201
outputs_pos.loss : 0.9768248796463013
outputs_pos.loss : 0.9688258171081543
outputs_pos.loss : 0.5641268491744995
outputs_pos.loss : 1.6999354362487793
outputs_pos.loss : 1.7865668535232544
outputs_pos.loss : 1.2750325202941895
outputs_pos.loss : 1.0694565773010254
Epoch 00635: adjusting learning rate of group 0 to 2.4752e-06.
outputs_pos.loss : 1.172753930091858
outputs_pos.loss : 0.8805769085884094
outputs_pos.loss : 1.2830238342285156
outputs_pos.loss : 1.1925266981124878
outputs_pos.loss : 1.7315802574157715
outputs_pos.loss : 1.0170449018478394
outputs_pos.loss : 0.9386939406394958
outputs_pos.loss : 1.94560706615448
Epoch 00636: adjusting learning rate of group 0 to 2.4751e-06.
outputs_pos.loss : 0.6583318710327148
outputs_pos.loss : 1.3475226163864136
outputs_pos.loss : 1.2898406982421875
outputs_pos.loss : 1.2083871364593506
outputs_pos.loss : 1.8203061819076538
outputs_pos.loss : 1.3683300018310547
outputs_pos.loss : 1.2555092573165894
outputs_pos.loss : 1.2522224187850952
Epoch 00637: adjusting learning rate of group 0 to 2.4751e-06.
outputs_pos.loss : 1.0348037481307983
outputs_pos.loss : 1.1955325603485107
outputs_pos.loss : 0.9945919513702393
outputs_pos.loss : 1.028296947479248
outputs_pos.loss : 0.9056908488273621
outputs_pos.loss : 1.1471210718154907
outputs_pos.loss : 1.3844504356384277
outputs_pos.loss : 0.8570222854614258
Epoch 00638: adjusting learning rate of group 0 to 2.4750e-06.
outputs_pos.loss : 1.286603331565857
outputs_pos.loss : 0.9942306280136108
outputs_pos.loss : 1.1806259155273438
outputs_pos.loss : 1.2716351747512817
outputs_pos.loss : 1.1859517097473145
outputs_pos.loss : 1.5712358951568604
outputs_pos.loss : 0.85587078332901
outputs_pos.loss : 1.1657594442367554
Epoch 00639: adjusting learning rate of group 0 to 2.4749e-06.
outputs_pos.loss : 1.901653528213501
outputs_pos.loss : 1.166367769241333
outputs_pos.loss : 1.3372927904129028
outputs_pos.loss : 0.9325937032699585
outputs_pos.loss : 1.270081639289856
outputs_pos.loss : 1.4094308614730835
outputs_pos.loss : 1.0466972589492798
outputs_pos.loss : 1.6456016302108765
Epoch 00640: adjusting learning rate of group 0 to 2.4748e-06.
outputs_pos.loss : 1.320068120956421
outputs_pos.loss : 1.0054289102554321
outputs_pos.loss : 0.8486300110816956
outputs_pos.loss : 0.75826096534729
outputs_pos.loss : 1.059708833694458
outputs_pos.loss : 1.3371222019195557
outputs_pos.loss : 0.9201705455780029
outputs_pos.loss : 1.048824429512024
Epoch 00641: adjusting learning rate of group 0 to 2.4747e-06.
outputs_pos.loss : 1.5310529470443726
outputs_pos.loss : 1.7738007307052612
outputs_pos.loss : 1.1408945322036743
outputs_pos.loss : 1.0979678630828857
outputs_pos.loss : 1.2452166080474854
outputs_pos.loss : 1.4689842462539673
outputs_pos.loss : 0.9651265740394592
outputs_pos.loss : 1.047287106513977
Epoch 00642: adjusting learning rate of group 0 to 2.4747e-06.
outputs_pos.loss : 1.1859771013259888
outputs_pos.loss : 1.5500104427337646
outputs_pos.loss : 1.137868881225586
outputs_pos.loss : 0.8276327252388
outputs_pos.loss : 1.1457468271255493
outputs_pos.loss : 1.5133050680160522
outputs_pos.loss : 1.0562829971313477
outputs_pos.loss : 1.0904122591018677
Epoch 00643: adjusting learning rate of group 0 to 2.4746e-06.
outputs_pos.loss : 1.0473833084106445
outputs_pos.loss : 0.7803706526756287
outputs_pos.loss : 0.8853819370269775
outputs_pos.loss : 1.0653774738311768
outputs_pos.loss : 1.1888562440872192
outputs_pos.loss : 1.4621703624725342
outputs_pos.loss : 0.9881162643432617
outputs_pos.loss : 0.8294816017150879
Epoch 00644: adjusting learning rate of group 0 to 2.4745e-06.
outputs_pos.loss : 1.0584197044372559
outputs_pos.loss : 1.2744905948638916
outputs_pos.loss : 1.041292667388916
outputs_pos.loss : 1.2124629020690918
outputs_pos.loss : 1.4723472595214844
outputs_pos.loss : 1.1082438230514526
outputs_pos.loss : 1.1646673679351807
outputs_pos.loss : 1.7718051671981812
Epoch 00645: adjusting learning rate of group 0 to 2.4744e-06.
outputs_pos.loss : 1.104953408241272
outputs_pos.loss : 1.5159722566604614
outputs_pos.loss : 1.090527057647705
outputs_pos.loss : 1.0943834781646729
outputs_pos.loss : 1.2158151865005493
outputs_pos.loss : 0.9149126410484314
outputs_pos.loss : 1.3590362071990967
outputs_pos.loss : 0.7900692820549011
Epoch 00646: adjusting learning rate of group 0 to 2.4743e-06.
outputs_pos.loss : 1.5069493055343628
outputs_pos.loss : 0.9853688478469849
outputs_pos.loss : 1.5021986961364746
outputs_pos.loss : 1.0929983854293823
outputs_pos.loss : 1.0113521814346313
outputs_pos.loss : 1.2040510177612305
outputs_pos.loss : 1.0161575078964233
outputs_pos.loss : 1.4538898468017578
Epoch 00647: adjusting learning rate of group 0 to 2.4743e-06.
outputs_pos.loss : 1.229737639427185
outputs_pos.loss : 1.4696605205535889
outputs_pos.loss : 0.9344090223312378
outputs_pos.loss : 0.9379486441612244
outputs_pos.loss : 1.1214908361434937
outputs_pos.loss : 1.2279653549194336
outputs_pos.loss : 0.9247451424598694
outputs_pos.loss : 1.1447665691375732
Epoch 00648: adjusting learning rate of group 0 to 2.4742e-06.
outputs_pos.loss : 1.3110167980194092
outputs_pos.loss : 1.5953612327575684
outputs_pos.loss : 0.9191792011260986
outputs_pos.loss : 1.0359209775924683
outputs_pos.loss : 1.0722408294677734
outputs_pos.loss : 1.1118686199188232
outputs_pos.loss : 1.5005626678466797
outputs_pos.loss : 1.0025988817214966
Epoch 00649: adjusting learning rate of group 0 to 2.4741e-06.
outputs_pos.loss : 1.2267653942108154
outputs_pos.loss : 1.7833774089813232
outputs_pos.loss : 1.1833665370941162
outputs_pos.loss : 1.0833615064620972
outputs_pos.loss : 1.2873929738998413
outputs_pos.loss : 1.2982237339019775
outputs_pos.loss : 1.3154555559158325
outputs_pos.loss : 1.2495602369308472
Epoch 00650: adjusting learning rate of group 0 to 2.4740e-06.
outputs_pos.loss : 1.0469319820404053
outputs_pos.loss : 1.203660249710083
outputs_pos.loss : 1.595355749130249
outputs_pos.loss : 1.1420955657958984
outputs_pos.loss : 1.2078391313552856
outputs_pos.loss : 1.1381078958511353
outputs_pos.loss : 1.0990612506866455
outputs_pos.loss : 1.4088733196258545
Epoch 00651: adjusting learning rate of group 0 to 2.4739e-06.
outputs_pos.loss : 1.6252241134643555
outputs_pos.loss : 0.994754433631897
outputs_pos.loss : 1.0574318170547485
outputs_pos.loss : 1.2341206073760986
outputs_pos.loss : 1.7020535469055176
outputs_pos.loss : 1.3540191650390625
outputs_pos.loss : 1.3270916938781738
outputs_pos.loss : 1.3279143571853638
Epoch 00652: adjusting learning rate of group 0 to 2.4739e-06.
outputs_pos.loss : 1.054322600364685
outputs_pos.loss : 1.1376124620437622
outputs_pos.loss : 1.0428868532180786
outputs_pos.loss : 1.2727301120758057
outputs_pos.loss : 1.6058073043823242
outputs_pos.loss : 1.366904377937317
outputs_pos.loss : 1.3273290395736694
outputs_pos.loss : 1.0832465887069702
Epoch 00653: adjusting learning rate of group 0 to 2.4738e-06.
outputs_pos.loss : 1.4353402853012085
outputs_pos.loss : 1.0486160516738892
outputs_pos.loss : 0.9477089047431946
outputs_pos.loss : 1.0017257928848267
outputs_pos.loss : 1.0471124649047852
outputs_pos.loss : 1.1482430696487427
outputs_pos.loss : 1.1054145097732544
outputs_pos.loss : 1.6146366596221924
Epoch 00654: adjusting learning rate of group 0 to 2.4737e-06.
outputs_pos.loss : 1.243187665939331
outputs_pos.loss : 1.1908973455429077
outputs_pos.loss : 0.9961664080619812
outputs_pos.loss : 0.7699404954910278
outputs_pos.loss : 1.105945348739624
outputs_pos.loss : 1.7632224559783936
outputs_pos.loss : 1.2657874822616577
outputs_pos.loss : 1.4590727090835571
Epoch 00655: adjusting learning rate of group 0 to 2.4736e-06.
outputs_pos.loss : 1.0782533884048462
outputs_pos.loss : 1.3185354471206665
outputs_pos.loss : 1.0766253471374512
outputs_pos.loss : 0.9167719483375549
outputs_pos.loss : 1.2970248460769653
outputs_pos.loss : 0.9169083833694458
outputs_pos.loss : 1.202277660369873
outputs_pos.loss : 1.0997027158737183
Epoch 00656: adjusting learning rate of group 0 to 2.4735e-06.
outputs_pos.loss : 1.2665956020355225
outputs_pos.loss : 0.9599423408508301
outputs_pos.loss : 1.2394593954086304
outputs_pos.loss : 1.5304890871047974
outputs_pos.loss : 1.5849320888519287
outputs_pos.loss : 1.037905216217041
outputs_pos.loss : 0.5857915878295898
outputs_pos.loss : 1.1811747550964355
Epoch 00657: adjusting learning rate of group 0 to 2.4735e-06.
outputs_pos.loss : 1.1849055290222168
outputs_pos.loss : 1.8025872707366943
outputs_pos.loss : 1.2134441137313843
outputs_pos.loss : 1.1847748756408691
outputs_pos.loss : 1.1207197904586792
outputs_pos.loss : 1.1865336894989014
outputs_pos.loss : 0.9193587899208069
outputs_pos.loss : 1.1800113916397095
Epoch 00658: adjusting learning rate of group 0 to 2.4734e-06.
outputs_pos.loss : 1.1497331857681274
outputs_pos.loss : 1.4466484785079956
outputs_pos.loss : 1.0551658868789673
outputs_pos.loss : 1.490254521369934
outputs_pos.loss : 1.408398985862732
outputs_pos.loss : 1.4800384044647217
outputs_pos.loss : 1.1285384893417358
outputs_pos.loss : 0.9452236294746399
Epoch 00659: adjusting learning rate of group 0 to 2.4733e-06.
outputs_pos.loss : 1.222284197807312
outputs_pos.loss : 1.2485389709472656
outputs_pos.loss : 0.9930858612060547
outputs_pos.loss : 1.1274265050888062
outputs_pos.loss : 1.0755228996276855
outputs_pos.loss : 1.300885558128357
outputs_pos.loss : 1.254418134689331
outputs_pos.loss : 0.7773966789245605
Epoch 00660: adjusting learning rate of group 0 to 2.4732e-06.
outputs_pos.loss : 1.5633037090301514
outputs_pos.loss : 0.9318517446517944
outputs_pos.loss : 1.334967851638794
outputs_pos.loss : 2.0495214462280273
outputs_pos.loss : 1.1257357597351074
outputs_pos.loss : 1.287717580795288
outputs_pos.loss : 0.8804895877838135
outputs_pos.loss : 1.3782979249954224
Epoch 00661: adjusting learning rate of group 0 to 2.4731e-06.
outputs_pos.loss : 1.4516633749008179
outputs_pos.loss : 0.9964714050292969
outputs_pos.loss : 0.7866752743721008
outputs_pos.loss : 1.2913769483566284
outputs_pos.loss : 1.1584498882293701
outputs_pos.loss : 1.104573130607605
outputs_pos.loss : 1.3056858777999878
outputs_pos.loss : 1.0906126499176025
Epoch 00662: adjusting learning rate of group 0 to 2.4731e-06.
outputs_pos.loss : 1.3832927942276
outputs_pos.loss : 1.2926570177078247
outputs_pos.loss : 1.2203794717788696
outputs_pos.loss : 0.8990042805671692
outputs_pos.loss : 1.0353552103042603
outputs_pos.loss : 1.649857521057129
outputs_pos.loss : 1.1059820652008057
outputs_pos.loss : 1.226541519165039
Epoch 00663: adjusting learning rate of group 0 to 2.4730e-06.
outputs_pos.loss : 1.2879185676574707
outputs_pos.loss : 1.3402763605117798
outputs_pos.loss : 1.460370421409607
outputs_pos.loss : 0.7367504835128784
outputs_pos.loss : 1.3366355895996094
outputs_pos.loss : 1.0423418283462524
outputs_pos.loss : 1.2276932001113892
outputs_pos.loss : 1.0028020143508911
Epoch 00664: adjusting learning rate of group 0 to 2.4729e-06.
outputs_pos.loss : 1.2299243211746216
outputs_pos.loss : 1.2607429027557373
outputs_pos.loss : 1.12705397605896
outputs_pos.loss : 1.1161748170852661
outputs_pos.loss : 1.459108829498291
outputs_pos.loss : 1.6816731691360474
outputs_pos.loss : 1.0632023811340332
outputs_pos.loss : 1.244347333908081
Epoch 00665: adjusting learning rate of group 0 to 2.4728e-06.
outputs_pos.loss : 1.0978858470916748
outputs_pos.loss : 1.0081024169921875
outputs_pos.loss : 1.6258468627929688
outputs_pos.loss : 1.3056608438491821
outputs_pos.loss : 1.1492979526519775
outputs_pos.loss : 0.6948781609535217
outputs_pos.loss : 1.3555691242218018
outputs_pos.loss : 1.1518006324768066
Epoch 00666: adjusting learning rate of group 0 to 2.4727e-06.
outputs_pos.loss : 0.7864831686019897
outputs_pos.loss : 1.1927568912506104
outputs_pos.loss : 0.8806312084197998
outputs_pos.loss : 1.359579086303711
outputs_pos.loss : 1.5495343208312988
outputs_pos.loss : 0.6533359289169312
outputs_pos.loss : 1.0885835886001587
outputs_pos.loss : 1.124330997467041
Epoch 00667: adjusting learning rate of group 0 to 2.4727e-06.
outputs_pos.loss : 1.5619813203811646
outputs_pos.loss : 1.0950233936309814
outputs_pos.loss : 1.242707371711731
outputs_pos.loss : 1.3071638345718384
outputs_pos.loss : 1.148085355758667
outputs_pos.loss : 1.2356181144714355
outputs_pos.loss : 1.2842644453048706
outputs_pos.loss : 1.3585963249206543
Epoch 00668: adjusting learning rate of group 0 to 2.4726e-06.
outputs_pos.loss : 1.4711440801620483
outputs_pos.loss : 1.1858360767364502
outputs_pos.loss : 0.9551351070404053
outputs_pos.loss : 1.3870192766189575
outputs_pos.loss : 1.6748087406158447
outputs_pos.loss : 1.2161202430725098
outputs_pos.loss : 1.072409749031067
outputs_pos.loss : 1.3832415342330933
Epoch 00669: adjusting learning rate of group 0 to 2.4725e-06.
outputs_pos.loss : 2.0890517234802246
outputs_pos.loss : 1.033630132675171
outputs_pos.loss : 1.1583294868469238
outputs_pos.loss : 1.0651600360870361
outputs_pos.loss : 1.1285388469696045
outputs_pos.loss : 1.8864508867263794
outputs_pos.loss : 1.0300954580307007
outputs_pos.loss : 1.1520817279815674
Epoch 00670: adjusting learning rate of group 0 to 2.4724e-06.
outputs_pos.loss : 1.1105750799179077
outputs_pos.loss : 0.914070188999176
outputs_pos.loss : 1.6234489679336548
outputs_pos.loss : 0.798396646976471
outputs_pos.loss : 1.3463057279586792
outputs_pos.loss : 1.4656190872192383
outputs_pos.loss : 1.4501209259033203
outputs_pos.loss : 0.7904049158096313
Epoch 00671: adjusting learning rate of group 0 to 2.4723e-06.
outputs_pos.loss : 1.1552448272705078
outputs_pos.loss : 1.7161052227020264
outputs_pos.loss : 0.9608685374259949
outputs_pos.loss : 1.178126573562622
outputs_pos.loss : 0.906644344329834
outputs_pos.loss : 1.1353689432144165
outputs_pos.loss : 0.9925736784934998
outputs_pos.loss : 0.8208861947059631
Epoch 00672: adjusting learning rate of group 0 to 2.4722e-06.
outputs_pos.loss : 0.8678428530693054
outputs_pos.loss : 1.4247790575027466
outputs_pos.loss : 1.2898296117782593
outputs_pos.loss : 1.2333108186721802
outputs_pos.loss : 1.3795802593231201
outputs_pos.loss : 0.771554172039032
outputs_pos.loss : 1.3581714630126953
outputs_pos.loss : 1.1747123003005981
Epoch 00673: adjusting learning rate of group 0 to 2.4722e-06.
outputs_pos.loss : 1.0148440599441528
outputs_pos.loss : 1.8545218706130981
outputs_pos.loss : 1.041885495185852
outputs_pos.loss : 1.1943070888519287
outputs_pos.loss : 1.9435570240020752
outputs_pos.loss : 1.0869773626327515
outputs_pos.loss : 1.1121928691864014
outputs_pos.loss : 1.3584831953048706
Epoch 00674: adjusting learning rate of group 0 to 2.4721e-06.
outputs_pos.loss : 0.9251965880393982
outputs_pos.loss : 0.8371831178665161
outputs_pos.loss : 1.021672010421753
outputs_pos.loss : 0.8350242972373962
outputs_pos.loss : 1.014339804649353
outputs_pos.loss : 1.8051583766937256
outputs_pos.loss : 1.0251719951629639
outputs_pos.loss : 1.2545920610427856
Epoch 00675: adjusting learning rate of group 0 to 2.4720e-06.
outputs_pos.loss : 1.2049062252044678
outputs_pos.loss : 1.1914204359054565
outputs_pos.loss : 0.9256543517112732
outputs_pos.loss : 0.9678587317466736
outputs_pos.loss : 1.4869753122329712
outputs_pos.loss : 0.8560370802879333
outputs_pos.loss : 1.2256885766983032
outputs_pos.loss : 0.9442980289459229
Epoch 00676: adjusting learning rate of group 0 to 2.4719e-06.
outputs_pos.loss : 1.0428322553634644
outputs_pos.loss : 1.0519685745239258
outputs_pos.loss : 0.80945885181427
outputs_pos.loss : 0.8838962316513062
outputs_pos.loss : 0.8140173554420471
outputs_pos.loss : 1.573376178741455
outputs_pos.loss : 1.010806918144226
outputs_pos.loss : 1.3045873641967773
Epoch 00677: adjusting learning rate of group 0 to 2.4718e-06.
outputs_pos.loss : 1.4008771181106567
outputs_pos.loss : 1.0331251621246338
outputs_pos.loss : 1.1246824264526367
outputs_pos.loss : 1.201143741607666
outputs_pos.loss : 1.21379554271698
outputs_pos.loss : 1.74612295627594
outputs_pos.loss : 0.8252492547035217
outputs_pos.loss : 1.0793249607086182
Epoch 00678: adjusting learning rate of group 0 to 2.4718e-06.
outputs_pos.loss : 1.088772177696228
outputs_pos.loss : 1.1627198457717896
outputs_pos.loss : 1.1059843301773071
outputs_pos.loss : 0.9346198439598083
outputs_pos.loss : 1.4889572858810425
outputs_pos.loss : 1.9046423435211182
outputs_pos.loss : 0.798577070236206
outputs_pos.loss : 0.7940346002578735
Epoch 00679: adjusting learning rate of group 0 to 2.4717e-06.
outputs_pos.loss : 1.105034589767456
outputs_pos.loss : 1.7170114517211914
outputs_pos.loss : 1.2049697637557983
outputs_pos.loss : 1.0182652473449707
outputs_pos.loss : 1.2188934087753296
outputs_pos.loss : 1.4254131317138672
outputs_pos.loss : 1.6768051385879517
outputs_pos.loss : 0.8452801704406738
Epoch 00680: adjusting learning rate of group 0 to 2.4716e-06.
outputs_pos.loss : 1.4161092042922974
outputs_pos.loss : 1.1521602869033813
outputs_pos.loss : 1.3404415845870972
outputs_pos.loss : 1.3553171157836914
outputs_pos.loss : 1.140658974647522
outputs_pos.loss : 1.188542127609253
outputs_pos.loss : 1.0170767307281494
outputs_pos.loss : 0.9077838063240051
Epoch 00681: adjusting learning rate of group 0 to 2.4715e-06.
outputs_pos.loss : 1.4263349771499634
outputs_pos.loss : 0.7779848575592041
outputs_pos.loss : 0.8886792063713074
outputs_pos.loss : 1.9903966188430786
outputs_pos.loss : 1.3295905590057373
outputs_pos.loss : 1.1058422327041626
outputs_pos.loss : 1.445888638496399
outputs_pos.loss : 0.8008100390434265
Epoch 00682: adjusting learning rate of group 0 to 2.4714e-06.
outputs_pos.loss : 1.0664879083633423
outputs_pos.loss : 0.9414635300636292
outputs_pos.loss : 2.0487399101257324
outputs_pos.loss : 1.0248420238494873
outputs_pos.loss : 1.5190989971160889
outputs_pos.loss : 1.5225615501403809
outputs_pos.loss : 1.1023539304733276
outputs_pos.loss : 1.3327716588974
Epoch 00683: adjusting learning rate of group 0 to 2.4713e-06.
outputs_pos.loss : 1.1344795227050781
outputs_pos.loss : 1.2817819118499756
outputs_pos.loss : 1.1080737113952637
outputs_pos.loss : 1.1361082792282104
outputs_pos.loss : 0.8231357932090759
outputs_pos.loss : 1.2790476083755493
outputs_pos.loss : 1.302053451538086
outputs_pos.loss : 1.348299264907837
Epoch 00684: adjusting learning rate of group 0 to 2.4713e-06.
outputs_pos.loss : 1.0918445587158203
outputs_pos.loss : 1.2185567617416382
outputs_pos.loss : 1.2364665269851685
outputs_pos.loss : 1.1732425689697266
outputs_pos.loss : 0.8282732963562012
outputs_pos.loss : 1.0504364967346191
outputs_pos.loss : 0.8517534732818604
outputs_pos.loss : 1.1959667205810547
Epoch 00685: adjusting learning rate of group 0 to 2.4712e-06.
outputs_pos.loss : 1.511062502861023
outputs_pos.loss : 0.8858006596565247
outputs_pos.loss : 1.283247947692871
outputs_pos.loss : 0.9517159461975098
outputs_pos.loss : 1.3869787454605103
outputs_pos.loss : 0.8007022738456726
outputs_pos.loss : 1.5080231428146362
outputs_pos.loss : 1.7341231107711792
Epoch 00686: adjusting learning rate of group 0 to 2.4711e-06.
outputs_pos.loss : 1.264519453048706
outputs_pos.loss : 0.8738728165626526
outputs_pos.loss : 1.2761194705963135
outputs_pos.loss : 0.7653976082801819
outputs_pos.loss : 1.0265238285064697
outputs_pos.loss : 1.2110297679901123
outputs_pos.loss : 0.902032732963562
outputs_pos.loss : 1.032997488975525
Epoch 00687: adjusting learning rate of group 0 to 2.4710e-06.
outputs_pos.loss : 1.1633356809616089
outputs_pos.loss : 1.7896884679794312
outputs_pos.loss : 1.0010943412780762
outputs_pos.loss : 1.3972375392913818
outputs_pos.loss : 1.1587762832641602
outputs_pos.loss : 1.0814669132232666
outputs_pos.loss : 1.1985191106796265
outputs_pos.loss : 1.4433733224868774
Epoch 00688: adjusting learning rate of group 0 to 2.4709e-06.
outputs_pos.loss : 0.9147176742553711
outputs_pos.loss : 0.953864336013794
outputs_pos.loss : 1.508756160736084
outputs_pos.loss : 0.9477248191833496
outputs_pos.loss : 1.2166638374328613
outputs_pos.loss : 1.1665605306625366
outputs_pos.loss : 2.4363906383514404
outputs_pos.loss : 1.2346673011779785
Epoch 00689: adjusting learning rate of group 0 to 2.4708e-06.
outputs_pos.loss : 1.2478413581848145
outputs_pos.loss : 1.1531163454055786
outputs_pos.loss : 1.5095232725143433
outputs_pos.loss : 1.474656105041504
outputs_pos.loss : 0.8716858625411987
outputs_pos.loss : 1.1727043390274048
outputs_pos.loss : 1.1204050779342651
outputs_pos.loss : 1.1083040237426758
Epoch 00690: adjusting learning rate of group 0 to 2.4707e-06.
outputs_pos.loss : 1.0154341459274292
outputs_pos.loss : 0.94917893409729
outputs_pos.loss : 1.15153968334198
outputs_pos.loss : 0.8727332353591919
outputs_pos.loss : 1.112693190574646
outputs_pos.loss : 1.2469170093536377
outputs_pos.loss : 1.6187671422958374
outputs_pos.loss : 1.0496337413787842
Epoch 00691: adjusting learning rate of group 0 to 2.4707e-06.
outputs_pos.loss : 1.7176746129989624
outputs_pos.loss : 1.4984956979751587
outputs_pos.loss : 2.044445276260376
outputs_pos.loss : 1.2924474477767944
outputs_pos.loss : 1.1156710386276245
outputs_pos.loss : 1.2895358800888062
outputs_pos.loss : 1.5289613008499146
outputs_pos.loss : 1.2695180177688599
Epoch 00692: adjusting learning rate of group 0 to 2.4706e-06.
outputs_pos.loss : 1.6556485891342163
outputs_pos.loss : 0.8251055479049683
outputs_pos.loss : 1.2667263746261597
outputs_pos.loss : 1.4955825805664062
outputs_pos.loss : 0.8317738771438599
outputs_pos.loss : 1.0607966184616089
outputs_pos.loss : 0.9686062932014465
outputs_pos.loss : 0.7718438506126404
Epoch 00693: adjusting learning rate of group 0 to 2.4705e-06.
outputs_pos.loss : 1.1701823472976685
outputs_pos.loss : 0.699769914150238
outputs_pos.loss : 0.8252630829811096
outputs_pos.loss : 1.0952199697494507
outputs_pos.loss : 1.1515600681304932
outputs_pos.loss : 1.1751433610916138
outputs_pos.loss : 1.3414894342422485
outputs_pos.loss : 1.2577664852142334
Epoch 00694: adjusting learning rate of group 0 to 2.4704e-06.
outputs_pos.loss : 1.0058852434158325
outputs_pos.loss : 1.2374114990234375
outputs_pos.loss : 1.810286045074463
outputs_pos.loss : 0.6964157819747925
outputs_pos.loss : 1.089240312576294
outputs_pos.loss : 0.9618885517120361
outputs_pos.loss : 1.3955258131027222
outputs_pos.loss : 1.093131184577942
Epoch 00695: adjusting learning rate of group 0 to 2.4703e-06.
outputs_pos.loss : 1.1928397417068481
outputs_pos.loss : 0.9857109785079956
outputs_pos.loss : 1.0519813299179077
outputs_pos.loss : 1.0983041524887085
outputs_pos.loss : 1.072428584098816
outputs_pos.loss : 1.4647687673568726
outputs_pos.loss : 1.1284915208816528
outputs_pos.loss : 1.7801311016082764
Epoch 00696: adjusting learning rate of group 0 to 2.4702e-06.
outputs_pos.loss : 0.8323074579238892
outputs_pos.loss : 0.9580044746398926
outputs_pos.loss : 1.1352179050445557
outputs_pos.loss : 1.197494387626648
outputs_pos.loss : 1.0661146640777588
outputs_pos.loss : 1.08632230758667
outputs_pos.loss : 1.2117948532104492
outputs_pos.loss : 0.9725858569145203
Epoch 00697: adjusting learning rate of group 0 to 2.4702e-06.
outputs_pos.loss : 1.4059239625930786
outputs_pos.loss : 0.8085893392562866
outputs_pos.loss : 1.4868931770324707
outputs_pos.loss : 1.2407866716384888
outputs_pos.loss : 1.0162874460220337
outputs_pos.loss : 1.2306745052337646
outputs_pos.loss : 1.3309959173202515
outputs_pos.loss : 1.188448429107666
Epoch 00698: adjusting learning rate of group 0 to 2.4701e-06.
outputs_pos.loss : 1.1878851652145386
outputs_pos.loss : 1.2427723407745361
outputs_pos.loss : 0.9198460578918457
outputs_pos.loss : 1.0631189346313477
outputs_pos.loss : 1.7198184728622437
outputs_pos.loss : 1.3676962852478027
outputs_pos.loss : 1.3892021179199219
outputs_pos.loss : 1.207836627960205
Epoch 00699: adjusting learning rate of group 0 to 2.4700e-06.
outputs_pos.loss : 1.1354426145553589
outputs_pos.loss : 1.1886967420578003
outputs_pos.loss : 1.2123234272003174
outputs_pos.loss : 1.4031932353973389
outputs_pos.loss : 1.61048424243927
outputs_pos.loss : 1.3880659341812134
outputs_pos.loss : 1.200018286705017
outputs_pos.loss : 1.1401753425598145
Epoch 00700: adjusting learning rate of group 0 to 2.4699e-06.
outputs_pos.loss : 1.214362621307373
outputs_pos.loss : 1.1592689752578735
outputs_pos.loss : 0.9627628922462463
outputs_pos.loss : 1.2753698825836182
outputs_pos.loss : 1.2468174695968628
outputs_pos.loss : 0.9331527948379517
outputs_pos.loss : 1.099930763244629
outputs_pos.loss : 0.8223405480384827
Epoch 00701: adjusting learning rate of group 0 to 2.4698e-06.
outputs_pos.loss : 1.5981382131576538
outputs_pos.loss : 1.469893217086792
outputs_pos.loss : 1.1133612394332886
outputs_pos.loss : 2.285290479660034
outputs_pos.loss : 1.175248384475708
outputs_pos.loss : 1.3675042390823364
outputs_pos.loss : 1.2087509632110596
outputs_pos.loss : 0.9577692151069641
Epoch 00702: adjusting learning rate of group 0 to 2.4697e-06.
outputs_pos.loss : 1.860190987586975
outputs_pos.loss : 1.1511913537979126
outputs_pos.loss : 1.6389561891555786
outputs_pos.loss : 1.4154291152954102
outputs_pos.loss : 1.2760611772537231
outputs_pos.loss : 1.301128625869751
outputs_pos.loss : 1.2297362089157104
outputs_pos.loss : 1.171664834022522
Epoch 00703: adjusting learning rate of group 0 to 2.4696e-06.
outputs_pos.loss : 1.465397596359253
outputs_pos.loss : 1.2247215509414673
outputs_pos.loss : 1.0770485401153564
outputs_pos.loss : 1.1223191022872925
outputs_pos.loss : 1.3639178276062012
outputs_pos.loss : 0.6055588126182556
outputs_pos.loss : 0.9457520246505737
outputs_pos.loss : 0.8742569088935852
Epoch 00704: adjusting learning rate of group 0 to 2.4696e-06.
outputs_pos.loss : 0.7879926562309265
outputs_pos.loss : 0.9567067623138428
outputs_pos.loss : 1.385423183441162
outputs_pos.loss : 1.125140905380249
outputs_pos.loss : 0.6818665862083435
outputs_pos.loss : 1.2365846633911133
outputs_pos.loss : 1.5399996042251587
outputs_pos.loss : 1.1336685419082642
Epoch 00705: adjusting learning rate of group 0 to 2.4695e-06.
outputs_pos.loss : 1.4141509532928467
outputs_pos.loss : 1.4800671339035034
outputs_pos.loss : 1.7833614349365234
outputs_pos.loss : 0.9807410836219788
outputs_pos.loss : 1.2613741159439087
outputs_pos.loss : 1.033448338508606
outputs_pos.loss : 1.1778461933135986
outputs_pos.loss : 1.1238185167312622
Epoch 00706: adjusting learning rate of group 0 to 2.4694e-06.
outputs_pos.loss : 1.201802134513855
outputs_pos.loss : 1.3057771921157837
outputs_pos.loss : 1.4442497491836548
outputs_pos.loss : 1.493182897567749
outputs_pos.loss : 1.346257209777832
outputs_pos.loss : 1.4687907695770264
outputs_pos.loss : 1.0473215579986572
outputs_pos.loss : 0.8186032176017761
Epoch 00707: adjusting learning rate of group 0 to 2.4693e-06.
outputs_pos.loss : 1.1421695947647095
outputs_pos.loss : 1.4118610620498657
outputs_pos.loss : 0.8549453020095825
outputs_pos.loss : 1.2060288190841675
outputs_pos.loss : 0.6724653840065002
outputs_pos.loss : 0.9532219767570496
outputs_pos.loss : 1.8706700801849365
outputs_pos.loss : 1.6382828950881958
Epoch 00708: adjusting learning rate of group 0 to 2.4692e-06.
outputs_pos.loss : 1.4495333433151245
outputs_pos.loss : 0.7721034288406372
outputs_pos.loss : 1.0758585929870605
outputs_pos.loss : 1.9845774173736572
outputs_pos.loss : 0.9278566241264343
outputs_pos.loss : 1.1196249723434448
outputs_pos.loss : 0.9994778633117676
outputs_pos.loss : 1.0566867589950562
Epoch 00709: adjusting learning rate of group 0 to 2.4691e-06.
outputs_pos.loss : 1.4075185060501099
outputs_pos.loss : 1.4616503715515137
outputs_pos.loss : 1.2436610460281372
outputs_pos.loss : 1.1199008226394653
outputs_pos.loss : 1.1028884649276733
outputs_pos.loss : 1.2131625413894653
outputs_pos.loss : 1.116676926612854
outputs_pos.loss : 1.034698724746704
Epoch 00710: adjusting learning rate of group 0 to 2.4690e-06.
outputs_pos.loss : 1.1030250787734985
outputs_pos.loss : 1.8919689655303955
outputs_pos.loss : 1.0955687761306763
outputs_pos.loss : 1.0803886651992798
outputs_pos.loss : 2.004477024078369
outputs_pos.loss : 0.7461008429527283
outputs_pos.loss : 1.1219451427459717
outputs_pos.loss : 0.8825925588607788
Epoch 00711: adjusting learning rate of group 0 to 2.4689e-06.
outputs_pos.loss : 1.6391799449920654
outputs_pos.loss : 1.5037205219268799
outputs_pos.loss : 1.082169771194458
outputs_pos.loss : 1.20096755027771
outputs_pos.loss : 1.1619794368743896
outputs_pos.loss : 1.0398110151290894
outputs_pos.loss : 1.6977005004882812
outputs_pos.loss : 1.305227518081665
Epoch 00712: adjusting learning rate of group 0 to 2.4689e-06.
outputs_pos.loss : 1.4045246839523315
outputs_pos.loss : 1.6959596872329712
outputs_pos.loss : 1.2553681135177612
outputs_pos.loss : 1.1217658519744873
outputs_pos.loss : 1.5382636785507202
outputs_pos.loss : 1.1064845323562622
outputs_pos.loss : 1.1562206745147705
outputs_pos.loss : 0.9813446402549744
Epoch 00713: adjusting learning rate of group 0 to 2.4688e-06.
outputs_pos.loss : 1.1076017618179321
outputs_pos.loss : 1.8828750848770142
outputs_pos.loss : 1.2483172416687012
outputs_pos.loss : 0.9295417070388794
outputs_pos.loss : 0.980853796005249
outputs_pos.loss : 1.042628526687622
outputs_pos.loss : 0.6266279220581055
outputs_pos.loss : 0.9345964193344116
Epoch 00714: adjusting learning rate of group 0 to 2.4687e-06.
outputs_pos.loss : 1.1814606189727783
outputs_pos.loss : 1.3694672584533691
outputs_pos.loss : 1.079196572303772
outputs_pos.loss : 1.0174182653427124
outputs_pos.loss : 1.1595443487167358
outputs_pos.loss : 1.1546765565872192
outputs_pos.loss : 1.0410003662109375
outputs_pos.loss : 1.4050915241241455
Epoch 00715: adjusting learning rate of group 0 to 2.4686e-06.
outputs_pos.loss : 1.124536156654358
outputs_pos.loss : 1.4109880924224854
outputs_pos.loss : 1.085141897201538
outputs_pos.loss : 1.1945912837982178
outputs_pos.loss : 0.5733271241188049
outputs_pos.loss : 1.3891633749008179
outputs_pos.loss : 0.7412059903144836
outputs_pos.loss : 1.1400935649871826
Epoch 00716: adjusting learning rate of group 0 to 2.4685e-06.
outputs_pos.loss : 1.5093263387680054
outputs_pos.loss : 1.1896978616714478
outputs_pos.loss : 0.982853889465332
outputs_pos.loss : 1.1188467741012573
outputs_pos.loss : 1.2563635110855103
outputs_pos.loss : 1.5559797286987305
outputs_pos.loss : 2.03709077835083
outputs_pos.loss : 1.5343973636627197
Epoch 00717: adjusting learning rate of group 0 to 2.4684e-06.
outputs_pos.loss : 1.2408322095870972
outputs_pos.loss : 1.183724284172058
outputs_pos.loss : 1.0811269283294678
outputs_pos.loss : 1.170059084892273
outputs_pos.loss : 1.2930952310562134
outputs_pos.loss : 1.2236881256103516
outputs_pos.loss : 1.0060912370681763
outputs_pos.loss : 1.3591749668121338
Epoch 00718: adjusting learning rate of group 0 to 2.4683e-06.
outputs_pos.loss : 1.1615428924560547
outputs_pos.loss : 1.4601608514785767
outputs_pos.loss : 1.2431398630142212
outputs_pos.loss : 1.1182610988616943
outputs_pos.loss : 1.402243733406067
outputs_pos.loss : 1.3794313669204712
outputs_pos.loss : 1.0711675882339478
outputs_pos.loss : 1.3075153827667236
Epoch 00719: adjusting learning rate of group 0 to 2.4682e-06.
outputs_pos.loss : 1.3321092128753662
outputs_pos.loss : 1.5734264850616455
outputs_pos.loss : 1.0120489597320557
outputs_pos.loss : 1.3798669576644897
outputs_pos.loss : 1.3084571361541748
outputs_pos.loss : 0.7404533624649048
outputs_pos.loss : 1.3214330673217773
outputs_pos.loss : 1.3571093082427979
Epoch 00720: adjusting learning rate of group 0 to 2.4682e-06.
outputs_pos.loss : 1.2695022821426392
outputs_pos.loss : 0.9280943870544434
outputs_pos.loss : 0.9232562780380249
outputs_pos.loss : 0.8207300901412964
outputs_pos.loss : 1.1094075441360474
outputs_pos.loss : 1.5334995985031128
outputs_pos.loss : 0.9565309882164001
outputs_pos.loss : 1.1065809726715088
Epoch 00721: adjusting learning rate of group 0 to 2.4681e-06.
outputs_pos.loss : 1.1976122856140137
outputs_pos.loss : 1.2123290300369263
outputs_pos.loss : 1.492150068283081
outputs_pos.loss : 1.4615955352783203
outputs_pos.loss : 0.9108077883720398
outputs_pos.loss : 1.091810941696167
outputs_pos.loss : 1.031563639640808
outputs_pos.loss : 1.5390384197235107
Epoch 00722: adjusting learning rate of group 0 to 2.4680e-06.
outputs_pos.loss : 1.4375724792480469
outputs_pos.loss : 0.9195258617401123
outputs_pos.loss : 1.1588687896728516
outputs_pos.loss : 1.1705138683319092
outputs_pos.loss : 1.5592659711837769
outputs_pos.loss : 1.1649178266525269
outputs_pos.loss : 0.9708399176597595
outputs_pos.loss : 1.2067269086837769
Epoch 00723: adjusting learning rate of group 0 to 2.4679e-06.
outputs_pos.loss : 1.2197974920272827
outputs_pos.loss : 1.1955909729003906
outputs_pos.loss : 1.0404220819473267
outputs_pos.loss : 1.5018055438995361
outputs_pos.loss : 1.3068927526474
outputs_pos.loss : 1.5731538534164429
outputs_pos.loss : 1.5558656454086304
outputs_pos.loss : 1.381891131401062
Epoch 00724: adjusting learning rate of group 0 to 2.4678e-06.
outputs_pos.loss : 1.0679210424423218
outputs_pos.loss : 0.8420953750610352
outputs_pos.loss : 1.1491748094558716
outputs_pos.loss : 1.1257048845291138
outputs_pos.loss : 1.3393839597702026
outputs_pos.loss : 1.004360556602478
outputs_pos.loss : 1.2752798795700073
outputs_pos.loss : 1.2993725538253784
Epoch 00725: adjusting learning rate of group 0 to 2.4677e-06.
outputs_pos.loss : 1.0376578569412231
outputs_pos.loss : 1.0305602550506592
outputs_pos.loss : 1.11448073387146
outputs_pos.loss : 1.2236603498458862
outputs_pos.loss : 1.1707394123077393
outputs_pos.loss : 1.409570574760437
outputs_pos.loss : 1.1889222860336304
outputs_pos.loss : 1.3314602375030518
Epoch 00726: adjusting learning rate of group 0 to 2.4676e-06.
outputs_pos.loss : 1.3808499574661255
outputs_pos.loss : 1.2726097106933594
outputs_pos.loss : 1.7340259552001953
outputs_pos.loss : 1.305128574371338
outputs_pos.loss : 1.4045665264129639
outputs_pos.loss : 1.4399628639221191
outputs_pos.loss : 1.6787391901016235
outputs_pos.loss : 1.4968786239624023
Epoch 00727: adjusting learning rate of group 0 to 2.4675e-06.
outputs_pos.loss : 0.788941502571106
outputs_pos.loss : 1.3812551498413086
outputs_pos.loss : 1.4205443859100342
outputs_pos.loss : 0.996990978717804
outputs_pos.loss : 1.2500343322753906
outputs_pos.loss : 1.3411848545074463
outputs_pos.loss : 1.6089481115341187
outputs_pos.loss : 1.4739837646484375
Epoch 00728: adjusting learning rate of group 0 to 2.4675e-06.
outputs_pos.loss : 1.2269549369812012
outputs_pos.loss : 1.1835969686508179
outputs_pos.loss : 1.3355653285980225
outputs_pos.loss : 1.023054838180542
outputs_pos.loss : 0.8390939831733704
outputs_pos.loss : 0.7319372892379761
outputs_pos.loss : 1.157631516456604
outputs_pos.loss : 1.6250566244125366
Epoch 00729: adjusting learning rate of group 0 to 2.4674e-06.
outputs_pos.loss : 1.2072298526763916
outputs_pos.loss : 0.9936814308166504
outputs_pos.loss : 1.483198642730713
outputs_pos.loss : 1.1281380653381348
outputs_pos.loss : 1.087086796760559
outputs_pos.loss : 1.2941288948059082
outputs_pos.loss : 1.3075017929077148
outputs_pos.loss : 1.2251644134521484
Epoch 00730: adjusting learning rate of group 0 to 2.4673e-06.
outputs_pos.loss : 0.9095026850700378
outputs_pos.loss : 1.1500494480133057
outputs_pos.loss : 1.3877813816070557
outputs_pos.loss : 1.467819094657898
outputs_pos.loss : 0.8189942240715027
outputs_pos.loss : 1.2747563123703003
outputs_pos.loss : 1.1311674118041992
outputs_pos.loss : 1.114962100982666
Epoch 00731: adjusting learning rate of group 0 to 2.4672e-06.
outputs_pos.loss : 0.9036351442337036
outputs_pos.loss : 1.031455159187317
outputs_pos.loss : 1.3867201805114746
outputs_pos.loss : 1.0020943880081177
outputs_pos.loss : 1.0288846492767334
outputs_pos.loss : 0.8914327025413513
outputs_pos.loss : 1.0528684854507446
outputs_pos.loss : 1.5114649534225464
Epoch 00732: adjusting learning rate of group 0 to 2.4671e-06.
outputs_pos.loss : 1.3295536041259766
outputs_pos.loss : 1.5196512937545776
outputs_pos.loss : 1.3951140642166138
outputs_pos.loss : 1.1269208192825317
outputs_pos.loss : 1.5727672576904297
outputs_pos.loss : 1.7539430856704712
outputs_pos.loss : 1.0296636819839478
outputs_pos.loss : 1.3001465797424316
Epoch 00733: adjusting learning rate of group 0 to 2.4670e-06.
outputs_pos.loss : 1.2666795253753662
outputs_pos.loss : 1.3749282360076904
outputs_pos.loss : 0.9399722814559937
outputs_pos.loss : 1.3412812948226929
outputs_pos.loss : 1.06797456741333
outputs_pos.loss : 1.211802363395691
outputs_pos.loss : 1.5234441757202148
outputs_pos.loss : 1.2973946332931519
Epoch 00734: adjusting learning rate of group 0 to 2.4669e-06.
outputs_pos.loss : 0.9750931859016418
outputs_pos.loss : 1.1051560640335083
outputs_pos.loss : 1.367128610610962
outputs_pos.loss : 1.0967350006103516
outputs_pos.loss : 1.363031029701233
outputs_pos.loss : 1.5640428066253662
outputs_pos.loss : 1.1661109924316406
outputs_pos.loss : 1.3618947267532349
Epoch 00735: adjusting learning rate of group 0 to 2.4668e-06.
outputs_pos.loss : 1.001158356666565
outputs_pos.loss : 1.3230488300323486
outputs_pos.loss : 0.9003047943115234
outputs_pos.loss : 1.4750202894210815
outputs_pos.loss : 1.1994291543960571
outputs_pos.loss : 1.2293838262557983
outputs_pos.loss : 0.8636792898178101
outputs_pos.loss : 1.186281681060791
Epoch 00736: adjusting learning rate of group 0 to 2.4667e-06.
outputs_pos.loss : 0.9991922378540039
outputs_pos.loss : 1.9963332414627075
outputs_pos.loss : 1.2719314098358154
outputs_pos.loss : 0.6888465285301208
outputs_pos.loss : 1.5356030464172363
outputs_pos.loss : 0.9013787508010864
outputs_pos.loss : 1.2511825561523438
outputs_pos.loss : 1.1644871234893799
Epoch 00737: adjusting learning rate of group 0 to 2.4666e-06.
outputs_pos.loss : 1.1810084581375122
outputs_pos.loss : 1.0799680948257446
outputs_pos.loss : 0.9377917051315308
outputs_pos.loss : 1.029068946838379
outputs_pos.loss : 0.7338176965713501
outputs_pos.loss : 0.9341170191764832
outputs_pos.loss : 1.239646553993225
outputs_pos.loss : 1.0683791637420654
Epoch 00738: adjusting learning rate of group 0 to 2.4666e-06.
outputs_pos.loss : 1.355522871017456
outputs_pos.loss : 2.1425206661224365
outputs_pos.loss : 0.8387322425842285
outputs_pos.loss : 1.529508113861084
outputs_pos.loss : 1.192884087562561
outputs_pos.loss : 1.2533434629440308
outputs_pos.loss : 1.484505534172058
outputs_pos.loss : 1.2932084798812866
Epoch 00739: adjusting learning rate of group 0 to 2.4665e-06.
outputs_pos.loss : 1.2759767770767212
outputs_pos.loss : 1.279402732849121
outputs_pos.loss : 1.3773822784423828
outputs_pos.loss : 1.2685266733169556
outputs_pos.loss : 1.353378176689148
outputs_pos.loss : 1.0437936782836914
outputs_pos.loss : 1.1171290874481201
outputs_pos.loss : 1.4318372011184692
Epoch 00740: adjusting learning rate of group 0 to 2.4664e-06.
outputs_pos.loss : 1.6353007555007935
outputs_pos.loss : 1.3939945697784424
outputs_pos.loss : 1.2999515533447266
outputs_pos.loss : 1.5960851907730103
outputs_pos.loss : 1.2060409784317017
outputs_pos.loss : 1.3327441215515137
outputs_pos.loss : 1.2964770793914795
outputs_pos.loss : 1.2691097259521484
Epoch 00741: adjusting learning rate of group 0 to 2.4663e-06.
outputs_pos.loss : 1.5636929273605347
outputs_pos.loss : 0.9705111384391785
outputs_pos.loss : 1.0361618995666504
outputs_pos.loss : 1.184708833694458
outputs_pos.loss : 1.082861304283142
outputs_pos.loss : 1.2040598392486572
outputs_pos.loss : 1.6031638383865356
outputs_pos.loss : 1.6781164407730103
Epoch 00742: adjusting learning rate of group 0 to 2.4662e-06.
outputs_pos.loss : 1.4842009544372559
outputs_pos.loss : 1.0289907455444336
outputs_pos.loss : 1.1381936073303223
outputs_pos.loss : 1.0342451333999634
outputs_pos.loss : 1.7833747863769531
outputs_pos.loss : 1.6849253177642822
outputs_pos.loss : 1.356687307357788
outputs_pos.loss : 1.2731953859329224
Epoch 00743: adjusting learning rate of group 0 to 2.4661e-06.
outputs_pos.loss : 1.2092939615249634
outputs_pos.loss : 1.9535174369812012
outputs_pos.loss : 1.3374707698822021
outputs_pos.loss : 1.0943255424499512
outputs_pos.loss : 1.573451280593872
outputs_pos.loss : 1.144091248512268
outputs_pos.loss : 1.1730701923370361
outputs_pos.loss : 1.408102035522461
Epoch 00744: adjusting learning rate of group 0 to 2.4660e-06.
outputs_pos.loss : 0.9657642841339111
outputs_pos.loss : 1.348097562789917
outputs_pos.loss : 1.3872350454330444
outputs_pos.loss : 1.6913931369781494
outputs_pos.loss : 1.374150037765503
outputs_pos.loss : 1.0349022150039673
outputs_pos.loss : 0.8440731167793274
outputs_pos.loss : 1.2939122915267944
Epoch 00745: adjusting learning rate of group 0 to 2.4659e-06.
outputs_pos.loss : 1.0511294603347778
outputs_pos.loss : 1.5057021379470825
outputs_pos.loss : 0.7948881387710571
outputs_pos.loss : 2.1547718048095703
outputs_pos.loss : 0.9718716740608215
outputs_pos.loss : 1.123230218887329
outputs_pos.loss : 1.110952377319336
outputs_pos.loss : 1.7509188652038574
Epoch 00746: adjusting learning rate of group 0 to 2.4658e-06.
outputs_pos.loss : 0.8498422503471375
outputs_pos.loss : 1.2689249515533447
outputs_pos.loss : 1.0178948640823364
outputs_pos.loss : 1.1012085676193237
outputs_pos.loss : 0.7570226788520813
outputs_pos.loss : 1.0139309167861938
outputs_pos.loss : 1.0242691040039062
outputs_pos.loss : 1.0474790334701538
Epoch 00747: adjusting learning rate of group 0 to 2.4657e-06.
outputs_pos.loss : 1.0721946954727173
outputs_pos.loss : 0.9092620015144348
outputs_pos.loss : 0.9216796159744263
outputs_pos.loss : 1.5491914749145508
outputs_pos.loss : 0.9422656893730164
outputs_pos.loss : 1.9326505661010742
outputs_pos.loss : 1.067394495010376
outputs_pos.loss : 0.9668786525726318
Epoch 00748: adjusting learning rate of group 0 to 2.4656e-06.
outputs_pos.loss : 1.1388568878173828
outputs_pos.loss : 0.9476235508918762
outputs_pos.loss : 1.0934144258499146
outputs_pos.loss : 1.3529977798461914
outputs_pos.loss : 1.1729936599731445
outputs_pos.loss : 1.2300947904586792
outputs_pos.loss : 1.045692801475525
outputs_pos.loss : 1.0774184465408325
Epoch 00749: adjusting learning rate of group 0 to 2.4656e-06.
outputs_pos.loss : 1.2439372539520264
outputs_pos.loss : 0.8676886558532715
outputs_pos.loss : 1.3028851747512817
outputs_pos.loss : 0.8143637776374817
outputs_pos.loss : 1.6646091938018799
outputs_pos.loss : 1.0358638763427734
outputs_pos.loss : 1.5946553945541382
outputs_pos.loss : 1.712485671043396
Epoch 00750: adjusting learning rate of group 0 to 2.4655e-06.
outputs_pos.loss : 1.1001416444778442
outputs_pos.loss : 1.3501614332199097
outputs_pos.loss : 1.2682300806045532
outputs_pos.loss : 1.2206653356552124
outputs_pos.loss : 1.4262574911117554
outputs_pos.loss : 1.4896626472473145
outputs_pos.loss : 0.8268816471099854
outputs_pos.loss : 1.5676311254501343
Epoch 00751: adjusting learning rate of group 0 to 2.4654e-06.
outputs_pos.loss : 1.1310031414031982
outputs_pos.loss : 1.4297407865524292
outputs_pos.loss : 1.1656765937805176
outputs_pos.loss : 1.2649860382080078
outputs_pos.loss : 0.6746780872344971
outputs_pos.loss : 1.2418920993804932
outputs_pos.loss : 1.9826499223709106
outputs_pos.loss : 0.8725882768630981
Epoch 00752: adjusting learning rate of group 0 to 2.4653e-06.
outputs_pos.loss : 1.2230801582336426
outputs_pos.loss : 1.1504453420639038
outputs_pos.loss : 0.9402924180030823
outputs_pos.loss : 0.744566023349762
outputs_pos.loss : 1.0798790454864502
outputs_pos.loss : 1.5311216115951538
outputs_pos.loss : 0.9993136525154114
outputs_pos.loss : 1.092241644859314
Epoch 00753: adjusting learning rate of group 0 to 2.4652e-06.
outputs_pos.loss : 1.601040005683899
outputs_pos.loss : 1.2135213613510132
outputs_pos.loss : 1.0069797039031982
outputs_pos.loss : 1.9503666162490845
outputs_pos.loss : 1.3296595811843872
outputs_pos.loss : 0.7951063513755798
outputs_pos.loss : 1.1647332906723022
outputs_pos.loss : 1.1192686557769775
Epoch 00754: adjusting learning rate of group 0 to 2.4651e-06.
outputs_pos.loss : 1.3330111503601074
outputs_pos.loss : 1.2437368631362915
outputs_pos.loss : 1.378942847251892
outputs_pos.loss : 1.1299577951431274
outputs_pos.loss : 0.800900399684906
outputs_pos.loss : 0.9796709418296814
outputs_pos.loss : 1.009107232093811
outputs_pos.loss : 0.808807373046875
Epoch 00755: adjusting learning rate of group 0 to 2.4650e-06.
outputs_pos.loss : 1.0556336641311646
outputs_pos.loss : 1.471670389175415
outputs_pos.loss : 1.2798818349838257
outputs_pos.loss : 1.457007646560669
outputs_pos.loss : 1.1828781366348267
outputs_pos.loss : 1.0873124599456787
outputs_pos.loss : 1.2958606481552124
outputs_pos.loss : 1.281199336051941
Epoch 00756: adjusting learning rate of group 0 to 2.4649e-06.
outputs_pos.loss : 1.4305870532989502
outputs_pos.loss : 0.975011944770813
outputs_pos.loss : 1.0024296045303345
outputs_pos.loss : 1.0793027877807617
outputs_pos.loss : 1.2783218622207642
outputs_pos.loss : 1.1745524406433105
outputs_pos.loss : 1.095706582069397
outputs_pos.loss : 1.868445634841919
Epoch 00757: adjusting learning rate of group 0 to 2.4648e-06.
outputs_pos.loss : 1.3220455646514893
outputs_pos.loss : 1.2381694316864014
outputs_pos.loss : 1.4506851434707642
outputs_pos.loss : 1.2140989303588867
outputs_pos.loss : 0.8186282515525818
outputs_pos.loss : 1.260213017463684
outputs_pos.loss : 1.0368388891220093
outputs_pos.loss : 1.1118665933609009
Epoch 00758: adjusting learning rate of group 0 to 2.4647e-06.
outputs_pos.loss : 1.4586725234985352
outputs_pos.loss : 1.4647953510284424
outputs_pos.loss : 0.7487585544586182
outputs_pos.loss : 1.0875126123428345
outputs_pos.loss : 1.0758423805236816
outputs_pos.loss : 0.9348760843276978
outputs_pos.loss : 0.9519486427307129
outputs_pos.loss : 1.6400609016418457
Epoch 00759: adjusting learning rate of group 0 to 2.4646e-06.
outputs_pos.loss : 1.4899259805679321
outputs_pos.loss : 1.0906493663787842
outputs_pos.loss : 1.1499038934707642
outputs_pos.loss : 1.2996715307235718
outputs_pos.loss : 0.8721235990524292
outputs_pos.loss : 1.3399991989135742
outputs_pos.loss : 1.6944273710250854
outputs_pos.loss : 1.2385344505310059
Epoch 00760: adjusting learning rate of group 0 to 2.4645e-06.
outputs_pos.loss : 1.0664035081863403
outputs_pos.loss : 1.493334174156189
outputs_pos.loss : 0.9837864637374878
outputs_pos.loss : 0.9299213290214539
outputs_pos.loss : 0.619867742061615
outputs_pos.loss : 0.8974928259849548
outputs_pos.loss : 0.665014922618866
outputs_pos.loss : 1.0478214025497437
Epoch 00761: adjusting learning rate of group 0 to 2.4644e-06.
outputs_pos.loss : 1.0156759023666382
outputs_pos.loss : 1.2650752067565918
outputs_pos.loss : 1.0502617359161377
outputs_pos.loss : 1.1112560033798218
outputs_pos.loss : 1.38747239112854
outputs_pos.loss : 1.236550211906433
outputs_pos.loss : 1.0703516006469727
outputs_pos.loss : 1.1675227880477905
Epoch 00762: adjusting learning rate of group 0 to 2.4644e-06.
outputs_pos.loss : 1.4556220769882202
outputs_pos.loss : 1.0330466032028198
outputs_pos.loss : 1.4880963563919067
outputs_pos.loss : 1.1511939764022827
outputs_pos.loss : 1.388248324394226
outputs_pos.loss : 0.9528396725654602
outputs_pos.loss : 1.1951252222061157
outputs_pos.loss : 1.7187421321868896
Epoch 00763: adjusting learning rate of group 0 to 2.4643e-06.
outputs_pos.loss : 0.9998835921287537
outputs_pos.loss : 1.6024178266525269
outputs_pos.loss : 1.165586233139038
outputs_pos.loss : 1.2587343454360962
outputs_pos.loss : 0.6138637065887451
outputs_pos.loss : 1.2053563594818115
outputs_pos.loss : 1.936246633529663
outputs_pos.loss : 1.1786764860153198
Epoch 00764: adjusting learning rate of group 0 to 2.4642e-06.
outputs_pos.loss : 1.2882609367370605
outputs_pos.loss : 1.4769631624221802
outputs_pos.loss : 1.0697355270385742
outputs_pos.loss : 1.4422900676727295
outputs_pos.loss : 1.2115010023117065
outputs_pos.loss : 1.1075652837753296
outputs_pos.loss : 1.1160837411880493
outputs_pos.loss : 1.3971976041793823
Epoch 00765: adjusting learning rate of group 0 to 2.4641e-06.
outputs_pos.loss : 1.2187663316726685
outputs_pos.loss : 0.9518162608146667
outputs_pos.loss : 1.4421331882476807
outputs_pos.loss : 1.2035919427871704
outputs_pos.loss : 1.298595905303955
outputs_pos.loss : 0.5545139312744141
outputs_pos.loss : 1.0978891849517822
outputs_pos.loss : 1.187723159790039
Epoch 00766: adjusting learning rate of group 0 to 2.4640e-06.
outputs_pos.loss : 0.8896721601486206
outputs_pos.loss : 1.0410809516906738
outputs_pos.loss : 0.8501514792442322
outputs_pos.loss : 1.092665195465088
outputs_pos.loss : 1.0828368663787842
outputs_pos.loss : 1.1805764436721802
outputs_pos.loss : 1.1993207931518555
outputs_pos.loss : 1.6562343835830688
Epoch 00767: adjusting learning rate of group 0 to 2.4639e-06.
outputs_pos.loss : 1.5545313358306885
outputs_pos.loss : 1.4530971050262451
outputs_pos.loss : 0.7810540795326233
outputs_pos.loss : 1.1564197540283203
outputs_pos.loss : 1.4562469720840454
outputs_pos.loss : 0.9274255037307739
outputs_pos.loss : 1.0630425214767456
outputs_pos.loss : 1.4793705940246582
Epoch 00768: adjusting learning rate of group 0 to 2.4638e-06.
outputs_pos.loss : 1.3776726722717285
outputs_pos.loss : 1.4812675714492798
outputs_pos.loss : 1.4217768907546997
outputs_pos.loss : 1.026536464691162
outputs_pos.loss : 1.0515424013137817
outputs_pos.loss : 1.318302035331726
outputs_pos.loss : 1.1548740863800049
outputs_pos.loss : 0.9956602454185486
Epoch 00769: adjusting learning rate of group 0 to 2.4637e-06.
outputs_pos.loss : 0.8976082801818848
outputs_pos.loss : 1.5631183385849
outputs_pos.loss : 1.0113542079925537
outputs_pos.loss : 1.0719740390777588
outputs_pos.loss : 1.3437983989715576
outputs_pos.loss : 1.2700159549713135
outputs_pos.loss : 0.9224612712860107
outputs_pos.loss : 1.442566990852356
Epoch 00770: adjusting learning rate of group 0 to 2.4636e-06.
outputs_pos.loss : 1.832716941833496
outputs_pos.loss : 1.6981310844421387
outputs_pos.loss : 0.9464718103408813
outputs_pos.loss : 1.0194001197814941
outputs_pos.loss : 1.313277006149292
outputs_pos.loss : 1.1435141563415527
outputs_pos.loss : 1.2562669515609741
outputs_pos.loss : 1.506302833557129
Epoch 00771: adjusting learning rate of group 0 to 2.4635e-06.
outputs_pos.loss : 0.9788985252380371
outputs_pos.loss : 1.14765465259552
outputs_pos.loss : 1.4580446481704712
outputs_pos.loss : 1.1954954862594604
outputs_pos.loss : 0.8946642875671387
outputs_pos.loss : 1.4832350015640259
outputs_pos.loss : 1.0427252054214478
outputs_pos.loss : 1.4605964422225952
Epoch 00772: adjusting learning rate of group 0 to 2.4634e-06.
outputs_pos.loss : 0.9390614032745361
outputs_pos.loss : 1.2957143783569336
outputs_pos.loss : 1.0686899423599243
outputs_pos.loss : 1.4523699283599854
outputs_pos.loss : 1.0284088850021362
outputs_pos.loss : 1.3983817100524902
outputs_pos.loss : 1.2643839120864868
outputs_pos.loss : 0.9285938143730164
Epoch 00773: adjusting learning rate of group 0 to 2.4633e-06.
outputs_pos.loss : 0.7587398886680603
outputs_pos.loss : 1.186794638633728
outputs_pos.loss : 1.666622281074524
outputs_pos.loss : 1.6157673597335815
outputs_pos.loss : 1.0966955423355103
outputs_pos.loss : 1.2147663831710815
outputs_pos.loss : 0.9433040022850037
outputs_pos.loss : 1.025899052619934
Epoch 00774: adjusting learning rate of group 0 to 2.4632e-06.
outputs_pos.loss : 1.4067931175231934
outputs_pos.loss : 1.465545654296875
outputs_pos.loss : 1.0044649839401245
outputs_pos.loss : 1.1421120166778564
outputs_pos.loss : 1.3138333559036255
outputs_pos.loss : 1.0822010040283203
outputs_pos.loss : 0.9925932884216309
outputs_pos.loss : 1.0609568357467651
Epoch 00775: adjusting learning rate of group 0 to 2.4631e-06.
outputs_pos.loss : 1.1771832704544067
outputs_pos.loss : 0.995093822479248
outputs_pos.loss : 0.9971123337745667
outputs_pos.loss : 1.5868258476257324
outputs_pos.loss : 1.394052267074585
outputs_pos.loss : 1.2837029695510864
outputs_pos.loss : 0.8141574263572693
outputs_pos.loss : 1.0351295471191406
Epoch 00776: adjusting learning rate of group 0 to 2.4630e-06.
outputs_pos.loss : 1.0759352445602417
outputs_pos.loss : 1.8104099035263062
outputs_pos.loss : 0.6523390412330627
outputs_pos.loss : 1.2264763116836548
outputs_pos.loss : 0.9404885172843933
outputs_pos.loss : 1.1321086883544922
outputs_pos.loss : 0.964588463306427
outputs_pos.loss : 1.107839822769165
Epoch 00777: adjusting learning rate of group 0 to 2.4629e-06.
outputs_pos.loss : 1.319851279258728
outputs_pos.loss : 1.0975779294967651
outputs_pos.loss : 0.9518282413482666
outputs_pos.loss : 0.9838959574699402
outputs_pos.loss : 1.576994776725769
outputs_pos.loss : 1.4797217845916748
outputs_pos.loss : 1.6886767148971558
outputs_pos.loss : 1.0874097347259521
Epoch 00778: adjusting learning rate of group 0 to 2.4628e-06.
outputs_pos.loss : 1.33944833278656
outputs_pos.loss : 1.4063434600830078
outputs_pos.loss : 1.0741028785705566
outputs_pos.loss : 1.1568024158477783
outputs_pos.loss : 1.2691508531570435
outputs_pos.loss : 0.9190470576286316
outputs_pos.loss : 0.7350713610649109
outputs_pos.loss : 1.3665862083435059
Epoch 00779: adjusting learning rate of group 0 to 2.4628e-06.
outputs_pos.loss : 0.8463742136955261
outputs_pos.loss : 1.2523239850997925
outputs_pos.loss : 1.0990307331085205
outputs_pos.loss : 1.1884180307388306
outputs_pos.loss : 1.0452345609664917
outputs_pos.loss : 1.4085124731063843
outputs_pos.loss : 1.90010404586792
outputs_pos.loss : 1.1273220777511597
Epoch 00780: adjusting learning rate of group 0 to 2.4627e-06.
outputs_pos.loss : 0.7949591279029846
outputs_pos.loss : 1.1047042608261108
outputs_pos.loss : 1.1217666864395142
outputs_pos.loss : 0.7501164674758911
outputs_pos.loss : 0.8301090002059937
outputs_pos.loss : 0.9383893013000488
outputs_pos.loss : 1.0520262718200684
outputs_pos.loss : 1.0650326013565063
Epoch 00781: adjusting learning rate of group 0 to 2.4626e-06.
outputs_pos.loss : 1.0134717226028442
outputs_pos.loss : 1.0654412508010864
outputs_pos.loss : 1.0872608423233032
outputs_pos.loss : 1.3974887132644653
outputs_pos.loss : 1.3371999263763428
outputs_pos.loss : 0.9602241516113281
outputs_pos.loss : 1.1738032102584839
outputs_pos.loss : 0.8632118701934814
Epoch 00782: adjusting learning rate of group 0 to 2.4625e-06.
outputs_pos.loss : 0.9938482642173767
outputs_pos.loss : 1.356584906578064
outputs_pos.loss : 1.3019039630889893
outputs_pos.loss : 1.0731194019317627
outputs_pos.loss : 1.2097278833389282
outputs_pos.loss : 0.7512597441673279
outputs_pos.loss : 1.0466482639312744
outputs_pos.loss : 0.9188938736915588
Epoch 00783: adjusting learning rate of group 0 to 2.4624e-06.
outputs_pos.loss : 1.5458860397338867
outputs_pos.loss : 1.8231078386306763
outputs_pos.loss : 1.239202857017517
outputs_pos.loss : 0.8968812227249146
outputs_pos.loss : 1.0001137256622314
outputs_pos.loss : 1.0044752359390259
outputs_pos.loss : 1.5310897827148438
outputs_pos.loss : 0.897887110710144
Epoch 00784: adjusting learning rate of group 0 to 2.4623e-06.
outputs_pos.loss : 1.0351721048355103
outputs_pos.loss : 1.0778192281723022
outputs_pos.loss : 0.8199656009674072
outputs_pos.loss : 1.3707666397094727
outputs_pos.loss : 1.1709522008895874
outputs_pos.loss : 1.1829317808151245
outputs_pos.loss : 1.4803675413131714
outputs_pos.loss : 0.9281220436096191
Epoch 00785: adjusting learning rate of group 0 to 2.4622e-06.
outputs_pos.loss : 1.1509387493133545
outputs_pos.loss : 1.2206352949142456
outputs_pos.loss : 1.0365020036697388
outputs_pos.loss : 1.1485788822174072
outputs_pos.loss : 1.0246325731277466
outputs_pos.loss : 0.8911321759223938
outputs_pos.loss : 0.8150426149368286
outputs_pos.loss : 1.3415577411651611
Epoch 00786: adjusting learning rate of group 0 to 2.4621e-06.
outputs_pos.loss : 1.3141855001449585
outputs_pos.loss : 1.2034852504730225
outputs_pos.loss : 1.4610710144042969
outputs_pos.loss : 0.8549785614013672
outputs_pos.loss : 1.0071847438812256
outputs_pos.loss : 1.3632547855377197
outputs_pos.loss : 1.2649763822555542
outputs_pos.loss : 1.0114564895629883
Epoch 00787: adjusting learning rate of group 0 to 2.4620e-06.
outputs_pos.loss : 0.7697198987007141
outputs_pos.loss : 0.8658484816551208
outputs_pos.loss : 0.9341306686401367
outputs_pos.loss : 1.542402744293213
outputs_pos.loss : 1.2098685503005981
outputs_pos.loss : 1.698382019996643
outputs_pos.loss : 1.5573066473007202
outputs_pos.loss : 1.279923915863037
Epoch 00788: adjusting learning rate of group 0 to 2.4619e-06.
outputs_pos.loss : 0.9788984060287476
outputs_pos.loss : 0.8886968493461609
outputs_pos.loss : 1.0253229141235352
outputs_pos.loss : 1.2905112504959106
outputs_pos.loss : 0.8678502440452576
outputs_pos.loss : 0.9566429853439331
outputs_pos.loss : 1.4848663806915283
outputs_pos.loss : 1.331670880317688
Epoch 00789: adjusting learning rate of group 0 to 2.4618e-06.
outputs_pos.loss : 0.9276068210601807
outputs_pos.loss : 1.3458354473114014
outputs_pos.loss : 1.4553126096725464
outputs_pos.loss : 1.0018446445465088
outputs_pos.loss : 1.132625699043274
outputs_pos.loss : 1.008548378944397
outputs_pos.loss : 1.1199772357940674
outputs_pos.loss : 1.2794989347457886
Epoch 00790: adjusting learning rate of group 0 to 2.4617e-06.
outputs_pos.loss : 1.483101487159729
outputs_pos.loss : 1.3504958152770996
outputs_pos.loss : 1.1902012825012207
outputs_pos.loss : 1.0973930358886719
outputs_pos.loss : 1.2897733449935913
outputs_pos.loss : 1.2077597379684448
outputs_pos.loss : 1.6221657991409302
outputs_pos.loss : 1.5327508449554443
Epoch 00791: adjusting learning rate of group 0 to 2.4616e-06.
outputs_pos.loss : 1.0321391820907593
outputs_pos.loss : 0.8444142937660217
outputs_pos.loss : 2.3792762756347656
outputs_pos.loss : 1.461706280708313
outputs_pos.loss : 0.9653380513191223
outputs_pos.loss : 1.4561980962753296
outputs_pos.loss : 0.6581121683120728
outputs_pos.loss : 0.7942416071891785
Epoch 00792: adjusting learning rate of group 0 to 2.4615e-06.
outputs_pos.loss : 0.9616657495498657
outputs_pos.loss : 1.5010106563568115
outputs_pos.loss : 1.19111168384552
outputs_pos.loss : 1.4480056762695312
outputs_pos.loss : 1.2687480449676514
outputs_pos.loss : 1.2465722560882568
outputs_pos.loss : 1.1544430255889893
outputs_pos.loss : 1.0518054962158203
Epoch 00793: adjusting learning rate of group 0 to 2.4614e-06.
outputs_pos.loss : 1.0185548067092896
outputs_pos.loss : 1.1779944896697998
outputs_pos.loss : 1.177701711654663
outputs_pos.loss : 1.2829166650772095
outputs_pos.loss : 0.9633625745773315
outputs_pos.loss : 0.9057843089103699
outputs_pos.loss : 1.3355143070220947
outputs_pos.loss : 1.256811499595642
Epoch 00794: adjusting learning rate of group 0 to 2.4613e-06.
outputs_pos.loss : 1.1682016849517822
outputs_pos.loss : 1.171511173248291
outputs_pos.loss : 0.9937292337417603
outputs_pos.loss : 0.8378972411155701
outputs_pos.loss : 0.8526063561439514
outputs_pos.loss : 1.8903006315231323
outputs_pos.loss : 1.537524938583374
outputs_pos.loss : 1.642629861831665
Epoch 00795: adjusting learning rate of group 0 to 2.4612e-06.
outputs_pos.loss : 0.9463273286819458
outputs_pos.loss : 1.2301414012908936
outputs_pos.loss : 1.3463701009750366
outputs_pos.loss : 1.3439967632293701
outputs_pos.loss : 0.8067122101783752
outputs_pos.loss : 1.513627052307129
outputs_pos.loss : 1.3689967393875122
outputs_pos.loss : 1.280597448348999
Epoch 00796: adjusting learning rate of group 0 to 2.4611e-06.
outputs_pos.loss : 1.0324106216430664
outputs_pos.loss : 1.187410831451416
outputs_pos.loss : 0.9835828542709351
outputs_pos.loss : 1.1114883422851562
outputs_pos.loss : 1.3244297504425049
outputs_pos.loss : 1.3616899251937866
outputs_pos.loss : 0.9553664922714233
outputs_pos.loss : 0.754280149936676
Epoch 00797: adjusting learning rate of group 0 to 2.4610e-06.
outputs_pos.loss : 1.360555648803711
outputs_pos.loss : 1.1179133653640747
outputs_pos.loss : 1.1776968240737915
outputs_pos.loss : 1.0183244943618774
outputs_pos.loss : 0.8898023962974548
outputs_pos.loss : 1.2432191371917725
outputs_pos.loss : 1.5098845958709717
outputs_pos.loss : 1.1963664293289185
Epoch 00798: adjusting learning rate of group 0 to 2.4609e-06.
outputs_pos.loss : 1.4515726566314697
outputs_pos.loss : 1.374982237815857
outputs_pos.loss : 1.138393521308899
outputs_pos.loss : 1.1343713998794556
outputs_pos.loss : 1.4753243923187256
outputs_pos.loss : 1.1077089309692383
outputs_pos.loss : 1.4524784088134766
outputs_pos.loss : 1.3677773475646973
Epoch 00799: adjusting learning rate of group 0 to 2.4608e-06.
outputs_pos.loss : 1.2663394212722778
outputs_pos.loss : 1.1839210987091064
outputs_pos.loss : 0.9566642045974731
outputs_pos.loss : 1.281092882156372
outputs_pos.loss : 1.0924079418182373
outputs_pos.loss : 1.317684531211853
outputs_pos.loss : 1.4435932636260986
outputs_pos.loss : 1.0917097330093384
Epoch 00800: adjusting learning rate of group 0 to 2.4607e-06.
outputs_pos.loss : 1.4665571451187134
outputs_pos.loss : 1.351280927658081
outputs_pos.loss : 0.8887074589729309
outputs_pos.loss : 1.2336403131484985
outputs_pos.loss : 1.3924833536148071
outputs_pos.loss : 1.1609337329864502
outputs_pos.loss : 0.8403447270393372
outputs_pos.loss : 1.6682153940200806
Epoch 00801: adjusting learning rate of group 0 to 2.4606e-06.
outputs_pos.loss : 1.458655834197998
outputs_pos.loss : 0.9154987931251526
outputs_pos.loss : 1.0610980987548828
outputs_pos.loss : 1.3015739917755127
outputs_pos.loss : 1.551550269126892
outputs_pos.loss : 1.4243390560150146
outputs_pos.loss : 1.1753805875778198
outputs_pos.loss : 1.2474560737609863
Epoch 00802: adjusting learning rate of group 0 to 2.4605e-06.
outputs_pos.loss : 1.3421781063079834
outputs_pos.loss : 1.0378082990646362
outputs_pos.loss : 1.0682690143585205
outputs_pos.loss : 1.0430887937545776
outputs_pos.loss : 1.353051781654358
outputs_pos.loss : 1.174891710281372
outputs_pos.loss : 1.5074784755706787
outputs_pos.loss : 0.9404529333114624
Epoch 00803: adjusting learning rate of group 0 to 2.4604e-06.
outputs_pos.loss : 0.9381653666496277
outputs_pos.loss : 1.3109793663024902
outputs_pos.loss : 1.043904423713684
outputs_pos.loss : 1.207538366317749
outputs_pos.loss : 0.7322752475738525
outputs_pos.loss : 1.1731411218643188
outputs_pos.loss : 0.9311076998710632
outputs_pos.loss : 0.8780643343925476
Epoch 00804: adjusting learning rate of group 0 to 2.4603e-06.
outputs_pos.loss : 0.8167874813079834
outputs_pos.loss : 0.9478459358215332
outputs_pos.loss : 1.3140000104904175
outputs_pos.loss : 0.9574592113494873
outputs_pos.loss : 0.8372596502304077
outputs_pos.loss : 0.948662281036377
outputs_pos.loss : 0.8689599633216858
outputs_pos.loss : 1.311320424079895
Epoch 00805: adjusting learning rate of group 0 to 2.4602e-06.
outputs_pos.loss : 0.9690645337104797
outputs_pos.loss : 1.3270015716552734
outputs_pos.loss : 1.0727289915084839
outputs_pos.loss : 0.7678305506706238
outputs_pos.loss : 1.146001935005188
outputs_pos.loss : 0.6668937802314758
outputs_pos.loss : 1.9541754722595215
outputs_pos.loss : 1.228284239768982
Epoch 00806: adjusting learning rate of group 0 to 2.4601e-06.
outputs_pos.loss : 1.8914871215820312
outputs_pos.loss : 1.372098445892334
outputs_pos.loss : 1.108093500137329
outputs_pos.loss : 1.5133777856826782
outputs_pos.loss : 0.830974280834198
outputs_pos.loss : 1.2530969381332397
outputs_pos.loss : 1.2555185556411743
outputs_pos.loss : 2.0774121284484863
Epoch 00807: adjusting learning rate of group 0 to 2.4600e-06.
outputs_pos.loss : 1.335811972618103
outputs_pos.loss : 1.3467378616333008
outputs_pos.loss : 1.7040458917617798
outputs_pos.loss : 1.0561983585357666
outputs_pos.loss : 1.2145951986312866
outputs_pos.loss : 1.122692584991455
outputs_pos.loss : 1.7325680255889893
outputs_pos.loss : 1.2618906497955322
Epoch 00808: adjusting learning rate of group 0 to 2.4599e-06.
outputs_pos.loss : 1.6094900369644165
outputs_pos.loss : 1.1134788990020752
outputs_pos.loss : 0.9909249544143677
outputs_pos.loss : 1.235731840133667
outputs_pos.loss : 1.509328842163086
outputs_pos.loss : 0.9708631634712219
outputs_pos.loss : 1.2201955318450928
outputs_pos.loss : 1.4422266483306885
Epoch 00809: adjusting learning rate of group 0 to 2.4598e-06.
outputs_pos.loss : 1.4616438150405884
outputs_pos.loss : 1.2667804956436157
outputs_pos.loss : 1.050028920173645
outputs_pos.loss : 1.0540415048599243
outputs_pos.loss : 1.0806828737258911
outputs_pos.loss : 1.6721910238265991
outputs_pos.loss : 1.0157042741775513
outputs_pos.loss : 1.8651505708694458
Epoch 00810: adjusting learning rate of group 0 to 2.4597e-06.
outputs_pos.loss : 1.1192233562469482
outputs_pos.loss : 1.5699410438537598
outputs_pos.loss : 1.2798857688903809
outputs_pos.loss : 1.1924952268600464
outputs_pos.loss : 1.0976709127426147
outputs_pos.loss : 1.0036625862121582
outputs_pos.loss : 1.0896717309951782
outputs_pos.loss : 1.094988226890564
Epoch 00811: adjusting learning rate of group 0 to 2.4596e-06.
outputs_pos.loss : 1.3853132724761963
outputs_pos.loss : 1.3128752708435059
outputs_pos.loss : 1.092249870300293
outputs_pos.loss : 1.387046456336975
outputs_pos.loss : 0.9728307723999023
outputs_pos.loss : 0.9996038675308228
outputs_pos.loss : 1.2043033838272095
outputs_pos.loss : 1.164762258529663
Epoch 00812: adjusting learning rate of group 0 to 2.4595e-06.
outputs_pos.loss : 1.1895077228546143
outputs_pos.loss : 1.2962391376495361
outputs_pos.loss : 1.0882086753845215
outputs_pos.loss : 1.6872798204421997
outputs_pos.loss : 1.3006794452667236
outputs_pos.loss : 0.5856356620788574
outputs_pos.loss : 0.9220964312553406
outputs_pos.loss : 1.1924787759780884
Epoch 00813: adjusting learning rate of group 0 to 2.4594e-06.
outputs_pos.loss : 1.2743165493011475
outputs_pos.loss : 1.3430490493774414
outputs_pos.loss : 1.0475863218307495
outputs_pos.loss : 1.3027360439300537
outputs_pos.loss : 1.5229636430740356
outputs_pos.loss : 2.1014113426208496
outputs_pos.loss : 1.1224697828292847
outputs_pos.loss : 1.1782221794128418
Epoch 00814: adjusting learning rate of group 0 to 2.4594e-06.
outputs_pos.loss : 1.2176605463027954
outputs_pos.loss : 1.3081326484680176
outputs_pos.loss : 1.0990849733352661
outputs_pos.loss : 1.2279040813446045
outputs_pos.loss : 2.029799699783325
outputs_pos.loss : 0.8053739666938782
outputs_pos.loss : 1.1090471744537354
outputs_pos.loss : 0.9272944927215576
Epoch 00815: adjusting learning rate of group 0 to 2.4593e-06.
outputs_pos.loss : 1.3441667556762695
outputs_pos.loss : 1.035301923751831
outputs_pos.loss : 0.8262413144111633
outputs_pos.loss : 1.2998656034469604
outputs_pos.loss : 1.4672783613204956
outputs_pos.loss : 1.108648419380188
outputs_pos.loss : 0.9350708723068237
outputs_pos.loss : 0.7006616592407227
Epoch 00816: adjusting learning rate of group 0 to 2.4592e-06.
outputs_pos.loss : 1.0377390384674072
outputs_pos.loss : 1.758416771888733
outputs_pos.loss : 1.2975515127182007
outputs_pos.loss : 1.1886975765228271
outputs_pos.loss : 1.0607515573501587
outputs_pos.loss : 1.7114636898040771
outputs_pos.loss : 1.3561502695083618
outputs_pos.loss : 1.2687594890594482
Epoch 00817: adjusting learning rate of group 0 to 2.4591e-06.
outputs_pos.loss : 1.0839735269546509
outputs_pos.loss : 0.8537176847457886
outputs_pos.loss : 1.6089236736297607
outputs_pos.loss : 1.0086320638656616
outputs_pos.loss : 1.412170648574829
outputs_pos.loss : 0.9384220242500305
outputs_pos.loss : 0.8879326581954956
outputs_pos.loss : 1.366455078125
Epoch 00818: adjusting learning rate of group 0 to 2.4590e-06.
outputs_pos.loss : 0.8897351622581482
outputs_pos.loss : 1.4368791580200195
outputs_pos.loss : 1.3847464323043823
outputs_pos.loss : 1.3616174459457397
outputs_pos.loss : 1.702858805656433
outputs_pos.loss : 1.1405943632125854
outputs_pos.loss : 1.794754981994629
outputs_pos.loss : 1.3865230083465576
Epoch 00819: adjusting learning rate of group 0 to 2.4589e-06.
outputs_pos.loss : 1.4023774862289429
outputs_pos.loss : 1.2651572227478027
outputs_pos.loss : 0.805760383605957
outputs_pos.loss : 1.404517412185669
outputs_pos.loss : 0.993689239025116
outputs_pos.loss : 1.168765664100647
outputs_pos.loss : 2.2274160385131836
outputs_pos.loss : 1.40225088596344
Epoch 00820: adjusting learning rate of group 0 to 2.4588e-06.
outputs_pos.loss : 1.055039882659912
outputs_pos.loss : 1.4492332935333252
outputs_pos.loss : 1.34721040725708
outputs_pos.loss : 1.300497055053711
outputs_pos.loss : 1.361125111579895
outputs_pos.loss : 1.0580922365188599
outputs_pos.loss : 1.2245566844940186
outputs_pos.loss : 1.5568851232528687
Epoch 00821: adjusting learning rate of group 0 to 2.4587e-06.
outputs_pos.loss : 0.9543095827102661
outputs_pos.loss : 1.0283108949661255
outputs_pos.loss : 1.338454246520996
outputs_pos.loss : 1.390088677406311
outputs_pos.loss : 1.0443845987319946
outputs_pos.loss : 0.828723669052124
outputs_pos.loss : 1.4716092348098755
outputs_pos.loss : 0.852914571762085
Epoch 00822: adjusting learning rate of group 0 to 2.4586e-06.
outputs_pos.loss : 1.0742799043655396
outputs_pos.loss : 0.8717473149299622
outputs_pos.loss : 1.284833550453186
outputs_pos.loss : 0.995517373085022
outputs_pos.loss : 1.4002407789230347
outputs_pos.loss : 0.9803228378295898
outputs_pos.loss : 1.234241008758545
outputs_pos.loss : 1.2310054302215576
Epoch 00823: adjusting learning rate of group 0 to 2.4585e-06.
outputs_pos.loss : 0.9628109931945801
outputs_pos.loss : 1.3345789909362793
outputs_pos.loss : 1.5348767042160034
outputs_pos.loss : 1.0516173839569092
outputs_pos.loss : 0.778556227684021
outputs_pos.loss : 1.1003998517990112
outputs_pos.loss : 1.2797034978866577
outputs_pos.loss : 1.0396921634674072
Epoch 00824: adjusting learning rate of group 0 to 2.4584e-06.
outputs_pos.loss : 1.1816327571868896
outputs_pos.loss : 1.022314429283142
outputs_pos.loss : 1.2804412841796875
outputs_pos.loss : 1.0551646947860718
outputs_pos.loss : 1.1298160552978516
outputs_pos.loss : 1.2236605882644653
outputs_pos.loss : 1.1463478803634644
outputs_pos.loss : 1.3242048025131226
Epoch 00825: adjusting learning rate of group 0 to 2.4583e-06.
outputs_pos.loss : 1.2069803476333618
outputs_pos.loss : 1.6463409662246704
outputs_pos.loss : 1.9260947704315186
outputs_pos.loss : 1.1467176675796509
outputs_pos.loss : 1.2043803930282593
outputs_pos.loss : 1.2150765657424927
outputs_pos.loss : 1.8655256032943726
outputs_pos.loss : 0.7543736100196838
Epoch 00826: adjusting learning rate of group 0 to 2.4581e-06.
outputs_pos.loss : 1.2232526540756226
outputs_pos.loss : 0.8797209858894348
outputs_pos.loss : 1.1157513856887817
outputs_pos.loss : 1.3212885856628418
outputs_pos.loss : 1.460587739944458
outputs_pos.loss : 1.7759898900985718
outputs_pos.loss : 1.0776969194412231
outputs_pos.loss : 1.1233036518096924
Epoch 00827: adjusting learning rate of group 0 to 2.4580e-06.
outputs_pos.loss : 0.740066647529602
outputs_pos.loss : 1.4902007579803467
outputs_pos.loss : 1.4267023801803589
outputs_pos.loss : 0.9773775935173035
outputs_pos.loss : 1.4096688032150269
outputs_pos.loss : 1.3905458450317383
outputs_pos.loss : 0.843742847442627
outputs_pos.loss : 1.0224764347076416
Epoch 00828: adjusting learning rate of group 0 to 2.4579e-06.
outputs_pos.loss : 1.1287710666656494
outputs_pos.loss : 1.1030839681625366
outputs_pos.loss : 0.6731109023094177
outputs_pos.loss : 0.9770665168762207
outputs_pos.loss : 1.0812636613845825
outputs_pos.loss : 1.290906548500061
outputs_pos.loss : 0.9042256474494934
outputs_pos.loss : 0.9708161354064941
Epoch 00829: adjusting learning rate of group 0 to 2.4578e-06.
outputs_pos.loss : 1.0744848251342773
outputs_pos.loss : 1.528088927268982
outputs_pos.loss : 1.2806761264801025
outputs_pos.loss : 1.0670273303985596
outputs_pos.loss : 1.1938122510910034
outputs_pos.loss : 1.3012899160385132
outputs_pos.loss : 1.1939787864685059
outputs_pos.loss : 1.4767987728118896
Epoch 00830: adjusting learning rate of group 0 to 2.4577e-06.
outputs_pos.loss : 1.1576220989227295
outputs_pos.loss : 1.2178198099136353
outputs_pos.loss : 1.831685185432434
outputs_pos.loss : 1.2110426425933838
outputs_pos.loss : 1.0957995653152466
outputs_pos.loss : 1.4898642301559448
outputs_pos.loss : 1.0266845226287842
outputs_pos.loss : 0.8441161513328552
Epoch 00831: adjusting learning rate of group 0 to 2.4576e-06.
outputs_pos.loss : 0.8017740249633789
outputs_pos.loss : 0.9985311031341553
outputs_pos.loss : 1.0633958578109741
outputs_pos.loss : 0.9663774371147156
outputs_pos.loss : 1.639191746711731
outputs_pos.loss : 1.1696776151657104
outputs_pos.loss : 1.0349361896514893
outputs_pos.loss : 1.7097506523132324
Epoch 00832: adjusting learning rate of group 0 to 2.4575e-06.
outputs_pos.loss : 1.6608245372772217
outputs_pos.loss : 1.1826146841049194
outputs_pos.loss : 1.6657466888427734
outputs_pos.loss : 0.857607901096344
outputs_pos.loss : 1.5811522006988525
outputs_pos.loss : 1.347699522972107
outputs_pos.loss : 0.5902204513549805
outputs_pos.loss : 1.0452772378921509
Epoch 00833: adjusting learning rate of group 0 to 2.4574e-06.
outputs_pos.loss : 1.0662871599197388
outputs_pos.loss : 1.06480872631073
outputs_pos.loss : 0.9844017624855042
outputs_pos.loss : 1.1974421739578247
outputs_pos.loss : 0.9935857057571411
outputs_pos.loss : 1.043718695640564
outputs_pos.loss : 1.4439619779586792
outputs_pos.loss : 1.16673743724823
Epoch 00834: adjusting learning rate of group 0 to 2.4573e-06.
outputs_pos.loss : 1.0905557870864868
outputs_pos.loss : 1.0810178518295288
outputs_pos.loss : 1.204055905342102
outputs_pos.loss : 1.3998527526855469
outputs_pos.loss : 1.1966278553009033
outputs_pos.loss : 1.3738369941711426
outputs_pos.loss : 1.3769440650939941
outputs_pos.loss : 1.1587896347045898
Epoch 00835: adjusting learning rate of group 0 to 2.4572e-06.
outputs_pos.loss : 1.1683363914489746
outputs_pos.loss : 1.73186457157135
outputs_pos.loss : 0.9868052005767822
outputs_pos.loss : 1.416926383972168
outputs_pos.loss : 1.1772891283035278
outputs_pos.loss : 0.9950727224349976
outputs_pos.loss : 0.9057278037071228
outputs_pos.loss : 1.0479121208190918
Epoch 00836: adjusting learning rate of group 0 to 2.4571e-06.
outputs_pos.loss : 1.6166013479232788
outputs_pos.loss : 0.7784494161605835
outputs_pos.loss : 1.182279348373413
outputs_pos.loss : 1.0122853517532349
outputs_pos.loss : 1.1142483949661255
outputs_pos.loss : 1.1066581010818481
outputs_pos.loss : 1.0186078548431396
outputs_pos.loss : 0.9663053750991821
Epoch 00837: adjusting learning rate of group 0 to 2.4570e-06.
outputs_pos.loss : 1.0404452085494995
outputs_pos.loss : 1.0480965375900269
outputs_pos.loss : 1.0810139179229736
outputs_pos.loss : 2.2727415561676025
outputs_pos.loss : 1.0306122303009033
outputs_pos.loss : 1.4359257221221924
outputs_pos.loss : 0.924360990524292
outputs_pos.loss : 1.580007791519165
Epoch 00838: adjusting learning rate of group 0 to 2.4569e-06.
outputs_pos.loss : 1.270795464515686
outputs_pos.loss : 0.9145887494087219
outputs_pos.loss : 1.2380454540252686
outputs_pos.loss : 1.4860202074050903
outputs_pos.loss : 1.130760908126831
outputs_pos.loss : 1.3033941984176636
outputs_pos.loss : 1.243294358253479
outputs_pos.loss : 0.9535021781921387
Epoch 00839: adjusting learning rate of group 0 to 2.4568e-06.
outputs_pos.loss : 0.9624932408332825
outputs_pos.loss : 1.0914570093154907
outputs_pos.loss : 1.2990970611572266
outputs_pos.loss : 1.1971566677093506
outputs_pos.loss : 1.0235743522644043
outputs_pos.loss : 1.2175307273864746
outputs_pos.loss : 1.1606186628341675
outputs_pos.loss : 1.5629791021347046
Epoch 00840: adjusting learning rate of group 0 to 2.4567e-06.
outputs_pos.loss : 1.0129048824310303
outputs_pos.loss : 1.7126246690750122
outputs_pos.loss : 1.1821625232696533
outputs_pos.loss : 0.9311155080795288
outputs_pos.loss : 1.1964809894561768
outputs_pos.loss : 0.8070764541625977
outputs_pos.loss : 1.5806994438171387
outputs_pos.loss : 0.9370610117912292
Epoch 00841: adjusting learning rate of group 0 to 2.4566e-06.
outputs_pos.loss : 0.8530560731887817
outputs_pos.loss : 1.3208709955215454
outputs_pos.loss : 2.0071301460266113
outputs_pos.loss : 0.982870876789093
outputs_pos.loss : 1.3611681461334229
outputs_pos.loss : 0.9939106106758118
outputs_pos.loss : 1.7658196687698364
outputs_pos.loss : 0.8666359186172485
Epoch 00842: adjusting learning rate of group 0 to 2.4565e-06.
outputs_pos.loss : 1.1361716985702515
outputs_pos.loss : 1.4810543060302734
outputs_pos.loss : 1.2922120094299316
outputs_pos.loss : 1.7510172128677368
outputs_pos.loss : 1.0853317975997925
outputs_pos.loss : 1.0884708166122437
outputs_pos.loss : 1.309228777885437
outputs_pos.loss : 0.8796907067298889
Epoch 00843: adjusting learning rate of group 0 to 2.4564e-06.
outputs_pos.loss : 1.2075504064559937
outputs_pos.loss : 1.3769276142120361
outputs_pos.loss : 1.257452368736267
outputs_pos.loss : 0.9780613780021667
outputs_pos.loss : 1.1853835582733154
outputs_pos.loss : 0.7793251276016235
outputs_pos.loss : 1.144296407699585
outputs_pos.loss : 1.2555803060531616
Epoch 00844: adjusting learning rate of group 0 to 2.4563e-06.
outputs_pos.loss : 1.1094820499420166
outputs_pos.loss : 0.7893197536468506
outputs_pos.loss : 1.1461470127105713
outputs_pos.loss : 0.8144466280937195
outputs_pos.loss : 1.1568580865859985
outputs_pos.loss : 1.9015077352523804
outputs_pos.loss : 1.401282548904419
outputs_pos.loss : 1.0135236978530884
Epoch 00845: adjusting learning rate of group 0 to 2.4562e-06.
outputs_pos.loss : 1.21729576587677
outputs_pos.loss : 1.2946300506591797
outputs_pos.loss : 1.253545880317688
outputs_pos.loss : 1.0349453687667847
outputs_pos.loss : 1.3289108276367188
outputs_pos.loss : 1.2440004348754883
outputs_pos.loss : 1.1625828742980957
outputs_pos.loss : 1.50690495967865
Epoch 00846: adjusting learning rate of group 0 to 2.4561e-06.
outputs_pos.loss : 0.9667905569076538
outputs_pos.loss : 1.1259019374847412
outputs_pos.loss : 0.9478882551193237
outputs_pos.loss : 1.115789771080017
outputs_pos.loss : 1.0092576742172241
outputs_pos.loss : 1.159471869468689
outputs_pos.loss : 1.3905075788497925
outputs_pos.loss : 1.4276102781295776
Epoch 00847: adjusting learning rate of group 0 to 2.4560e-06.
outputs_pos.loss : 1.243852972984314
outputs_pos.loss : 1.0296202898025513
outputs_pos.loss : 0.6253542304039001
outputs_pos.loss : 1.0893712043762207
outputs_pos.loss : 0.8550159931182861
outputs_pos.loss : 0.8773004412651062
outputs_pos.loss : 1.0970118045806885
outputs_pos.loss : 0.9864599704742432
Epoch 00848: adjusting learning rate of group 0 to 2.4559e-06.
outputs_pos.loss : 1.1936604976654053
outputs_pos.loss : 0.934836208820343
outputs_pos.loss : 0.8888486623764038
outputs_pos.loss : 0.9839320182800293
outputs_pos.loss : 1.0375630855560303
outputs_pos.loss : 0.9883987903594971
outputs_pos.loss : 1.1841152906417847
outputs_pos.loss : 1.1800687313079834
Epoch 00849: adjusting learning rate of group 0 to 2.4558e-06.
outputs_pos.loss : 1.1965477466583252
outputs_pos.loss : 0.9150148630142212
outputs_pos.loss : 1.4034839868545532
outputs_pos.loss : 1.188176155090332
outputs_pos.loss : 0.9058892726898193
outputs_pos.loss : 0.5017821788787842
outputs_pos.loss : 0.9813879132270813
outputs_pos.loss : 0.9464932680130005
Epoch 00850: adjusting learning rate of group 0 to 2.4557e-06.
outputs_pos.loss : 1.1323720216751099
outputs_pos.loss : 0.6621068120002747
outputs_pos.loss : 1.011526346206665
outputs_pos.loss : 1.0444700717926025
outputs_pos.loss : 0.7997704744338989
outputs_pos.loss : 0.9602228403091431
outputs_pos.loss : 1.1452752351760864
outputs_pos.loss : 0.916778564453125
Epoch 00851: adjusting learning rate of group 0 to 2.4556e-06.
outputs_pos.loss : 1.4871530532836914
outputs_pos.loss : 1.131862998008728
outputs_pos.loss : 1.3673123121261597
outputs_pos.loss : 1.1636316776275635
outputs_pos.loss : 1.1740809679031372
outputs_pos.loss : 0.9841019511222839
outputs_pos.loss : 1.1255359649658203
outputs_pos.loss : 0.8289198279380798
Epoch 00852: adjusting learning rate of group 0 to 2.4555e-06.
outputs_pos.loss : 1.5139652490615845
outputs_pos.loss : 1.2189788818359375
outputs_pos.loss : 1.122752070426941
outputs_pos.loss : 1.154686450958252
outputs_pos.loss : 1.4994508028030396
outputs_pos.loss : 1.144116997718811
outputs_pos.loss : 0.8634869456291199
outputs_pos.loss : 0.8249478936195374
Epoch 00853: adjusting learning rate of group 0 to 2.4554e-06.
outputs_pos.loss : 1.3929924964904785
outputs_pos.loss : 1.19827139377594
outputs_pos.loss : 0.8890917301177979
outputs_pos.loss : 1.1368329524993896
outputs_pos.loss : 0.8125112652778625
outputs_pos.loss : 1.4170644283294678
outputs_pos.loss : 0.8584020137786865
outputs_pos.loss : 1.176617980003357
Epoch 00854: adjusting learning rate of group 0 to 2.4553e-06.
outputs_pos.loss : 1.9723758697509766
outputs_pos.loss : 0.8628363609313965
outputs_pos.loss : 0.8549613356590271
outputs_pos.loss : 1.0040149688720703
outputs_pos.loss : 0.8155882358551025
outputs_pos.loss : 0.9506193399429321
outputs_pos.loss : 1.2103149890899658
outputs_pos.loss : 1.0612987279891968
Epoch 00855: adjusting learning rate of group 0 to 2.4552e-06.
outputs_pos.loss : 1.3761886358261108
outputs_pos.loss : 0.8950164914131165
outputs_pos.loss : 1.4092793464660645
outputs_pos.loss : 1.3269824981689453
outputs_pos.loss : 1.4687331914901733
outputs_pos.loss : 1.1664948463439941
outputs_pos.loss : 0.9600088000297546
outputs_pos.loss : 1.8548048734664917
Epoch 00856: adjusting learning rate of group 0 to 2.4551e-06.
outputs_pos.loss : 1.2176839113235474
outputs_pos.loss : 1.2190320491790771
outputs_pos.loss : 1.5338008403778076
outputs_pos.loss : 1.047861099243164
outputs_pos.loss : 1.256147027015686
outputs_pos.loss : 1.3588175773620605
outputs_pos.loss : 1.0441497564315796
outputs_pos.loss : 1.2626845836639404
Epoch 00857: adjusting learning rate of group 0 to 2.4550e-06.
outputs_pos.loss : 1.0244688987731934
outputs_pos.loss : 1.1715360879898071
outputs_pos.loss : 1.1570501327514648
outputs_pos.loss : 1.723444938659668
outputs_pos.loss : 1.2602794170379639
outputs_pos.loss : 0.978618323802948
outputs_pos.loss : 1.1909531354904175
outputs_pos.loss : 1.1795331239700317
Epoch 00858: adjusting learning rate of group 0 to 2.4549e-06.
outputs_pos.loss : 0.9900540709495544
outputs_pos.loss : 1.078309416770935
outputs_pos.loss : 0.9259258508682251
outputs_pos.loss : 0.9777297377586365
outputs_pos.loss : 1.1183713674545288
outputs_pos.loss : 0.9292961359024048
outputs_pos.loss : 1.4767045974731445
outputs_pos.loss : 0.859412670135498
Epoch 00859: adjusting learning rate of group 0 to 2.4548e-06.
outputs_pos.loss : 1.281869888305664
outputs_pos.loss : 0.9373561143875122
outputs_pos.loss : 1.1768004894256592
outputs_pos.loss : 1.0773564577102661
outputs_pos.loss : 1.2356183528900146
outputs_pos.loss : 0.9220407009124756
outputs_pos.loss : 0.7868922352790833
outputs_pos.loss : 0.8992276787757874
Epoch 00860: adjusting learning rate of group 0 to 2.4547e-06.
outputs_pos.loss : 0.8754753470420837
outputs_pos.loss : 1.3455654382705688
outputs_pos.loss : 1.0202564001083374
outputs_pos.loss : 1.3369789123535156
outputs_pos.loss : 1.3703893423080444
outputs_pos.loss : 0.9077532291412354
outputs_pos.loss : 1.3011974096298218
outputs_pos.loss : 1.2459036111831665
Epoch 00861: adjusting learning rate of group 0 to 2.4545e-06.
outputs_pos.loss : 0.941312313079834
outputs_pos.loss : 1.2972391843795776
outputs_pos.loss : 1.1335728168487549
outputs_pos.loss : 1.234239935874939
outputs_pos.loss : 1.6315490007400513
outputs_pos.loss : 1.509283185005188
outputs_pos.loss : 1.0033953189849854
outputs_pos.loss : 1.2413079738616943
Epoch 00862: adjusting learning rate of group 0 to 2.4544e-06.
outputs_pos.loss : 1.2233169078826904
outputs_pos.loss : 1.395330786705017
outputs_pos.loss : 1.1954288482666016
outputs_pos.loss : 1.3835850954055786
outputs_pos.loss : 1.5966688394546509
outputs_pos.loss : 1.1794756650924683
outputs_pos.loss : 1.2280240058898926
outputs_pos.loss : 1.6118334531784058
Epoch 00863: adjusting learning rate of group 0 to 2.4543e-06.
outputs_pos.loss : 0.8008321523666382
outputs_pos.loss : 0.9735592007637024
outputs_pos.loss : 1.6472890377044678
outputs_pos.loss : 1.1131457090377808
outputs_pos.loss : 1.5475555658340454
outputs_pos.loss : 1.0395798683166504
outputs_pos.loss : 1.1972782611846924
outputs_pos.loss : 1.3451634645462036
Epoch 00864: adjusting learning rate of group 0 to 2.4542e-06.
outputs_pos.loss : 1.1323192119598389
outputs_pos.loss : 1.031096339225769
outputs_pos.loss : 1.4932364225387573
outputs_pos.loss : 1.4230016469955444
outputs_pos.loss : 1.0119469165802002
outputs_pos.loss : 0.9427404403686523
outputs_pos.loss : 1.4411240816116333
outputs_pos.loss : 1.574051856994629
Epoch 00865: adjusting learning rate of group 0 to 2.4541e-06.
outputs_pos.loss : 1.3432625532150269
outputs_pos.loss : 1.3947789669036865
outputs_pos.loss : 1.419818639755249
outputs_pos.loss : 0.8966991901397705
outputs_pos.loss : 1.067642092704773
outputs_pos.loss : 1.1886645555496216
outputs_pos.loss : 1.9348047971725464
outputs_pos.loss : 1.1605039834976196
Epoch 00866: adjusting learning rate of group 0 to 2.4540e-06.
outputs_pos.loss : 1.411881923675537
outputs_pos.loss : 1.81992769241333
outputs_pos.loss : 1.0358879566192627
outputs_pos.loss : 1.1862576007843018
outputs_pos.loss : 1.3339072465896606
outputs_pos.loss : 1.0946333408355713
outputs_pos.loss : 1.0149757862091064
outputs_pos.loss : 1.5513243675231934
Epoch 00867: adjusting learning rate of group 0 to 2.4539e-06.
outputs_pos.loss : 1.1194281578063965
outputs_pos.loss : 1.3550949096679688
outputs_pos.loss : 1.4140442609786987
outputs_pos.loss : 1.444223403930664
outputs_pos.loss : 1.3805285692214966
outputs_pos.loss : 0.9949325323104858
outputs_pos.loss : 1.4215068817138672
outputs_pos.loss : 1.287543535232544
Epoch 00868: adjusting learning rate of group 0 to 2.4538e-06.
outputs_pos.loss : 1.40278959274292
outputs_pos.loss : 1.1190478801727295
outputs_pos.loss : 1.0538303852081299
outputs_pos.loss : 0.9757331013679504
outputs_pos.loss : 1.1935299634933472
outputs_pos.loss : 1.1974024772644043
outputs_pos.loss : 1.1229734420776367
outputs_pos.loss : 1.4127652645111084
Epoch 00869: adjusting learning rate of group 0 to 2.4537e-06.
outputs_pos.loss : 1.0997148752212524
outputs_pos.loss : 1.1639827489852905
outputs_pos.loss : 0.9137239456176758
outputs_pos.loss : 1.3983372449874878
outputs_pos.loss : 1.2828153371810913
outputs_pos.loss : 2.097651481628418
outputs_pos.loss : 0.8970796465873718
outputs_pos.loss : 1.364241123199463
Epoch 00870: adjusting learning rate of group 0 to 2.4536e-06.
outputs_pos.loss : 1.206700086593628
outputs_pos.loss : 1.2941598892211914
outputs_pos.loss : 1.8120851516723633
outputs_pos.loss : 1.4209529161453247
outputs_pos.loss : 1.0692509412765503
outputs_pos.loss : 0.9709196090698242
outputs_pos.loss : 1.1484966278076172
outputs_pos.loss : 1.4275028705596924
Epoch 00871: adjusting learning rate of group 0 to 2.4535e-06.
outputs_pos.loss : 1.847966194152832
outputs_pos.loss : 1.271679401397705
outputs_pos.loss : 1.275600552558899
outputs_pos.loss : 1.0380374193191528
outputs_pos.loss : 0.987970769405365
outputs_pos.loss : 1.115209698677063
outputs_pos.loss : 1.1206318140029907
outputs_pos.loss : 1.015885353088379
Epoch 00872: adjusting learning rate of group 0 to 2.4534e-06.
outputs_pos.loss : 1.3345471620559692
outputs_pos.loss : 1.2288991212844849
outputs_pos.loss : 1.5085489749908447
outputs_pos.loss : 1.1076228618621826
outputs_pos.loss : 1.5324413776397705
outputs_pos.loss : 1.3738453388214111
outputs_pos.loss : 1.04244065284729
outputs_pos.loss : 0.7279824614524841
Epoch 00873: adjusting learning rate of group 0 to 2.4533e-06.
outputs_pos.loss : 1.202656626701355
outputs_pos.loss : 1.1343759298324585
outputs_pos.loss : 1.1773453950881958
outputs_pos.loss : 1.1549962759017944
outputs_pos.loss : 1.272829294204712
outputs_pos.loss : 1.2007139921188354
outputs_pos.loss : 1.0236103534698486
outputs_pos.loss : 1.0767345428466797
Epoch 00874: adjusting learning rate of group 0 to 2.4532e-06.
outputs_pos.loss : 1.0493327379226685
outputs_pos.loss : 1.382124662399292
outputs_pos.loss : 1.0008901357650757
outputs_pos.loss : 1.0022995471954346
outputs_pos.loss : 0.9600170254707336
outputs_pos.loss : 1.302761435508728
outputs_pos.loss : 0.9777343273162842
outputs_pos.loss : 1.3165812492370605
Epoch 00875: adjusting learning rate of group 0 to 2.4531e-06.
outputs_pos.loss : 0.7427253127098083
outputs_pos.loss : 0.9122418761253357
outputs_pos.loss : 1.2949868440628052
outputs_pos.loss : 0.74922114610672
outputs_pos.loss : 1.388118028640747
outputs_pos.loss : 1.9061336517333984
outputs_pos.loss : 0.9378947019577026
outputs_pos.loss : 1.323927879333496
Epoch 00876: adjusting learning rate of group 0 to 2.4530e-06.
outputs_pos.loss : 0.9723370671272278
outputs_pos.loss : 1.288851022720337
outputs_pos.loss : 1.0299675464630127
outputs_pos.loss : 0.6566980481147766
outputs_pos.loss : 1.3463371992111206
outputs_pos.loss : 1.9865282773971558
outputs_pos.loss : 0.9553242921829224
outputs_pos.loss : 1.2696053981781006
Epoch 00877: adjusting learning rate of group 0 to 2.4529e-06.
outputs_pos.loss : 0.8232934474945068
outputs_pos.loss : 1.2877994775772095
outputs_pos.loss : 1.158742070198059
outputs_pos.loss : 1.2087979316711426
outputs_pos.loss : 0.8966975212097168
outputs_pos.loss : 1.3298742771148682
outputs_pos.loss : 0.9984536170959473
outputs_pos.loss : 1.157257080078125
Epoch 00878: adjusting learning rate of group 0 to 2.4527e-06.
outputs_pos.loss : 1.1532138586044312
outputs_pos.loss : 0.924125075340271
outputs_pos.loss : 0.8609347343444824
outputs_pos.loss : 1.3150978088378906
outputs_pos.loss : 1.0605742931365967
outputs_pos.loss : 1.3406152725219727
outputs_pos.loss : 0.9865766763687134
outputs_pos.loss : 1.0194361209869385
Epoch 00879: adjusting learning rate of group 0 to 2.4526e-06.
outputs_pos.loss : 0.8299915194511414
outputs_pos.loss : 1.4904391765594482
outputs_pos.loss : 0.9641073346138
outputs_pos.loss : 1.1505578756332397
outputs_pos.loss : 1.1656997203826904
outputs_pos.loss : 1.0516732931137085
outputs_pos.loss : 1.1121238470077515
outputs_pos.loss : 1.3852912187576294
Epoch 00880: adjusting learning rate of group 0 to 2.4525e-06.
outputs_pos.loss : 1.126063346862793
outputs_pos.loss : 0.8841487169265747
outputs_pos.loss : 1.0312258005142212
outputs_pos.loss : 0.7207444310188293
outputs_pos.loss : 1.202352523803711
outputs_pos.loss : 1.7898367643356323
outputs_pos.loss : 1.4760226011276245
outputs_pos.loss : 1.011328935623169
Epoch 00881: adjusting learning rate of group 0 to 2.4524e-06.
outputs_pos.loss : 1.009845495223999
outputs_pos.loss : 1.1914432048797607
outputs_pos.loss : 0.7867268323898315
outputs_pos.loss : 1.0053468942642212
outputs_pos.loss : 0.886089563369751
outputs_pos.loss : 1.2290456295013428
outputs_pos.loss : 1.5808554887771606
outputs_pos.loss : 1.6038209199905396
Epoch 00882: adjusting learning rate of group 0 to 2.4523e-06.
outputs_pos.loss : 0.9328762292861938
outputs_pos.loss : 1.1566240787506104
outputs_pos.loss : 0.9116217494010925
outputs_pos.loss : 0.8917446136474609
outputs_pos.loss : 1.2945526838302612
outputs_pos.loss : 1.1729222536087036
outputs_pos.loss : 1.3615005016326904
outputs_pos.loss : 1.7363865375518799
Epoch 00883: adjusting learning rate of group 0 to 2.4522e-06.
outputs_pos.loss : 0.672758162021637
outputs_pos.loss : 1.1320624351501465
outputs_pos.loss : 1.7736871242523193
outputs_pos.loss : 1.029652714729309
outputs_pos.loss : 1.1309514045715332
outputs_pos.loss : 0.9401407837867737
outputs_pos.loss : 1.449783205986023
outputs_pos.loss : 0.6894519925117493
Epoch 00884: adjusting learning rate of group 0 to 2.4521e-06.
outputs_pos.loss : 1.2759712934494019
outputs_pos.loss : 1.5496670007705688
outputs_pos.loss : 1.0750006437301636
outputs_pos.loss : 0.8315979838371277
outputs_pos.loss : 1.1127123832702637
outputs_pos.loss : 0.9613508582115173
outputs_pos.loss : 1.2031210660934448
outputs_pos.loss : 1.3092821836471558
Epoch 00885: adjusting learning rate of group 0 to 2.4520e-06.
outputs_pos.loss : 1.8375697135925293
outputs_pos.loss : 1.0164157152175903
outputs_pos.loss : 1.0925663709640503
outputs_pos.loss : 1.4342467784881592
outputs_pos.loss : 1.082206130027771
outputs_pos.loss : 1.1847186088562012
outputs_pos.loss : 0.7714601755142212
outputs_pos.loss : 1.3850467205047607
Epoch 00886: adjusting learning rate of group 0 to 2.4519e-06.
outputs_pos.loss : 0.9779098629951477
outputs_pos.loss : 1.2558218240737915
outputs_pos.loss : 1.0311367511749268
outputs_pos.loss : 1.3317391872406006
outputs_pos.loss : 1.2062822580337524
outputs_pos.loss : 0.6130083203315735
outputs_pos.loss : 0.8912005424499512
outputs_pos.loss : 0.8565434813499451
Epoch 00887: adjusting learning rate of group 0 to 2.4518e-06.
outputs_pos.loss : 0.9440919160842896
outputs_pos.loss : 1.2786591053009033
outputs_pos.loss : 1.3336089849472046
outputs_pos.loss : 0.969795823097229
outputs_pos.loss : 0.8170920610427856
outputs_pos.loss : 1.5800981521606445
outputs_pos.loss : 0.9942407608032227
outputs_pos.loss : 0.7649186849594116
Epoch 00888: adjusting learning rate of group 0 to 2.4517e-06.
outputs_pos.loss : 0.9350851774215698
outputs_pos.loss : 1.4609363079071045
outputs_pos.loss : 1.038802981376648
outputs_pos.loss : 0.5417976975440979
outputs_pos.loss : 0.8129904866218567
outputs_pos.loss : 0.984775185585022
outputs_pos.loss : 0.9981608986854553
outputs_pos.loss : 0.9195797443389893
Epoch 00889: adjusting learning rate of group 0 to 2.4516e-06.
outputs_pos.loss : 1.0302870273590088
outputs_pos.loss : 1.1274133920669556
outputs_pos.loss : 1.3750884532928467
outputs_pos.loss : 1.2411655187606812
outputs_pos.loss : 1.0186043977737427
outputs_pos.loss : 1.2165379524230957
outputs_pos.loss : 1.0260447263717651
outputs_pos.loss : 1.0478131771087646
Epoch 00890: adjusting learning rate of group 0 to 2.4515e-06.
outputs_pos.loss : 0.7249504923820496
outputs_pos.loss : 2.2412781715393066
outputs_pos.loss : 1.079887866973877
outputs_pos.loss : 0.5719450116157532
outputs_pos.loss : 0.8959046602249146
outputs_pos.loss : 1.161462664604187
outputs_pos.loss : 1.1819132566452026
outputs_pos.loss : 1.4868056774139404
Epoch 00891: adjusting learning rate of group 0 to 2.4513e-06.
outputs_pos.loss : 0.8450952172279358
outputs_pos.loss : 1.1415992975234985
outputs_pos.loss : 1.2594242095947266
outputs_pos.loss : 1.1024196147918701
outputs_pos.loss : 1.3139108419418335
outputs_pos.loss : 0.8805769085884094
outputs_pos.loss : 0.9887359738349915
outputs_pos.loss : 1.4453130960464478
Epoch 00892: adjusting learning rate of group 0 to 2.4512e-06.
outputs_pos.loss : 0.9176938533782959
outputs_pos.loss : 0.8947510719299316
outputs_pos.loss : 1.1373882293701172
outputs_pos.loss : 1.157568097114563
outputs_pos.loss : 1.4871877431869507
outputs_pos.loss : 1.1969399452209473
outputs_pos.loss : 0.9489014148712158
outputs_pos.loss : 1.4761325120925903
Epoch 00893: adjusting learning rate of group 0 to 2.4511e-06.
outputs_pos.loss : 1.145153284072876
outputs_pos.loss : 1.125291347503662
outputs_pos.loss : 1.0701004266738892
outputs_pos.loss : 1.0340983867645264
outputs_pos.loss : 0.8530682325363159
outputs_pos.loss : 1.2917741537094116
outputs_pos.loss : 0.829389214515686
outputs_pos.loss : 1.2328745126724243
Epoch 00894: adjusting learning rate of group 0 to 2.4510e-06.
outputs_pos.loss : 0.90630042552948
outputs_pos.loss : 1.198266625404358
outputs_pos.loss : 1.09830641746521
outputs_pos.loss : 1.0195856094360352
outputs_pos.loss : 1.1342575550079346
outputs_pos.loss : 1.151432991027832
outputs_pos.loss : 1.536148190498352
outputs_pos.loss : 0.859900951385498
Epoch 00895: adjusting learning rate of group 0 to 2.4509e-06.
outputs_pos.loss : 1.143682599067688
outputs_pos.loss : 0.9086577892303467
outputs_pos.loss : 1.2216496467590332
outputs_pos.loss : 1.011949062347412
outputs_pos.loss : 0.983566403388977
outputs_pos.loss : 1.4348329305648804
outputs_pos.loss : 1.0328818559646606
outputs_pos.loss : 1.0087053775787354
Epoch 00896: adjusting learning rate of group 0 to 2.4508e-06.
outputs_pos.loss : 1.0323628187179565
outputs_pos.loss : 0.902264416217804
outputs_pos.loss : 1.0472147464752197
outputs_pos.loss : 1.1885143518447876
outputs_pos.loss : 1.1022249460220337
outputs_pos.loss : 1.0146344900131226
outputs_pos.loss : 1.2047992944717407
outputs_pos.loss : 1.165822148323059
Epoch 00897: adjusting learning rate of group 0 to 2.4507e-06.
outputs_pos.loss : 1.4990237951278687
outputs_pos.loss : 1.142822504043579
outputs_pos.loss : 1.0929855108261108
outputs_pos.loss : 1.1945010423660278
outputs_pos.loss : 0.6538872122764587
outputs_pos.loss : 0.7148383855819702
outputs_pos.loss : 0.9729861617088318
outputs_pos.loss : 1.4856935739517212
Epoch 00898: adjusting learning rate of group 0 to 2.4506e-06.
outputs_pos.loss : 0.8187156915664673
outputs_pos.loss : 1.2508265972137451
outputs_pos.loss : 1.2697633504867554
outputs_pos.loss : 0.9441595077514648
outputs_pos.loss : 0.8864685297012329
outputs_pos.loss : 2.1177306175231934
outputs_pos.loss : 1.013264536857605
outputs_pos.loss : 0.8722286820411682
Epoch 00899: adjusting learning rate of group 0 to 2.4505e-06.
outputs_pos.loss : 1.0513124465942383
outputs_pos.loss : 0.91725754737854
outputs_pos.loss : 1.094319224357605
outputs_pos.loss : 1.550154685974121
outputs_pos.loss : 1.081418514251709
outputs_pos.loss : 0.9099004864692688
outputs_pos.loss : 0.7182604074478149
outputs_pos.loss : 1.1160777807235718
Epoch 00900: adjusting learning rate of group 0 to 2.4504e-06.
outputs_pos.loss : 0.7362177968025208
outputs_pos.loss : 0.9964332580566406
outputs_pos.loss : 0.9827848076820374
outputs_pos.loss : 1.4309061765670776
outputs_pos.loss : 0.9577102661132812
outputs_pos.loss : 0.6481047868728638
outputs_pos.loss : 1.447597861289978
outputs_pos.loss : 1.0697499513626099
Epoch 00901: adjusting learning rate of group 0 to 2.4503e-06.
outputs_pos.loss : 1.1154770851135254
outputs_pos.loss : 0.951953649520874
outputs_pos.loss : 1.010980486869812
outputs_pos.loss : 1.781407356262207
outputs_pos.loss : 1.111269235610962
outputs_pos.loss : 1.0838288068771362
outputs_pos.loss : 1.4769856929779053
outputs_pos.loss : 1.407230019569397
Epoch 00902: adjusting learning rate of group 0 to 2.4501e-06.
outputs_pos.loss : 0.8558761477470398
outputs_pos.loss : 0.9603016972541809
outputs_pos.loss : 1.0802147388458252
outputs_pos.loss : 1.1721938848495483
outputs_pos.loss : 1.4373775720596313
outputs_pos.loss : 1.5263690948486328
outputs_pos.loss : 1.4040197134017944
outputs_pos.loss : 1.152917742729187
Epoch 00903: adjusting learning rate of group 0 to 2.4500e-06.
outputs_pos.loss : 0.9341598153114319
outputs_pos.loss : 0.931016743183136
outputs_pos.loss : 0.624335765838623
outputs_pos.loss : 1.0852423906326294
outputs_pos.loss : 1.600853443145752
outputs_pos.loss : 1.5507107973098755
outputs_pos.loss : 1.1639149188995361
outputs_pos.loss : 0.7629402875900269
Epoch 00904: adjusting learning rate of group 0 to 2.4499e-06.
outputs_pos.loss : 1.1133968830108643
outputs_pos.loss : 0.8983173966407776
outputs_pos.loss : 1.2945513725280762
outputs_pos.loss : 1.3463138341903687
outputs_pos.loss : 1.3710033893585205
outputs_pos.loss : 1.2245547771453857
outputs_pos.loss : 0.878527045249939
outputs_pos.loss : 0.8343115448951721
Epoch 00905: adjusting learning rate of group 0 to 2.4498e-06.
outputs_pos.loss : 1.3847581148147583
outputs_pos.loss : 1.0824809074401855
outputs_pos.loss : 1.2621312141418457
outputs_pos.loss : 1.166191577911377
outputs_pos.loss : 1.008470058441162
outputs_pos.loss : 1.5495383739471436
outputs_pos.loss : 1.2257720232009888
outputs_pos.loss : 1.3996914625167847
Epoch 00906: adjusting learning rate of group 0 to 2.4497e-06.
outputs_pos.loss : 0.8237249851226807
outputs_pos.loss : 0.7594306468963623
outputs_pos.loss : 1.0479000806808472
outputs_pos.loss : 0.9913550019264221
outputs_pos.loss : 1.4551794528961182
outputs_pos.loss : 1.2024211883544922
outputs_pos.loss : 0.9386194944381714
outputs_pos.loss : 0.9042904376983643
Epoch 00907: adjusting learning rate of group 0 to 2.4496e-06.
outputs_pos.loss : 0.957258939743042
outputs_pos.loss : 0.9675264358520508
outputs_pos.loss : 0.8783210515975952
outputs_pos.loss : 0.9915124177932739
outputs_pos.loss : 1.1435275077819824
outputs_pos.loss : 1.0432698726654053
outputs_pos.loss : 1.309535264968872
outputs_pos.loss : 0.7219542860984802
Epoch 00908: adjusting learning rate of group 0 to 2.4495e-06.
outputs_pos.loss : 1.0736422538757324
outputs_pos.loss : 2.091675281524658
outputs_pos.loss : 1.214103102684021
outputs_pos.loss : 1.372829556465149
outputs_pos.loss : 1.0327281951904297
outputs_pos.loss : 0.9780855774879456
outputs_pos.loss : 1.0432019233703613
outputs_pos.loss : 0.8164405226707458
Epoch 00909: adjusting learning rate of group 0 to 2.4494e-06.
outputs_pos.loss : 1.1556293964385986
outputs_pos.loss : 1.0988110303878784
outputs_pos.loss : 0.7573210000991821
outputs_pos.loss : 1.056261420249939
outputs_pos.loss : 1.3957992792129517
outputs_pos.loss : 1.0028676986694336
outputs_pos.loss : 1.3019821643829346
outputs_pos.loss : 1.7727288007736206
Epoch 00910: adjusting learning rate of group 0 to 2.4493e-06.
outputs_pos.loss : 1.3031961917877197
outputs_pos.loss : 1.3191711902618408
outputs_pos.loss : 1.7278728485107422
outputs_pos.loss : 1.1306345462799072
outputs_pos.loss : 0.9913524985313416
outputs_pos.loss : 1.124150037765503
outputs_pos.loss : 1.237341046333313
outputs_pos.loss : 1.1858271360397339
Epoch 00911: adjusting learning rate of group 0 to 2.4492e-06.
outputs_pos.loss : 1.3394707441329956
outputs_pos.loss : 1.0101031064987183
outputs_pos.loss : 1.0742096900939941
outputs_pos.loss : 0.8278869986534119
outputs_pos.loss : 0.9722550511360168
outputs_pos.loss : 1.0036464929580688
outputs_pos.loss : 1.189880609512329
outputs_pos.loss : 0.9825729727745056
Epoch 00912: adjusting learning rate of group 0 to 2.4490e-06.
outputs_pos.loss : 0.8978525996208191
outputs_pos.loss : 1.0690823793411255
outputs_pos.loss : 0.7933366298675537
outputs_pos.loss : 1.0880900621414185
outputs_pos.loss : 1.4746286869049072
outputs_pos.loss : 1.4812119007110596
outputs_pos.loss : 1.7527642250061035
outputs_pos.loss : 1.236379861831665
Epoch 00913: adjusting learning rate of group 0 to 2.4489e-06.
outputs_pos.loss : 1.4363993406295776
outputs_pos.loss : 1.2273139953613281
outputs_pos.loss : 0.9423614740371704
outputs_pos.loss : 1.321284294128418
outputs_pos.loss : 1.4778950214385986
outputs_pos.loss : 1.2433207035064697
outputs_pos.loss : 0.9953941702842712
outputs_pos.loss : 1.6973931789398193
Epoch 00914: adjusting learning rate of group 0 to 2.4488e-06.
outputs_pos.loss : 1.5597429275512695
outputs_pos.loss : 0.7965202331542969
outputs_pos.loss : 1.5682215690612793
outputs_pos.loss : 1.0706605911254883
outputs_pos.loss : 0.7863382697105408
outputs_pos.loss : 0.8036718964576721
outputs_pos.loss : 1.11422598361969
outputs_pos.loss : 1.310954213142395
Epoch 00915: adjusting learning rate of group 0 to 2.4487e-06.
outputs_pos.loss : 1.2464756965637207
outputs_pos.loss : 1.2638230323791504
outputs_pos.loss : 1.00516939163208
outputs_pos.loss : 1.2799623012542725
outputs_pos.loss : 1.0031334161758423
outputs_pos.loss : 1.1085869073867798
outputs_pos.loss : 1.060530662536621
outputs_pos.loss : 1.4670952558517456
Epoch 00916: adjusting learning rate of group 0 to 2.4486e-06.
outputs_pos.loss : 0.827545702457428
outputs_pos.loss : 1.1945725679397583
outputs_pos.loss : 1.0832953453063965
outputs_pos.loss : 1.6457765102386475
outputs_pos.loss : 1.6092519760131836
outputs_pos.loss : 1.5560553073883057
outputs_pos.loss : 1.0772510766983032
outputs_pos.loss : 1.2904211282730103
Epoch 00917: adjusting learning rate of group 0 to 2.4485e-06.
outputs_pos.loss : 1.0444434881210327
outputs_pos.loss : 1.3047131299972534
outputs_pos.loss : 0.9441591501235962
outputs_pos.loss : 1.2736132144927979
outputs_pos.loss : 1.2184470891952515
outputs_pos.loss : 0.9931977987289429
outputs_pos.loss : 1.3928325176239014
outputs_pos.loss : 1.5945037603378296
Epoch 00918: adjusting learning rate of group 0 to 2.4484e-06.
outputs_pos.loss : 1.1781898736953735
outputs_pos.loss : 0.9581099152565002
outputs_pos.loss : 1.4569002389907837
outputs_pos.loss : 1.7640832662582397
outputs_pos.loss : 1.1865637302398682
outputs_pos.loss : 1.511814832687378
outputs_pos.loss : 0.9017389416694641
outputs_pos.loss : 1.3350205421447754
Epoch 00919: adjusting learning rate of group 0 to 2.4483e-06.
outputs_pos.loss : 1.0015987157821655
outputs_pos.loss : 1.1563326120376587
outputs_pos.loss : 1.5126402378082275
outputs_pos.loss : 0.852855920791626
outputs_pos.loss : 1.2768404483795166
outputs_pos.loss : 0.7594913840293884
outputs_pos.loss : 1.161871075630188
outputs_pos.loss : 1.4588946104049683
Epoch 00920: adjusting learning rate of group 0 to 2.4482e-06.
outputs_pos.loss : 1.0653414726257324
outputs_pos.loss : 1.1295757293701172
outputs_pos.loss : 0.9932875037193298
outputs_pos.loss : 1.1908105611801147
outputs_pos.loss : 0.8970707058906555
outputs_pos.loss : 1.2259862422943115
outputs_pos.loss : 0.740631103515625
outputs_pos.loss : 1.0086067914962769
Epoch 00921: adjusting learning rate of group 0 to 2.4480e-06.
outputs_pos.loss : 0.9719717502593994
outputs_pos.loss : 1.0165364742279053
outputs_pos.loss : 0.8168227672576904
outputs_pos.loss : 0.6739139556884766
outputs_pos.loss : 2.3490443229675293
outputs_pos.loss : 1.149986982345581
outputs_pos.loss : 1.1415828466415405
outputs_pos.loss : 1.0283408164978027
Epoch 00922: adjusting learning rate of group 0 to 2.4479e-06.
outputs_pos.loss : 0.9977517127990723
outputs_pos.loss : 0.9009873270988464
outputs_pos.loss : 1.2679716348648071
outputs_pos.loss : 1.0737533569335938
outputs_pos.loss : 1.2753698825836182
outputs_pos.loss : 1.1021196842193604
outputs_pos.loss : 1.0796043872833252
outputs_pos.loss : 1.324168086051941
Epoch 00923: adjusting learning rate of group 0 to 2.4478e-06.
outputs_pos.loss : 1.3580957651138306
outputs_pos.loss : 1.196427822113037
outputs_pos.loss : 0.8121302723884583
outputs_pos.loss : 1.258617877960205
outputs_pos.loss : 0.935366153717041
outputs_pos.loss : 1.240116000175476
outputs_pos.loss : 1.189517617225647
outputs_pos.loss : 1.0863418579101562
Epoch 00924: adjusting learning rate of group 0 to 2.4477e-06.
outputs_pos.loss : 1.7311040163040161
outputs_pos.loss : 1.3046337366104126
outputs_pos.loss : 1.0712251663208008
outputs_pos.loss : 1.1047800779342651
outputs_pos.loss : 1.0786885023117065
outputs_pos.loss : 1.003753423690796
outputs_pos.loss : 1.0388983488082886
outputs_pos.loss : 0.9465017318725586
Epoch 00925: adjusting learning rate of group 0 to 2.4476e-06.
outputs_pos.loss : 1.40948486328125
outputs_pos.loss : 1.2837185859680176
outputs_pos.loss : 1.6521425247192383
outputs_pos.loss : 1.1622658967971802
outputs_pos.loss : 0.7080566883087158
outputs_pos.loss : 1.1909205913543701
outputs_pos.loss : 0.7794445753097534
outputs_pos.loss : 0.9091976881027222
Epoch 00926: adjusting learning rate of group 0 to 2.4475e-06.
outputs_pos.loss : 1.2619866132736206
outputs_pos.loss : 1.0479694604873657
outputs_pos.loss : 1.1084648370742798
outputs_pos.loss : 1.3154094219207764
outputs_pos.loss : 1.7009507417678833
outputs_pos.loss : 1.0502296686172485
outputs_pos.loss : 1.2389352321624756
outputs_pos.loss : 1.2466976642608643
Epoch 00927: adjusting learning rate of group 0 to 2.4474e-06.
outputs_pos.loss : 1.4198135137557983
outputs_pos.loss : 1.3187412023544312
outputs_pos.loss : 1.218493103981018
outputs_pos.loss : 1.2757940292358398
outputs_pos.loss : 1.030131220817566
outputs_pos.loss : 0.9722439646720886
outputs_pos.loss : 0.9854412078857422
outputs_pos.loss : 1.119934320449829
Epoch 00928: adjusting learning rate of group 0 to 2.4473e-06.
outputs_pos.loss : 1.1125658750534058
outputs_pos.loss : 1.50131356716156
outputs_pos.loss : 1.2859010696411133
outputs_pos.loss : 1.1157863140106201
outputs_pos.loss : 1.5115442276000977
outputs_pos.loss : 0.7509835958480835
outputs_pos.loss : 0.9491083025932312
outputs_pos.loss : 1.5223129987716675
Epoch 00929: adjusting learning rate of group 0 to 2.4471e-06.
outputs_pos.loss : 1.453515648841858
outputs_pos.loss : 1.8217942714691162
outputs_pos.loss : 1.0803420543670654
outputs_pos.loss : 0.9617050290107727
outputs_pos.loss : 1.4090245962142944
outputs_pos.loss : 1.3542780876159668
outputs_pos.loss : 2.411771059036255
outputs_pos.loss : 1.2081161737442017
Epoch 00930: adjusting learning rate of group 0 to 2.4470e-06.
outputs_pos.loss : 1.438843011856079
outputs_pos.loss : 0.6431811451911926
outputs_pos.loss : 1.536847472190857
outputs_pos.loss : 1.6175485849380493
outputs_pos.loss : 1.0953906774520874
outputs_pos.loss : 0.9891121983528137
outputs_pos.loss : 0.9462610483169556
outputs_pos.loss : 1.2266361713409424
Epoch 00931: adjusting learning rate of group 0 to 2.4469e-06.
outputs_pos.loss : 1.2795419692993164
outputs_pos.loss : 1.186767816543579
outputs_pos.loss : 0.9283600449562073
outputs_pos.loss : 1.847804307937622
outputs_pos.loss : 1.4384703636169434
outputs_pos.loss : 1.4516229629516602
outputs_pos.loss : 1.3453086614608765
outputs_pos.loss : 0.7150262594223022
Epoch 00932: adjusting learning rate of group 0 to 2.4468e-06.
outputs_pos.loss : 0.9889411330223083
outputs_pos.loss : 1.2770123481750488
outputs_pos.loss : 0.8658533692359924
outputs_pos.loss : 0.9253670573234558
outputs_pos.loss : 1.2464948892593384
outputs_pos.loss : 1.5660144090652466
outputs_pos.loss : 0.8468119502067566
outputs_pos.loss : 1.1009396314620972
Epoch 00933: adjusting learning rate of group 0 to 2.4467e-06.
outputs_pos.loss : 1.381323218345642
outputs_pos.loss : 1.453442931175232
outputs_pos.loss : 1.3656823635101318
outputs_pos.loss : 1.1490129232406616
outputs_pos.loss : 1.2080941200256348
outputs_pos.loss : 0.9533087611198425
outputs_pos.loss : 1.178025484085083
outputs_pos.loss : 1.1382555961608887
Epoch 00934: adjusting learning rate of group 0 to 2.4466e-06.
outputs_pos.loss : 1.2837231159210205
outputs_pos.loss : 0.9798462986946106
outputs_pos.loss : 1.0712534189224243
outputs_pos.loss : 1.1705073118209839
outputs_pos.loss : 1.1112817525863647
outputs_pos.loss : 0.6914269924163818
outputs_pos.loss : 1.2694553136825562
outputs_pos.loss : 1.2923026084899902
Epoch 00935: adjusting learning rate of group 0 to 2.4465e-06.
outputs_pos.loss : 1.0893290042877197
outputs_pos.loss : 1.8335598707199097
outputs_pos.loss : 1.0629937648773193
outputs_pos.loss : 1.3613206148147583
outputs_pos.loss : 0.8561791181564331
outputs_pos.loss : 1.1688131093978882
outputs_pos.loss : 1.6634178161621094
outputs_pos.loss : 0.9650139808654785
Epoch 00936: adjusting learning rate of group 0 to 2.4463e-06.
outputs_pos.loss : 1.2918965816497803
outputs_pos.loss : 1.0705320835113525
outputs_pos.loss : 1.2488317489624023
outputs_pos.loss : 1.4417625665664673
outputs_pos.loss : 1.6158901453018188
outputs_pos.loss : 1.2729216814041138
outputs_pos.loss : 1.1727060079574585
outputs_pos.loss : 1.07675039768219
Epoch 00937: adjusting learning rate of group 0 to 2.4462e-06.
outputs_pos.loss : 0.8696181178092957
outputs_pos.loss : 1.845780611038208
outputs_pos.loss : 1.095436930656433
outputs_pos.loss : 1.4413330554962158
outputs_pos.loss : 1.169074535369873
outputs_pos.loss : 0.9662300944328308
outputs_pos.loss : 1.2188557386398315
outputs_pos.loss : 1.1265966892242432
Epoch 00938: adjusting learning rate of group 0 to 2.4461e-06.
outputs_pos.loss : 1.138943076133728
outputs_pos.loss : 0.9610447883605957
outputs_pos.loss : 0.8954619765281677
outputs_pos.loss : 1.0157192945480347
outputs_pos.loss : 1.0182017087936401
outputs_pos.loss : 1.0015790462493896
outputs_pos.loss : 0.9761723279953003
outputs_pos.loss : 1.5922763347625732
Epoch 00939: adjusting learning rate of group 0 to 2.4460e-06.
outputs_pos.loss : 1.0258142948150635
outputs_pos.loss : 0.997517466545105
outputs_pos.loss : 1.2302433252334595
outputs_pos.loss : 0.9586120247840881
outputs_pos.loss : 1.1547387838363647
outputs_pos.loss : 1.177425742149353
outputs_pos.loss : 1.3459596633911133
outputs_pos.loss : 1.306428074836731
Epoch 00940: adjusting learning rate of group 0 to 2.4459e-06.
outputs_pos.loss : 1.4488012790679932
outputs_pos.loss : 1.270500659942627
outputs_pos.loss : 1.4419331550598145
outputs_pos.loss : 1.2232226133346558
outputs_pos.loss : 1.0966005325317383
outputs_pos.loss : 1.0887869596481323
outputs_pos.loss : 1.2239261865615845
outputs_pos.loss : 0.9245796799659729
Epoch 00941: adjusting learning rate of group 0 to 2.4458e-06.
outputs_pos.loss : 1.039021611213684
outputs_pos.loss : 1.7275587320327759
outputs_pos.loss : 0.969045102596283
outputs_pos.loss : 1.0312702655792236
outputs_pos.loss : 1.4259427785873413
outputs_pos.loss : 1.644777774810791
outputs_pos.loss : 1.1900503635406494
outputs_pos.loss : 1.0830087661743164
Epoch 00942: adjusting learning rate of group 0 to 2.4457e-06.
outputs_pos.loss : 1.8525443077087402
outputs_pos.loss : 1.0481630563735962
outputs_pos.loss : 1.384394645690918
outputs_pos.loss : 1.0910077095031738
outputs_pos.loss : 0.9804352521896362
outputs_pos.loss : 1.3142247200012207
outputs_pos.loss : 1.1327495574951172
outputs_pos.loss : 0.8087520599365234
Epoch 00943: adjusting learning rate of group 0 to 2.4455e-06.
outputs_pos.loss : 1.125603437423706
outputs_pos.loss : 0.8847185373306274
outputs_pos.loss : 1.6830849647521973
outputs_pos.loss : 1.0376613140106201
outputs_pos.loss : 1.7320387363433838
outputs_pos.loss : 1.271026849746704
outputs_pos.loss : 1.1066628694534302
outputs_pos.loss : 1.1297292709350586
Epoch 00944: adjusting learning rate of group 0 to 2.4454e-06.
outputs_pos.loss : 1.2515385150909424
outputs_pos.loss : 0.9460371136665344
outputs_pos.loss : 1.3043923377990723
outputs_pos.loss : 1.3369944095611572
outputs_pos.loss : 1.2446473836898804
outputs_pos.loss : 1.076823115348816
outputs_pos.loss : 1.0482453107833862
outputs_pos.loss : 1.4773136377334595
Epoch 00945: adjusting learning rate of group 0 to 2.4453e-06.
outputs_pos.loss : 1.2073739767074585
outputs_pos.loss : 0.7548568844795227
outputs_pos.loss : 1.3687849044799805
outputs_pos.loss : 1.0858230590820312
outputs_pos.loss : 1.2725021839141846
outputs_pos.loss : 0.7214257121086121
outputs_pos.loss : 1.0497323274612427
outputs_pos.loss : 1.6969220638275146
Epoch 00946: adjusting learning rate of group 0 to 2.4452e-06.
outputs_pos.loss : 1.2968477010726929
outputs_pos.loss : 1.2235865592956543
outputs_pos.loss : 1.046158790588379
outputs_pos.loss : 1.995999813079834
outputs_pos.loss : 1.232216715812683
outputs_pos.loss : 1.2930477857589722
outputs_pos.loss : 1.2665088176727295
outputs_pos.loss : 0.9948861598968506
Epoch 00947: adjusting learning rate of group 0 to 2.4451e-06.
outputs_pos.loss : 0.8979772329330444
outputs_pos.loss : 0.7544599771499634
outputs_pos.loss : 0.9760754108428955
outputs_pos.loss : 1.3678151369094849
outputs_pos.loss : 1.1367861032485962
outputs_pos.loss : 1.918028712272644
outputs_pos.loss : 1.1043429374694824
outputs_pos.loss : 1.0155097246170044
Epoch 00948: adjusting learning rate of group 0 to 2.4450e-06.
outputs_pos.loss : 1.3103026151657104
outputs_pos.loss : 0.9805463552474976
outputs_pos.loss : 1.2016905546188354
outputs_pos.loss : 1.0889008045196533
outputs_pos.loss : 1.7450766563415527
outputs_pos.loss : 1.1803197860717773
outputs_pos.loss : 1.1974631547927856
outputs_pos.loss : 1.3793474435806274
Epoch 00949: adjusting learning rate of group 0 to 2.4449e-06.
outputs_pos.loss : 0.9799991846084595
outputs_pos.loss : 0.9534053206443787
outputs_pos.loss : 0.9306337237358093
outputs_pos.loss : 1.0280935764312744
outputs_pos.loss : 1.392132043838501
outputs_pos.loss : 1.1000776290893555
outputs_pos.loss : 1.3683075904846191
outputs_pos.loss : 1.2718394994735718
Epoch 00950: adjusting learning rate of group 0 to 2.4447e-06.
outputs_pos.loss : 1.6258834600448608
outputs_pos.loss : 1.027653455734253
outputs_pos.loss : 1.1079543828964233
outputs_pos.loss : 0.8752399682998657
outputs_pos.loss : 1.5089665651321411
outputs_pos.loss : 1.713432788848877
outputs_pos.loss : 1.175373911857605
outputs_pos.loss : 1.1403255462646484
Epoch 00951: adjusting learning rate of group 0 to 2.4446e-06.
outputs_pos.loss : 1.2011852264404297
outputs_pos.loss : 1.2583369016647339
outputs_pos.loss : 1.0693292617797852
outputs_pos.loss : 1.1566261053085327
outputs_pos.loss : 1.7023016214370728
outputs_pos.loss : 1.5551011562347412
outputs_pos.loss : 0.9645748138427734
outputs_pos.loss : 0.8858136534690857
Epoch 00952: adjusting learning rate of group 0 to 2.4445e-06.
outputs_pos.loss : 1.456792950630188
outputs_pos.loss : 1.9920361042022705
outputs_pos.loss : 0.8699069023132324
outputs_pos.loss : 1.019575834274292
outputs_pos.loss : 0.8615784645080566
outputs_pos.loss : 0.7807743549346924
outputs_pos.loss : 1.1370569467544556
outputs_pos.loss : 1.3332382440567017
Epoch 00953: adjusting learning rate of group 0 to 2.4444e-06.
outputs_pos.loss : 1.1831061840057373
outputs_pos.loss : 1.8598878383636475
outputs_pos.loss : 1.313780426979065
outputs_pos.loss : 0.7401078939437866
outputs_pos.loss : 1.2461912631988525
outputs_pos.loss : 1.2263692617416382
outputs_pos.loss : 0.9548375606536865
outputs_pos.loss : 0.9672924280166626
Epoch 00954: adjusting learning rate of group 0 to 2.4443e-06.
outputs_pos.loss : 1.003435730934143
outputs_pos.loss : 1.439727783203125
outputs_pos.loss : 1.3344846963882446
outputs_pos.loss : 0.9981641173362732
outputs_pos.loss : 1.2969244718551636
outputs_pos.loss : 1.0508198738098145
outputs_pos.loss : 0.9264410734176636
outputs_pos.loss : 0.8268401622772217
Epoch 00955: adjusting learning rate of group 0 to 2.4442e-06.
outputs_pos.loss : 1.0332592725753784
outputs_pos.loss : 1.1348881721496582
outputs_pos.loss : 1.1493868827819824
outputs_pos.loss : 0.6717516779899597
outputs_pos.loss : 1.1492899656295776
outputs_pos.loss : 1.2635982036590576
outputs_pos.loss : 1.2597277164459229
outputs_pos.loss : 1.0171061754226685
Epoch 00956: adjusting learning rate of group 0 to 2.4440e-06.
outputs_pos.loss : 1.007678747177124
outputs_pos.loss : 1.055076241493225
outputs_pos.loss : 1.3417303562164307
outputs_pos.loss : 1.146691918373108
outputs_pos.loss : 0.8889862895011902
outputs_pos.loss : 1.1372928619384766
outputs_pos.loss : 0.9359691143035889
outputs_pos.loss : 1.0275404453277588
Epoch 00957: adjusting learning rate of group 0 to 2.4439e-06.
outputs_pos.loss : 1.3058698177337646
outputs_pos.loss : 1.3745746612548828
outputs_pos.loss : 1.2768266201019287
outputs_pos.loss : 0.943101167678833
outputs_pos.loss : 1.2232309579849243
outputs_pos.loss : 1.3094987869262695
outputs_pos.loss : 1.018576979637146
outputs_pos.loss : 1.8656648397445679
Epoch 00958: adjusting learning rate of group 0 to 2.4438e-06.
outputs_pos.loss : 1.0970523357391357
outputs_pos.loss : 1.2844072580337524
outputs_pos.loss : 1.0029518604278564
outputs_pos.loss : 1.0393904447555542
outputs_pos.loss : 1.2993485927581787
outputs_pos.loss : 1.0124707221984863
outputs_pos.loss : 0.960340142250061
outputs_pos.loss : 1.0175451040267944
Epoch 00959: adjusting learning rate of group 0 to 2.4437e-06.
outputs_pos.loss : 1.497208833694458
outputs_pos.loss : 1.074571132659912
outputs_pos.loss : 1.5999538898468018
outputs_pos.loss : 1.0142024755477905
outputs_pos.loss : 1.250969409942627
outputs_pos.loss : 1.2995423078536987
outputs_pos.loss : 0.7212891578674316
outputs_pos.loss : 1.2417243719100952
Epoch 00960: adjusting learning rate of group 0 to 2.4436e-06.
outputs_pos.loss : 0.9160959720611572
outputs_pos.loss : 0.911897599697113
outputs_pos.loss : 0.9153680205345154
outputs_pos.loss : 1.1354352235794067
outputs_pos.loss : 1.2401646375656128
outputs_pos.loss : 1.1796624660491943
outputs_pos.loss : 1.5185226202011108
outputs_pos.loss : 1.8411166667938232
Epoch 00961: adjusting learning rate of group 0 to 2.4435e-06.
outputs_pos.loss : 1.438012719154358
outputs_pos.loss : 1.09023118019104
outputs_pos.loss : 1.2925779819488525
outputs_pos.loss : 1.2417237758636475
outputs_pos.loss : 1.0920510292053223
outputs_pos.loss : 0.8674667477607727
outputs_pos.loss : 1.3096381425857544
outputs_pos.loss : 1.1024823188781738
Epoch 00962: adjusting learning rate of group 0 to 2.4433e-06.
outputs_pos.loss : 1.0789040327072144
outputs_pos.loss : 1.6052677631378174
outputs_pos.loss : 1.0297653675079346
outputs_pos.loss : 1.1662614345550537
outputs_pos.loss : 0.7786648869514465
outputs_pos.loss : 0.9794310331344604
outputs_pos.loss : 0.7690099477767944
outputs_pos.loss : 1.1661696434020996
Epoch 00963: adjusting learning rate of group 0 to 2.4432e-06.
outputs_pos.loss : 1.3698725700378418
outputs_pos.loss : 1.2157909870147705
outputs_pos.loss : 1.4607737064361572
outputs_pos.loss : 0.7490618824958801
outputs_pos.loss : 1.494616150856018
outputs_pos.loss : 0.8338183164596558
outputs_pos.loss : 1.4210271835327148
outputs_pos.loss : 1.0780879259109497
Epoch 00964: adjusting learning rate of group 0 to 2.4431e-06.
outputs_pos.loss : 1.5715022087097168
outputs_pos.loss : 1.3661285638809204
outputs_pos.loss : 0.8051801919937134
outputs_pos.loss : 1.8049179315567017
outputs_pos.loss : 1.0686450004577637
outputs_pos.loss : 1.2692300081253052
outputs_pos.loss : 1.0833919048309326
outputs_pos.loss : 1.0091041326522827
Epoch 00965: adjusting learning rate of group 0 to 2.4430e-06.
outputs_pos.loss : 1.0474896430969238
outputs_pos.loss : 1.16925048828125
outputs_pos.loss : 1.570514440536499
outputs_pos.loss : 1.49753737449646
outputs_pos.loss : 1.088968276977539
outputs_pos.loss : 1.041081190109253
outputs_pos.loss : 1.576784372329712
outputs_pos.loss : 1.1519769430160522
Epoch 00966: adjusting learning rate of group 0 to 2.4429e-06.
outputs_pos.loss : 1.4336092472076416
outputs_pos.loss : 1.133622646331787
outputs_pos.loss : 0.9879026412963867
outputs_pos.loss : 0.9483209252357483
outputs_pos.loss : 1.5834814310073853
outputs_pos.loss : 1.1133089065551758
outputs_pos.loss : 0.7866623401641846
outputs_pos.loss : 1.3172554969787598
Epoch 00967: adjusting learning rate of group 0 to 2.4428e-06.
outputs_pos.loss : 0.877573549747467
outputs_pos.loss : 1.1346232891082764
outputs_pos.loss : 1.0709463357925415
outputs_pos.loss : 0.8186821937561035
outputs_pos.loss : 1.2686011791229248
outputs_pos.loss : 1.296910047531128
outputs_pos.loss : 0.8933621644973755
outputs_pos.loss : 1.254758358001709
Epoch 00968: adjusting learning rate of group 0 to 2.4426e-06.
outputs_pos.loss : 1.0630290508270264
outputs_pos.loss : 1.5205111503601074
outputs_pos.loss : 1.4571597576141357
outputs_pos.loss : 1.3988019227981567
outputs_pos.loss : 1.4047160148620605
outputs_pos.loss : 1.8640187978744507
outputs_pos.loss : 1.3292580842971802
outputs_pos.loss : 0.8151066899299622
Epoch 00969: adjusting learning rate of group 0 to 2.4425e-06.
outputs_pos.loss : 1.6244269609451294
outputs_pos.loss : 0.9268820285797119
outputs_pos.loss : 1.1944481134414673
outputs_pos.loss : 1.2999407052993774
outputs_pos.loss : 0.9984521865844727
outputs_pos.loss : 1.152851939201355
outputs_pos.loss : 0.8981311321258545
outputs_pos.loss : 0.7099888920783997
Epoch 00970: adjusting learning rate of group 0 to 2.4424e-06.
outputs_pos.loss : 1.3442034721374512
outputs_pos.loss : 1.0017720460891724
outputs_pos.loss : 1.1764612197875977
outputs_pos.loss : 0.43580150604248047
outputs_pos.loss : 0.8607824444770813
outputs_pos.loss : 0.6927914619445801
outputs_pos.loss : 1.4443718194961548
outputs_pos.loss : 1.011542797088623
Epoch 00971: adjusting learning rate of group 0 to 2.4423e-06.
outputs_pos.loss : 0.8206104040145874
outputs_pos.loss : 1.3553088903427124
outputs_pos.loss : 1.1025372743606567
outputs_pos.loss : 1.0602595806121826
outputs_pos.loss : 1.2352007627487183
outputs_pos.loss : 1.23969304561615
outputs_pos.loss : 1.4473671913146973
outputs_pos.loss : 1.1178525686264038
Epoch 00972: adjusting learning rate of group 0 to 2.4422e-06.
outputs_pos.loss : 1.0989947319030762
outputs_pos.loss : 1.3345155715942383
outputs_pos.loss : 0.5815714597702026
outputs_pos.loss : 1.0656441450119019
outputs_pos.loss : 1.6392850875854492
outputs_pos.loss : 0.9884223937988281
outputs_pos.loss : 0.8303016424179077
outputs_pos.loss : 1.1169764995574951
Epoch 00973: adjusting learning rate of group 0 to 2.4421e-06.
outputs_pos.loss : 1.5570809841156006
outputs_pos.loss : 0.9161025881767273
outputs_pos.loss : 1.238059401512146
outputs_pos.loss : 1.1505029201507568
outputs_pos.loss : 1.2627476453781128
outputs_pos.loss : 0.8482644557952881
outputs_pos.loss : 1.4825758934020996
outputs_pos.loss : 1.5213549137115479
Epoch 00974: adjusting learning rate of group 0 to 2.4419e-06.
outputs_pos.loss : 1.3827142715454102
outputs_pos.loss : 1.0404994487762451
outputs_pos.loss : 1.53167724609375
outputs_pos.loss : 1.3217052221298218
outputs_pos.loss : 1.3127681016921997
outputs_pos.loss : 1.5553624629974365
outputs_pos.loss : 1.4085540771484375
outputs_pos.loss : 1.2476108074188232
Epoch 00975: adjusting learning rate of group 0 to 2.4418e-06.
outputs_pos.loss : 1.180281162261963
outputs_pos.loss : 1.128236174583435
outputs_pos.loss : 1.0255141258239746
outputs_pos.loss : 1.0653668642044067
outputs_pos.loss : 1.395948886871338
outputs_pos.loss : 1.4826968908309937
outputs_pos.loss : 1.0579694509506226
outputs_pos.loss : 1.0346429347991943
Epoch 00976: adjusting learning rate of group 0 to 2.4417e-06.
outputs_pos.loss : 0.7513473629951477
outputs_pos.loss : 0.9991662502288818
outputs_pos.loss : 1.184831976890564
outputs_pos.loss : 1.9454865455627441
outputs_pos.loss : 1.234573483467102
outputs_pos.loss : 1.2260655164718628
outputs_pos.loss : 0.9609881043434143
outputs_pos.loss : 1.7837015390396118
Epoch 00977: adjusting learning rate of group 0 to 2.4416e-06.
outputs_pos.loss : 1.1457029581069946
outputs_pos.loss : 0.9444944858551025
outputs_pos.loss : 1.2814953327178955
outputs_pos.loss : 1.024239420890808
outputs_pos.loss : 0.9848730564117432
outputs_pos.loss : 0.9426438212394714
outputs_pos.loss : 1.8784847259521484
outputs_pos.loss : 1.3391315937042236
Epoch 00978: adjusting learning rate of group 0 to 2.4415e-06.
outputs_pos.loss : 1.2531428337097168
outputs_pos.loss : 1.0306873321533203
outputs_pos.loss : 1.1956268548965454
outputs_pos.loss : 1.1936075687408447
outputs_pos.loss : 0.6190342903137207
outputs_pos.loss : 1.1206356287002563
outputs_pos.loss : 1.2944161891937256
outputs_pos.loss : 1.5641534328460693
Epoch 00979: adjusting learning rate of group 0 to 2.4413e-06.
outputs_pos.loss : 1.6182811260223389
outputs_pos.loss : 1.6352614164352417
outputs_pos.loss : 1.4112814664840698
outputs_pos.loss : 1.357727289199829
outputs_pos.loss : 1.0327701568603516
outputs_pos.loss : 1.4130066633224487
outputs_pos.loss : 1.3617980480194092
outputs_pos.loss : 1.0063936710357666
Epoch 00980: adjusting learning rate of group 0 to 2.4412e-06.
outputs_pos.loss : 1.1595622301101685
outputs_pos.loss : 1.0052390098571777
outputs_pos.loss : 1.12618887424469
outputs_pos.loss : 1.1250510215759277
outputs_pos.loss : 0.7332954406738281
outputs_pos.loss : 1.3086411952972412
outputs_pos.loss : 1.9176807403564453
outputs_pos.loss : 1.204476237297058
Epoch 00981: adjusting learning rate of group 0 to 2.4411e-06.
outputs_pos.loss : 0.9981129765510559
outputs_pos.loss : 1.1159359216690063
outputs_pos.loss : 1.262220859527588
outputs_pos.loss : 1.0515964031219482
outputs_pos.loss : 1.11709725856781
outputs_pos.loss : 0.9301268458366394
outputs_pos.loss : 0.9674498438835144
outputs_pos.loss : 1.5569120645523071
Epoch 00982: adjusting learning rate of group 0 to 2.4410e-06.
outputs_pos.loss : 1.0290895700454712
outputs_pos.loss : 1.195211410522461
outputs_pos.loss : 2.6188862323760986
outputs_pos.loss : 1.3065109252929688
outputs_pos.loss : 0.9158430099487305
outputs_pos.loss : 0.9942266941070557
outputs_pos.loss : 1.2924611568450928
outputs_pos.loss : 1.4574649333953857
Epoch 00983: adjusting learning rate of group 0 to 2.4409e-06.
outputs_pos.loss : 1.1095695495605469
outputs_pos.loss : 0.7340703010559082
outputs_pos.loss : 1.8796825408935547
outputs_pos.loss : 1.3038679361343384
outputs_pos.loss : 1.2123910188674927
outputs_pos.loss : 1.1411917209625244
outputs_pos.loss : 1.0889760255813599
outputs_pos.loss : 1.1399942636489868
Epoch 00984: adjusting learning rate of group 0 to 2.4407e-06.
outputs_pos.loss : 1.1122568845748901
outputs_pos.loss : 0.9897897839546204
outputs_pos.loss : 1.1890138387680054
outputs_pos.loss : 2.1635656356811523
outputs_pos.loss : 1.013975739479065
outputs_pos.loss : 1.9582622051239014
outputs_pos.loss : 1.0988198518753052
outputs_pos.loss : 1.0425678491592407
Epoch 00985: adjusting learning rate of group 0 to 2.4406e-06.
outputs_pos.loss : 1.053424596786499
outputs_pos.loss : 1.410109043121338
outputs_pos.loss : 0.8452054858207703
outputs_pos.loss : 1.2274335622787476
outputs_pos.loss : 0.9845340251922607
outputs_pos.loss : 1.6367213726043701
outputs_pos.loss : 1.0381615161895752
outputs_pos.loss : 1.7546205520629883
Epoch 00986: adjusting learning rate of group 0 to 2.4405e-06.
outputs_pos.loss : 1.1078734397888184
outputs_pos.loss : 1.4534934759140015
outputs_pos.loss : 1.1861708164215088
outputs_pos.loss : 0.9539379477500916
outputs_pos.loss : 1.1538105010986328
outputs_pos.loss : 0.762812614440918
outputs_pos.loss : 1.0330049991607666
outputs_pos.loss : 1.3244982957839966
Epoch 00987: adjusting learning rate of group 0 to 2.4404e-06.
outputs_pos.loss : 1.3833712339401245
outputs_pos.loss : 0.9195214509963989
outputs_pos.loss : 1.2936517000198364
outputs_pos.loss : 1.3121020793914795
outputs_pos.loss : 1.3939956426620483
outputs_pos.loss : 1.137434720993042
outputs_pos.loss : 1.193748116493225
outputs_pos.loss : 1.3747007846832275
Epoch 00988: adjusting learning rate of group 0 to 2.4403e-06.
outputs_pos.loss : 1.2389569282531738
outputs_pos.loss : 1.113803744316101
outputs_pos.loss : 1.335051417350769
outputs_pos.loss : 1.0365550518035889
outputs_pos.loss : 1.0618947744369507
outputs_pos.loss : 0.9532054662704468
outputs_pos.loss : 1.3992269039154053
outputs_pos.loss : 1.0523520708084106
Epoch 00989: adjusting learning rate of group 0 to 2.4401e-06.
outputs_pos.loss : 1.0283136367797852
outputs_pos.loss : 1.4713765382766724
outputs_pos.loss : 0.8925623297691345
outputs_pos.loss : 1.4473940134048462
outputs_pos.loss : 2.6703238487243652
outputs_pos.loss : 1.0920631885528564
outputs_pos.loss : 0.7885488867759705
outputs_pos.loss : 1.0809460878372192
Epoch 00990: adjusting learning rate of group 0 to 2.4400e-06.
outputs_pos.loss : 0.8689794540405273
outputs_pos.loss : 1.149758219718933
outputs_pos.loss : 1.094584345817566
outputs_pos.loss : 1.214776635169983
outputs_pos.loss : 1.1313761472702026
outputs_pos.loss : 0.7494537234306335
outputs_pos.loss : 0.8785082101821899
outputs_pos.loss : 1.1553016901016235
Epoch 00991: adjusting learning rate of group 0 to 2.4399e-06.
outputs_pos.loss : 1.171776294708252
outputs_pos.loss : 0.8427660465240479
outputs_pos.loss : 1.3182344436645508
outputs_pos.loss : 1.1467444896697998
outputs_pos.loss : 1.1871042251586914
outputs_pos.loss : 1.2582285404205322
outputs_pos.loss : 1.3253874778747559
outputs_pos.loss : 1.3317865133285522
Epoch 00992: adjusting learning rate of group 0 to 2.4398e-06.
outputs_pos.loss : 1.0756702423095703
outputs_pos.loss : 1.0666841268539429
outputs_pos.loss : 1.1198867559432983
outputs_pos.loss : 0.9342760443687439
outputs_pos.loss : 0.783461332321167
outputs_pos.loss : 1.1150907278060913
outputs_pos.loss : 1.3676244020462036
outputs_pos.loss : 1.7610479593276978
Epoch 00993: adjusting learning rate of group 0 to 2.4397e-06.
outputs_pos.loss : 1.0801682472229004
outputs_pos.loss : 1.3715633153915405
outputs_pos.loss : 1.326385498046875
outputs_pos.loss : 1.3742960691452026
outputs_pos.loss : 1.1649353504180908
outputs_pos.loss : 1.15475594997406
outputs_pos.loss : 0.8983163237571716
outputs_pos.loss : 0.9291880130767822
Epoch 00994: adjusting learning rate of group 0 to 2.4395e-06.
outputs_pos.loss : 0.8924184441566467
outputs_pos.loss : 1.1407641172409058
outputs_pos.loss : 0.7468635439872742
outputs_pos.loss : 1.125510334968567
outputs_pos.loss : 1.3291798830032349
outputs_pos.loss : 1.6003034114837646
outputs_pos.loss : 1.1412053108215332
outputs_pos.loss : 0.9966433644294739
Epoch 00995: adjusting learning rate of group 0 to 2.4394e-06.
outputs_pos.loss : 1.104688286781311
outputs_pos.loss : 1.010416865348816
outputs_pos.loss : 0.9853830933570862
outputs_pos.loss : 0.8305345773696899
outputs_pos.loss : 1.4046880006790161
outputs_pos.loss : 1.2718250751495361
outputs_pos.loss : 1.1368601322174072
outputs_pos.loss : 0.9993985295295715
Epoch 00996: adjusting learning rate of group 0 to 2.4393e-06.
outputs_pos.loss : 0.759334146976471
outputs_pos.loss : 0.9202852249145508
outputs_pos.loss : 0.931337296962738
outputs_pos.loss : 1.0532186031341553
outputs_pos.loss : 1.2846711874008179
outputs_pos.loss : 1.0103778839111328
outputs_pos.loss : 1.6449716091156006
outputs_pos.loss : 1.3601865768432617
Epoch 00997: adjusting learning rate of group 0 to 2.4392e-06.
outputs_pos.loss : 1.4325300455093384
outputs_pos.loss : 1.1174964904785156
outputs_pos.loss : 0.8089900612831116
outputs_pos.loss : 0.9958421587944031
outputs_pos.loss : 1.4766179323196411
outputs_pos.loss : 1.4347478151321411
outputs_pos.loss : 1.629698395729065
outputs_pos.loss : 1.2278236150741577
Epoch 00998: adjusting learning rate of group 0 to 2.4391e-06.
outputs_pos.loss : 0.9776725172996521
outputs_pos.loss : 1.6646883487701416
outputs_pos.loss : 1.2541295289993286
outputs_pos.loss : 1.1901652812957764
outputs_pos.loss : 1.6359238624572754
outputs_pos.loss : 1.2926151752471924
outputs_pos.loss : 0.8509378433227539
outputs_pos.loss : 0.7380658984184265
Epoch 00999: adjusting learning rate of group 0 to 2.4389e-06.
outputs_pos.loss : 0.7790181636810303
outputs_pos.loss : 1.3154997825622559
outputs_pos.loss : 1.1385679244995117
outputs_pos.loss : 0.9693206548690796
outputs_pos.loss : 1.0096622705459595
outputs_pos.loss : 1.180371880531311
outputs_pos.loss : 1.1211298704147339
outputs_pos.loss : 0.9667526483535767
Epoch 01000: adjusting learning rate of group 0 to 2.4388e-06.
outputs_pos.loss : 1.1528171300888062
outputs_pos.loss : 1.137413740158081
outputs_pos.loss : 0.8412811756134033
outputs_pos.loss : 0.9984663128852844
outputs_pos.loss : 1.1405377388000488
outputs_pos.loss : 1.0249879360198975
outputs_pos.loss : 0.6622622609138489
outputs_pos.loss : 1.2940878868103027
Epoch 01001: adjusting learning rate of group 0 to 2.4387e-06.
outputs_pos.loss : 0.9578502178192139
outputs_pos.loss : 1.9557290077209473
outputs_pos.loss : 1.6680803298950195
outputs_pos.loss : 1.4699089527130127
outputs_pos.loss : 1.3592324256896973
outputs_pos.loss : 0.7917868494987488
outputs_pos.loss : 0.8550099730491638
outputs_pos.loss : 0.6623876094818115
Epoch 01002: adjusting learning rate of group 0 to 2.4386e-06.
outputs_pos.loss : 1.1977943181991577
outputs_pos.loss : 0.9494656324386597
outputs_pos.loss : 1.1251912117004395
outputs_pos.loss : 1.2944782972335815
outputs_pos.loss : 1.7418241500854492
outputs_pos.loss : 0.9814060926437378
outputs_pos.loss : 0.5612248778343201
outputs_pos.loss : 1.2510781288146973
Epoch 01003: adjusting learning rate of group 0 to 2.4385e-06.
outputs_pos.loss : 1.10683274269104
outputs_pos.loss : 1.0956906080245972
outputs_pos.loss : 1.115535020828247
outputs_pos.loss : 1.125639796257019
outputs_pos.loss : 1.8789597749710083
outputs_pos.loss : 1.1302827596664429
outputs_pos.loss : 1.240077018737793
outputs_pos.loss : 0.9665815234184265
Epoch 01004: adjusting learning rate of group 0 to 2.4383e-06.
outputs_pos.loss : 1.5142474174499512
outputs_pos.loss : 0.9893925786018372
outputs_pos.loss : 1.0301433801651
outputs_pos.loss : 1.7545760869979858
outputs_pos.loss : 0.8622797727584839
outputs_pos.loss : 1.6743117570877075
outputs_pos.loss : 1.0277740955352783
outputs_pos.loss : 0.8519375920295715
Epoch 01005: adjusting learning rate of group 0 to 2.4382e-06.
outputs_pos.loss : 0.9818888902664185
outputs_pos.loss : 1.042725682258606
outputs_pos.loss : 1.2064722776412964
outputs_pos.loss : 1.1718387603759766
outputs_pos.loss : 1.1073580980300903
outputs_pos.loss : 1.2777825593948364
outputs_pos.loss : 1.4454400539398193
outputs_pos.loss : 0.9980837106704712
Epoch 01006: adjusting learning rate of group 0 to 2.4381e-06.
outputs_pos.loss : 1.4591939449310303
outputs_pos.loss : 1.17723548412323
outputs_pos.loss : 1.0293337106704712
outputs_pos.loss : 1.1683986186981201
outputs_pos.loss : 1.3859498500823975
outputs_pos.loss : 1.2948423624038696
outputs_pos.loss : 1.107099175453186
outputs_pos.loss : 1.096774697303772
Epoch 01007: adjusting learning rate of group 0 to 2.4380e-06.
outputs_pos.loss : 1.4588507413864136
outputs_pos.loss : 1.6120692491531372
outputs_pos.loss : 0.7491212487220764
outputs_pos.loss : 1.0994079113006592
outputs_pos.loss : 0.8037986159324646
outputs_pos.loss : 1.2032428979873657
outputs_pos.loss : 1.0463658571243286
outputs_pos.loss : 1.1928281784057617
Epoch 01008: adjusting learning rate of group 0 to 2.4378e-06.
outputs_pos.loss : 1.8077856302261353
outputs_pos.loss : 1.4357538223266602
outputs_pos.loss : 0.8296393752098083
outputs_pos.loss : 1.0318822860717773
outputs_pos.loss : 0.8692201972007751
outputs_pos.loss : 1.5957717895507812
outputs_pos.loss : 0.8513372540473938
outputs_pos.loss : 1.004780650138855
Epoch 01009: adjusting learning rate of group 0 to 2.4377e-06.
outputs_pos.loss : 1.3433560132980347
outputs_pos.loss : 1.3497231006622314
outputs_pos.loss : 0.9781146049499512
outputs_pos.loss : 1.2295821905136108
outputs_pos.loss : 1.2481276988983154
outputs_pos.loss : 1.291522741317749
outputs_pos.loss : 1.1383044719696045
outputs_pos.loss : 1.2580480575561523
Epoch 01010: adjusting learning rate of group 0 to 2.4376e-06.
outputs_pos.loss : 1.4263055324554443
outputs_pos.loss : 0.8106022477149963
outputs_pos.loss : 1.1678082942962646
outputs_pos.loss : 1.1730180978775024
outputs_pos.loss : 0.8905851244926453
outputs_pos.loss : 1.4059393405914307
outputs_pos.loss : 0.9491112232208252
outputs_pos.loss : 1.1532838344573975
Epoch 01011: adjusting learning rate of group 0 to 2.4375e-06.
outputs_pos.loss : 0.9104618430137634
outputs_pos.loss : 0.7395774126052856
outputs_pos.loss : 0.9719594717025757
outputs_pos.loss : 0.7136224508285522
outputs_pos.loss : 1.0917823314666748
outputs_pos.loss : 1.3837417364120483
outputs_pos.loss : 1.4297767877578735
outputs_pos.loss : 0.9893330931663513
Epoch 01012: adjusting learning rate of group 0 to 2.4374e-06.
outputs_pos.loss : 1.4999362230300903
outputs_pos.loss : 1.124747395515442
outputs_pos.loss : 1.016867756843567
outputs_pos.loss : 0.9047450423240662
outputs_pos.loss : 0.6911075711250305
outputs_pos.loss : 1.4903088808059692
outputs_pos.loss : 0.8464898467063904
outputs_pos.loss : 0.8966491222381592
Epoch 01013: adjusting learning rate of group 0 to 2.4372e-06.
outputs_pos.loss : 1.268189787864685
outputs_pos.loss : 1.2367585897445679
outputs_pos.loss : 1.0230885744094849
outputs_pos.loss : 1.3663378953933716
outputs_pos.loss : 0.8629292249679565
outputs_pos.loss : 1.3121658563613892
outputs_pos.loss : 0.8703363537788391
outputs_pos.loss : 1.066713571548462
Epoch 01014: adjusting learning rate of group 0 to 2.4371e-06.
outputs_pos.loss : 1.1350469589233398
outputs_pos.loss : 1.2521182298660278
outputs_pos.loss : 0.8671811819076538
outputs_pos.loss : 1.4217844009399414
outputs_pos.loss : 1.0758461952209473
outputs_pos.loss : 1.110587239265442
outputs_pos.loss : 1.1509779691696167
outputs_pos.loss : 1.3690770864486694
Epoch 01015: adjusting learning rate of group 0 to 2.4370e-06.
outputs_pos.loss : 0.932034432888031
outputs_pos.loss : 1.6055811643600464
outputs_pos.loss : 1.7634518146514893
outputs_pos.loss : 1.0424659252166748
outputs_pos.loss : 1.0997720956802368
outputs_pos.loss : 1.0272091627120972
outputs_pos.loss : 1.083085298538208
outputs_pos.loss : 1.0455279350280762
Epoch 01016: adjusting learning rate of group 0 to 2.4369e-06.
outputs_pos.loss : 1.2955743074417114
outputs_pos.loss : 0.9997164011001587
outputs_pos.loss : 1.0684705972671509
outputs_pos.loss : 1.138335943222046
outputs_pos.loss : 0.8185716867446899
outputs_pos.loss : 1.9647610187530518
outputs_pos.loss : 0.9731220602989197
outputs_pos.loss : 1.7234376668930054
Epoch 01017: adjusting learning rate of group 0 to 2.4367e-06.
outputs_pos.loss : 0.9459295868873596
outputs_pos.loss : 1.6069246530532837
outputs_pos.loss : 1.1238349676132202
outputs_pos.loss : 0.9699031114578247
outputs_pos.loss : 0.9267105460166931
outputs_pos.loss : 0.9154669046401978
outputs_pos.loss : 1.1657458543777466
outputs_pos.loss : 1.4647825956344604
Epoch 01018: adjusting learning rate of group 0 to 2.4366e-06.
outputs_pos.loss : 1.0052508115768433
outputs_pos.loss : 1.1423953771591187
outputs_pos.loss : 1.239991545677185
outputs_pos.loss : 1.3679896593093872
outputs_pos.loss : 0.9168252348899841
outputs_pos.loss : 1.061408519744873
outputs_pos.loss : 1.483769178390503
outputs_pos.loss : 1.0978742837905884
Epoch 01019: adjusting learning rate of group 0 to 2.4365e-06.
outputs_pos.loss : 0.9799308180809021
outputs_pos.loss : 0.9277727603912354
outputs_pos.loss : 1.3091224431991577
outputs_pos.loss : 1.0150127410888672
outputs_pos.loss : 0.9774702787399292
outputs_pos.loss : 1.4048588275909424
outputs_pos.loss : 0.9804587960243225
outputs_pos.loss : 1.236549973487854
Epoch 01020: adjusting learning rate of group 0 to 2.4364e-06.
outputs_pos.loss : 1.568265676498413
outputs_pos.loss : 1.0819036960601807
outputs_pos.loss : 0.9998344779014587
outputs_pos.loss : 0.5988455414772034
outputs_pos.loss : 0.9333683252334595
outputs_pos.loss : 0.9374210238456726
outputs_pos.loss : 0.6692211031913757
outputs_pos.loss : 0.7879461646080017
Epoch 01021: adjusting learning rate of group 0 to 2.4362e-06.
outputs_pos.loss : 1.4843056201934814
outputs_pos.loss : 1.3546770811080933
outputs_pos.loss : 1.5625289678573608
outputs_pos.loss : 0.9963987469673157
outputs_pos.loss : 1.267400860786438
outputs_pos.loss : 1.701479196548462
outputs_pos.loss : 0.9037493467330933
outputs_pos.loss : 1.1089985370635986
Epoch 01022: adjusting learning rate of group 0 to 2.4361e-06.
outputs_pos.loss : 1.4341827630996704
outputs_pos.loss : 0.8849902153015137
outputs_pos.loss : 1.3376508951187134
outputs_pos.loss : 1.2308485507965088
outputs_pos.loss : 1.1151450872421265
outputs_pos.loss : 1.3875101804733276
outputs_pos.loss : 1.2419624328613281
outputs_pos.loss : 1.3390995264053345
Epoch 01023: adjusting learning rate of group 0 to 2.4360e-06.
outputs_pos.loss : 1.0404423475265503
outputs_pos.loss : 1.1936193704605103
outputs_pos.loss : 0.8738921284675598
outputs_pos.loss : 1.2702808380126953
outputs_pos.loss : 1.4444659948349
outputs_pos.loss : 0.9769092798233032
outputs_pos.loss : 0.7772526741027832
outputs_pos.loss : 1.0303611755371094
Epoch 01024: adjusting learning rate of group 0 to 2.4359e-06.
outputs_pos.loss : 1.386955738067627
outputs_pos.loss : 1.383667230606079
outputs_pos.loss : 0.7027671933174133
outputs_pos.loss : 0.8229634165763855
outputs_pos.loss : 1.0079939365386963
outputs_pos.loss : 1.217887282371521
outputs_pos.loss : 1.3465676307678223
outputs_pos.loss : 1.4472568035125732
Epoch 01025: adjusting learning rate of group 0 to 2.4358e-06.
outputs_pos.loss : 1.1508982181549072
outputs_pos.loss : 0.9521027207374573
outputs_pos.loss : 1.0772011280059814
outputs_pos.loss : 1.3214079141616821
outputs_pos.loss : 1.2352538108825684
outputs_pos.loss : 1.411994218826294
outputs_pos.loss : 1.8500735759735107
outputs_pos.loss : 1.1995705366134644
Epoch 01026: adjusting learning rate of group 0 to 2.4356e-06.
outputs_pos.loss : 0.8703263401985168
outputs_pos.loss : 1.1172640323638916
outputs_pos.loss : 1.0907669067382812
outputs_pos.loss : 1.1927460432052612
outputs_pos.loss : 0.9325817227363586
outputs_pos.loss : 1.1793638467788696
outputs_pos.loss : 1.2819063663482666
outputs_pos.loss : 0.8795709013938904
Epoch 01027: adjusting learning rate of group 0 to 2.4355e-06.
outputs_pos.loss : 1.063779354095459
outputs_pos.loss : 2.0298988819122314
outputs_pos.loss : 2.81193470954895
outputs_pos.loss : 0.779496431350708
outputs_pos.loss : 1.1698482036590576
outputs_pos.loss : 1.4616807699203491
outputs_pos.loss : 0.986839771270752
outputs_pos.loss : 1.2967685461044312
Epoch 01028: adjusting learning rate of group 0 to 2.4354e-06.
outputs_pos.loss : 1.2122812271118164
outputs_pos.loss : 1.4911028146743774
outputs_pos.loss : 0.8926798105239868
outputs_pos.loss : 1.1258678436279297
outputs_pos.loss : 1.2753173112869263
outputs_pos.loss : 1.135444164276123
outputs_pos.loss : 1.2107737064361572
outputs_pos.loss : 0.9206635355949402
Epoch 01029: adjusting learning rate of group 0 to 2.4353e-06.
outputs_pos.loss : 1.2716479301452637
outputs_pos.loss : 1.3577295541763306
outputs_pos.loss : 1.6285165548324585
outputs_pos.loss : 1.3487305641174316
outputs_pos.loss : 1.2974110841751099
outputs_pos.loss : 0.9882612824440002
outputs_pos.loss : 0.7284087538719177
outputs_pos.loss : 0.9871368408203125
Epoch 01030: adjusting learning rate of group 0 to 2.4351e-06.
outputs_pos.loss : 0.6687717437744141
outputs_pos.loss : 1.5944652557373047
outputs_pos.loss : 1.3676787614822388
outputs_pos.loss : 1.3866065740585327
outputs_pos.loss : 1.6978400945663452
outputs_pos.loss : 1.0963228940963745
outputs_pos.loss : 1.8164269924163818
outputs_pos.loss : 1.2523558139801025
Epoch 01031: adjusting learning rate of group 0 to 2.4350e-06.
outputs_pos.loss : 1.2447007894515991
outputs_pos.loss : 1.0499345064163208
outputs_pos.loss : 1.2113349437713623
outputs_pos.loss : 0.7124338746070862
outputs_pos.loss : 0.8361091017723083
outputs_pos.loss : 1.3902525901794434
outputs_pos.loss : 1.1470736265182495
outputs_pos.loss : 1.1818815469741821
Epoch 01032: adjusting learning rate of group 0 to 2.4349e-06.
outputs_pos.loss : 1.2603391408920288
outputs_pos.loss : 1.3387792110443115
outputs_pos.loss : 1.0138401985168457
outputs_pos.loss : 1.51188063621521
outputs_pos.loss : 1.0604406595230103
outputs_pos.loss : 0.7229462265968323
outputs_pos.loss : 1.2917776107788086
outputs_pos.loss : 1.5779134035110474
Epoch 01033: adjusting learning rate of group 0 to 2.4348e-06.
outputs_pos.loss : 1.1160173416137695
outputs_pos.loss : 1.527164340019226
outputs_pos.loss : 0.9130176305770874
outputs_pos.loss : 1.0403897762298584
outputs_pos.loss : 1.1692860126495361
outputs_pos.loss : 1.0298618078231812
outputs_pos.loss : 1.5172251462936401
outputs_pos.loss : 1.5707935094833374
Epoch 01034: adjusting learning rate of group 0 to 2.4346e-06.
outputs_pos.loss : 1.1093674898147583
outputs_pos.loss : 0.8625739216804504
outputs_pos.loss : 0.9161744117736816
outputs_pos.loss : 0.9707286953926086
outputs_pos.loss : 1.2335246801376343
outputs_pos.loss : 1.140364170074463
outputs_pos.loss : 1.0858460664749146
outputs_pos.loss : 1.2714377641677856
Epoch 01035: adjusting learning rate of group 0 to 2.4345e-06.
outputs_pos.loss : 1.2960056066513062
outputs_pos.loss : 0.8712878823280334
outputs_pos.loss : 1.15315842628479
outputs_pos.loss : 1.4171953201293945
outputs_pos.loss : 0.6059622168540955
outputs_pos.loss : 1.4797993898391724
outputs_pos.loss : 0.8692545890808105
outputs_pos.loss : 0.9847963452339172
Epoch 01036: adjusting learning rate of group 0 to 2.4344e-06.
outputs_pos.loss : 0.9470940232276917
outputs_pos.loss : 1.3477855920791626
outputs_pos.loss : 1.3086458444595337
outputs_pos.loss : 0.847563624382019
outputs_pos.loss : 0.9066686034202576
outputs_pos.loss : 1.191711187362671
outputs_pos.loss : 1.4799070358276367
outputs_pos.loss : 0.9968293309211731
Epoch 01037: adjusting learning rate of group 0 to 2.4343e-06.
outputs_pos.loss : 1.3167147636413574
outputs_pos.loss : 0.8058947920799255
outputs_pos.loss : 1.1396249532699585
outputs_pos.loss : 0.6372371912002563
outputs_pos.loss : 1.0552706718444824
outputs_pos.loss : 1.8209494352340698
outputs_pos.loss : 2.0320136547088623
outputs_pos.loss : 1.3744010925292969
Epoch 01038: adjusting learning rate of group 0 to 2.4341e-06.
outputs_pos.loss : 1.1704870462417603
outputs_pos.loss : 1.1446781158447266
outputs_pos.loss : 1.3191434144973755
outputs_pos.loss : 0.7457945942878723
outputs_pos.loss : 0.8510830998420715
outputs_pos.loss : 0.39200082421302795
outputs_pos.loss : 0.8832748532295227
outputs_pos.loss : 1.007779598236084
Epoch 01039: adjusting learning rate of group 0 to 2.4340e-06.
outputs_pos.loss : 0.8321142792701721
outputs_pos.loss : 1.2473361492156982
outputs_pos.loss : 1.031085729598999
outputs_pos.loss : 1.2516566514968872
outputs_pos.loss : 1.3176270723342896
outputs_pos.loss : 1.1364368200302124
outputs_pos.loss : 1.3619874715805054
outputs_pos.loss : 1.240504503250122
Epoch 01040: adjusting learning rate of group 0 to 2.4339e-06.
outputs_pos.loss : 0.7612084746360779
outputs_pos.loss : 0.826194703578949
outputs_pos.loss : 0.9205744862556458
outputs_pos.loss : 0.9399917721748352
outputs_pos.loss : 1.649183750152588
outputs_pos.loss : 1.640194058418274
outputs_pos.loss : 1.0852903127670288
outputs_pos.loss : 1.3038544654846191
Epoch 01041: adjusting learning rate of group 0 to 2.4337e-06.
outputs_pos.loss : 1.424133539199829
outputs_pos.loss : 1.6785694360733032
outputs_pos.loss : 0.7136614918708801
outputs_pos.loss : 1.357374668121338
outputs_pos.loss : 0.8190754652023315
outputs_pos.loss : 0.9246877431869507
outputs_pos.loss : 1.723987340927124
outputs_pos.loss : 1.0244572162628174
Epoch 01042: adjusting learning rate of group 0 to 2.4336e-06.
outputs_pos.loss : 0.649333119392395
outputs_pos.loss : 1.1021875143051147
outputs_pos.loss : 1.1099286079406738
outputs_pos.loss : 1.0632102489471436
outputs_pos.loss : 1.0167372226715088
outputs_pos.loss : 0.988487720489502
outputs_pos.loss : 1.3716973066329956
outputs_pos.loss : 1.214788794517517
Epoch 01043: adjusting learning rate of group 0 to 2.4335e-06.
outputs_pos.loss : 1.2373137474060059
outputs_pos.loss : 1.105141520500183
outputs_pos.loss : 0.8469247221946716
outputs_pos.loss : 1.3629639148712158
outputs_pos.loss : 1.1999624967575073
outputs_pos.loss : 1.0186715126037598
outputs_pos.loss : 0.9973717927932739
outputs_pos.loss : 0.9891483187675476
Epoch 01044: adjusting learning rate of group 0 to 2.4334e-06.
outputs_pos.loss : 1.3146134614944458
outputs_pos.loss : 1.688841462135315
outputs_pos.loss : 1.083027720451355
outputs_pos.loss : 0.9406644105911255
outputs_pos.loss : 1.3942235708236694
outputs_pos.loss : 1.3137832880020142
outputs_pos.loss : 1.0920990705490112
outputs_pos.loss : 1.3147180080413818
Epoch 01045: adjusting learning rate of group 0 to 2.4332e-06.
outputs_pos.loss : 0.5766035914421082
outputs_pos.loss : 1.4931080341339111
outputs_pos.loss : 0.8529067039489746
outputs_pos.loss : 0.7874271869659424
outputs_pos.loss : 1.1273635625839233
outputs_pos.loss : 1.3325507640838623
outputs_pos.loss : 1.0994364023208618
outputs_pos.loss : 1.0568912029266357
Epoch 01046: adjusting learning rate of group 0 to 2.4331e-06.
outputs_pos.loss : 1.2822388410568237
outputs_pos.loss : 1.2831284999847412
outputs_pos.loss : 1.4205821752548218
outputs_pos.loss : 1.316579818725586
outputs_pos.loss : 1.4440385103225708
outputs_pos.loss : 1.2940489053726196
outputs_pos.loss : 1.2469483613967896
outputs_pos.loss : 1.0671794414520264
Epoch 01047: adjusting learning rate of group 0 to 2.4330e-06.
outputs_pos.loss : 1.435015320777893
outputs_pos.loss : 0.9145420789718628
outputs_pos.loss : 1.0957170724868774
outputs_pos.loss : 1.30740487575531
outputs_pos.loss : 1.0480566024780273
outputs_pos.loss : 1.0210152864456177
outputs_pos.loss : 1.444907546043396
outputs_pos.loss : 0.6679192781448364
Epoch 01048: adjusting learning rate of group 0 to 2.4329e-06.
outputs_pos.loss : 0.9730379581451416
outputs_pos.loss : 1.4108256101608276
outputs_pos.loss : 1.2549890279769897
outputs_pos.loss : 0.9507824778556824
outputs_pos.loss : 0.8854653835296631
outputs_pos.loss : 1.1525753736495972
outputs_pos.loss : 1.2804652452468872
outputs_pos.loss : 1.1313854455947876
Epoch 01049: adjusting learning rate of group 0 to 2.4327e-06.
outputs_pos.loss : 1.3286430835723877
outputs_pos.loss : 1.0228434801101685
outputs_pos.loss : 0.9948391318321228
outputs_pos.loss : 1.0879255533218384
outputs_pos.loss : 1.1207460165023804
outputs_pos.loss : 1.0321863889694214
outputs_pos.loss : 1.1263421773910522
outputs_pos.loss : 1.490477442741394
Epoch 01050: adjusting learning rate of group 0 to 2.4326e-06.
outputs_pos.loss : 1.3674476146697998
outputs_pos.loss : 1.2591328620910645
outputs_pos.loss : 1.6868482828140259
outputs_pos.loss : 1.576343059539795
outputs_pos.loss : 1.2183802127838135
outputs_pos.loss : 0.9400776028633118
outputs_pos.loss : 0.5490459203720093
outputs_pos.loss : 1.8498382568359375
Epoch 01051: adjusting learning rate of group 0 to 2.4325e-06.
outputs_pos.loss : 0.7192065119743347
outputs_pos.loss : 1.20900559425354
outputs_pos.loss : 1.4621914625167847
outputs_pos.loss : 1.55502450466156
outputs_pos.loss : 2.1869919300079346
outputs_pos.loss : 0.9346847534179688
outputs_pos.loss : 0.8987119197845459
outputs_pos.loss : 1.0912455320358276
Epoch 01052: adjusting learning rate of group 0 to 2.4324e-06.
outputs_pos.loss : 0.7041323184967041
outputs_pos.loss : 0.7060606479644775
outputs_pos.loss : 1.2560464143753052
outputs_pos.loss : 0.9736348986625671
outputs_pos.loss : 1.273512601852417
outputs_pos.loss : 1.0920490026474
outputs_pos.loss : 1.74074387550354
outputs_pos.loss : 0.6910343766212463
Epoch 01053: adjusting learning rate of group 0 to 2.4322e-06.
outputs_pos.loss : 1.5311120748519897
outputs_pos.loss : 1.3636444807052612
outputs_pos.loss : 1.3697891235351562
outputs_pos.loss : 0.777072012424469
outputs_pos.loss : 1.2750060558319092
outputs_pos.loss : 1.1149717569351196
outputs_pos.loss : 1.4373024702072144
outputs_pos.loss : 1.5799481868743896
Epoch 01054: adjusting learning rate of group 0 to 2.4321e-06.
outputs_pos.loss : 0.9279648065567017
outputs_pos.loss : 1.4679698944091797
outputs_pos.loss : 1.0854697227478027
outputs_pos.loss : 1.1140940189361572
outputs_pos.loss : 1.1377054452896118
outputs_pos.loss : 1.4681434631347656
outputs_pos.loss : 1.5674299001693726
outputs_pos.loss : 1.061265230178833
Epoch 01055: adjusting learning rate of group 0 to 2.4320e-06.
outputs_pos.loss : 1.3753232955932617
outputs_pos.loss : 0.8055492043495178
outputs_pos.loss : 1.1651517152786255
outputs_pos.loss : 1.216426134109497
outputs_pos.loss : 2.1079373359680176
outputs_pos.loss : 1.3154102563858032
outputs_pos.loss : 0.9776659607887268
outputs_pos.loss : 1.3594529628753662
Epoch 01056: adjusting learning rate of group 0 to 2.4318e-06.
outputs_pos.loss : 0.8031546473503113
outputs_pos.loss : 1.077788233757019
outputs_pos.loss : 1.070199966430664
outputs_pos.loss : 0.9863858819007874
outputs_pos.loss : 0.834350049495697
outputs_pos.loss : 1.233822226524353
outputs_pos.loss : 1.0618292093276978
outputs_pos.loss : 1.1445411443710327
Epoch 01057: adjusting learning rate of group 0 to 2.4317e-06.
outputs_pos.loss : 1.4407880306243896
outputs_pos.loss : 1.164409875869751
outputs_pos.loss : 1.032477617263794
outputs_pos.loss : 0.8233644962310791
outputs_pos.loss : 1.5632716417312622
outputs_pos.loss : 1.0911093950271606
outputs_pos.loss : 1.340796947479248
outputs_pos.loss : 0.6908115148544312
Epoch 01058: adjusting learning rate of group 0 to 2.4316e-06.
outputs_pos.loss : 1.574942946434021
outputs_pos.loss : 1.0701817274093628
outputs_pos.loss : 0.9304034113883972
outputs_pos.loss : 0.7000974416732788
outputs_pos.loss : 1.1767942905426025
outputs_pos.loss : 1.057148814201355
outputs_pos.loss : 1.1080571413040161
outputs_pos.loss : 0.9589579105377197
Epoch 01059: adjusting learning rate of group 0 to 2.4315e-06.
outputs_pos.loss : 1.211448311805725
outputs_pos.loss : 1.4113752841949463
outputs_pos.loss : 1.0147294998168945
outputs_pos.loss : 1.0248537063598633
outputs_pos.loss : 0.7018352150917053
outputs_pos.loss : 1.1893278360366821
outputs_pos.loss : 1.2000800371170044
outputs_pos.loss : 0.9792121052742004
Epoch 01060: adjusting learning rate of group 0 to 2.4313e-06.
outputs_pos.loss : 1.0204256772994995
outputs_pos.loss : 1.1671457290649414
outputs_pos.loss : 1.6766209602355957
outputs_pos.loss : 1.0409705638885498
outputs_pos.loss : 1.1681358814239502
outputs_pos.loss : 1.1260764598846436
outputs_pos.loss : 1.5892932415008545
outputs_pos.loss : 0.9736665487289429
Epoch 01061: adjusting learning rate of group 0 to 2.4312e-06.
outputs_pos.loss : 1.268643856048584
outputs_pos.loss : 1.1020433902740479
outputs_pos.loss : 1.367980718612671
outputs_pos.loss : 1.1693859100341797
outputs_pos.loss : 1.030140995979309
outputs_pos.loss : 1.0947730541229248
outputs_pos.loss : 1.0964043140411377
outputs_pos.loss : 1.3450545072555542
Epoch 01062: adjusting learning rate of group 0 to 2.4311e-06.
outputs_pos.loss : 1.234368085861206
outputs_pos.loss : 1.0089364051818848
outputs_pos.loss : 1.273988127708435
outputs_pos.loss : 1.106427550315857
outputs_pos.loss : 0.9105650186538696
outputs_pos.loss : 1.5714826583862305
outputs_pos.loss : 1.3628733158111572
outputs_pos.loss : 1.6373465061187744
Epoch 01063: adjusting learning rate of group 0 to 2.4309e-06.
outputs_pos.loss : 1.3325942754745483
outputs_pos.loss : 1.094688057899475
outputs_pos.loss : 1.056317925453186
outputs_pos.loss : 1.0333257913589478
outputs_pos.loss : 1.3397071361541748
outputs_pos.loss : 1.1741384267807007
outputs_pos.loss : 0.8861739039421082
outputs_pos.loss : 1.0695074796676636
Epoch 01064: adjusting learning rate of group 0 to 2.4308e-06.
outputs_pos.loss : 1.2065420150756836
outputs_pos.loss : 1.056825041770935
outputs_pos.loss : 1.0778801441192627
outputs_pos.loss : 1.2700852155685425
outputs_pos.loss : 0.8815017938613892
outputs_pos.loss : 1.21930992603302
outputs_pos.loss : 1.4263575077056885
outputs_pos.loss : 1.4174681901931763
Epoch 01065: adjusting learning rate of group 0 to 2.4307e-06.
outputs_pos.loss : 1.2301852703094482
outputs_pos.loss : 1.1490123271942139
outputs_pos.loss : 1.0861164331436157
outputs_pos.loss : 1.1640625
outputs_pos.loss : 1.3624032735824585
outputs_pos.loss : 1.1669163703918457
outputs_pos.loss : 0.981343686580658
outputs_pos.loss : 0.9809291362762451
Epoch 01066: adjusting learning rate of group 0 to 2.4306e-06.
outputs_pos.loss : 1.1903990507125854
outputs_pos.loss : 0.8433151841163635
outputs_pos.loss : 1.1847147941589355
outputs_pos.loss : 1.0482113361358643
outputs_pos.loss : 1.2162175178527832
outputs_pos.loss : 1.2499051094055176
outputs_pos.loss : 1.1351128816604614
outputs_pos.loss : 1.33281409740448
Epoch 01067: adjusting learning rate of group 0 to 2.4304e-06.
outputs_pos.loss : 1.609976887702942
outputs_pos.loss : 1.4822229146957397
outputs_pos.loss : 1.3994128704071045
outputs_pos.loss : 0.8046826124191284
outputs_pos.loss : 0.9588330388069153
outputs_pos.loss : 0.8990421295166016
outputs_pos.loss : 0.9022685289382935
outputs_pos.loss : 1.2203952074050903
Epoch 01068: adjusting learning rate of group 0 to 2.4303e-06.
outputs_pos.loss : 0.72139573097229
outputs_pos.loss : 1.2430570125579834
outputs_pos.loss : 0.8212292790412903
outputs_pos.loss : 1.010076880455017
outputs_pos.loss : 1.4896858930587769
outputs_pos.loss : 1.5597312450408936
outputs_pos.loss : 1.238608956336975
outputs_pos.loss : 0.8957746028900146
Epoch 01069: adjusting learning rate of group 0 to 2.4302e-06.
outputs_pos.loss : 0.8577229976654053
outputs_pos.loss : 1.5047489404678345
outputs_pos.loss : 1.4889086484909058
outputs_pos.loss : 1.7573835849761963
outputs_pos.loss : 1.0002596378326416
outputs_pos.loss : 0.8858526945114136
outputs_pos.loss : 1.2293310165405273
outputs_pos.loss : 1.1438895463943481
Epoch 01070: adjusting learning rate of group 0 to 2.4300e-06.
outputs_pos.loss : 1.2479891777038574
outputs_pos.loss : 1.0144609212875366
outputs_pos.loss : 1.1433053016662598
outputs_pos.loss : 1.3014756441116333
outputs_pos.loss : 1.0322321653366089
outputs_pos.loss : 0.8394409418106079
outputs_pos.loss : 1.1606109142303467
outputs_pos.loss : 1.0500375032424927
Epoch 01071: adjusting learning rate of group 0 to 2.4299e-06.
outputs_pos.loss : 1.5854406356811523
outputs_pos.loss : 0.8106322288513184
outputs_pos.loss : 1.1299049854278564
outputs_pos.loss : 1.7000387907028198
outputs_pos.loss : 1.2480552196502686
outputs_pos.loss : 0.9691674709320068
outputs_pos.loss : 0.9004335999488831
outputs_pos.loss : 1.2500288486480713
Epoch 01072: adjusting learning rate of group 0 to 2.4298e-06.
outputs_pos.loss : 0.9051510095596313
outputs_pos.loss : 1.0203535556793213
outputs_pos.loss : 1.6656674146652222
outputs_pos.loss : 0.8674071431159973
outputs_pos.loss : 1.4846152067184448
outputs_pos.loss : 0.9036231637001038
outputs_pos.loss : 1.1282495260238647
outputs_pos.loss : 0.8441280722618103
Epoch 01073: adjusting learning rate of group 0 to 2.4297e-06.
outputs_pos.loss : 1.20354163646698
outputs_pos.loss : 1.5210762023925781
outputs_pos.loss : 1.2507810592651367
outputs_pos.loss : 0.8925654888153076
outputs_pos.loss : 1.1463829278945923
outputs_pos.loss : 0.9172557592391968
outputs_pos.loss : 0.709768533706665
outputs_pos.loss : 0.9497204422950745
Epoch 01074: adjusting learning rate of group 0 to 2.4295e-06.
outputs_pos.loss : 1.0139321088790894
outputs_pos.loss : 0.8883993029594421
outputs_pos.loss : 1.375313401222229
outputs_pos.loss : 1.521490216255188
outputs_pos.loss : 1.160844326019287
outputs_pos.loss : 1.3805787563323975
outputs_pos.loss : 0.7589309811592102
outputs_pos.loss : 1.00800359249115
Epoch 01075: adjusting learning rate of group 0 to 2.4294e-06.
outputs_pos.loss : 1.6127713918685913
outputs_pos.loss : 0.9694491028785706
outputs_pos.loss : 1.0294265747070312
outputs_pos.loss : 1.1382373571395874
outputs_pos.loss : 0.5176958441734314
outputs_pos.loss : 1.432206392288208
outputs_pos.loss : 1.7412887811660767
outputs_pos.loss : 1.4467779397964478
Epoch 01076: adjusting learning rate of group 0 to 2.4293e-06.
outputs_pos.loss : 1.6055092811584473
outputs_pos.loss : 1.391488790512085
outputs_pos.loss : 0.906927227973938
outputs_pos.loss : 1.1886719465255737
outputs_pos.loss : 1.0431233644485474
outputs_pos.loss : 1.1213858127593994
outputs_pos.loss : 1.6712603569030762
outputs_pos.loss : 0.6073406934738159
Epoch 01077: adjusting learning rate of group 0 to 2.4291e-06.
outputs_pos.loss : 1.6783288717269897
outputs_pos.loss : 0.961208164691925
outputs_pos.loss : 2.2302515506744385
outputs_pos.loss : 1.0517433881759644
outputs_pos.loss : 1.0824415683746338
outputs_pos.loss : 1.3141212463378906
outputs_pos.loss : 1.212733507156372
outputs_pos.loss : 0.9304471015930176
Epoch 01078: adjusting learning rate of group 0 to 2.4290e-06.
outputs_pos.loss : 0.9616724252700806
outputs_pos.loss : 1.2029300928115845
outputs_pos.loss : 1.8875752687454224
outputs_pos.loss : 1.218209147453308
outputs_pos.loss : 1.1322206258773804
outputs_pos.loss : 1.889684796333313
outputs_pos.loss : 1.3428138494491577
outputs_pos.loss : 1.4846796989440918
Epoch 01079: adjusting learning rate of group 0 to 2.4289e-06.
outputs_pos.loss : 0.8989835381507874
outputs_pos.loss : 1.2837587594985962
outputs_pos.loss : 0.8807365894317627
outputs_pos.loss : 1.079319953918457
outputs_pos.loss : 0.9700762033462524
outputs_pos.loss : 1.0536736249923706
outputs_pos.loss : 1.3241386413574219
outputs_pos.loss : 1.0704553127288818
Epoch 01080: adjusting learning rate of group 0 to 2.4287e-06.
outputs_pos.loss : 1.1561201810836792
outputs_pos.loss : 1.1262811422348022
outputs_pos.loss : 1.2597159147262573
outputs_pos.loss : 1.0985734462738037
outputs_pos.loss : 1.0222856998443604
outputs_pos.loss : 0.9607791304588318
outputs_pos.loss : 1.1585452556610107
outputs_pos.loss : 1.041565179824829
Epoch 01081: adjusting learning rate of group 0 to 2.4286e-06.
outputs_pos.loss : 0.9234368205070496
outputs_pos.loss : 1.00359308719635
outputs_pos.loss : 1.2774511575698853
outputs_pos.loss : 1.4842491149902344
outputs_pos.loss : 1.4178543090820312
outputs_pos.loss : 1.313609004020691
outputs_pos.loss : 1.4321129322052002
outputs_pos.loss : 0.9587011933326721
Epoch 01082: adjusting learning rate of group 0 to 2.4285e-06.
outputs_pos.loss : 1.509629487991333
outputs_pos.loss : 0.9926888346672058
outputs_pos.loss : 1.2419579029083252
outputs_pos.loss : 1.2559982538223267
outputs_pos.loss : 1.198905348777771
outputs_pos.loss : 0.7406871914863586
outputs_pos.loss : 0.8519173860549927
outputs_pos.loss : 1.1089448928833008
Epoch 01083: adjusting learning rate of group 0 to 2.4283e-06.
outputs_pos.loss : 1.3551735877990723
outputs_pos.loss : 1.2929861545562744
outputs_pos.loss : 1.332341194152832
outputs_pos.loss : 1.5085796117782593
outputs_pos.loss : 1.3329728841781616
outputs_pos.loss : 1.2120506763458252
outputs_pos.loss : 1.0721684694290161
outputs_pos.loss : 1.154130458831787
Epoch 01084: adjusting learning rate of group 0 to 2.4282e-06.
outputs_pos.loss : 0.9564937353134155
outputs_pos.loss : 1.1939706802368164
outputs_pos.loss : 1.0977845191955566
outputs_pos.loss : 0.9333005547523499
outputs_pos.loss : 0.6295337080955505
outputs_pos.loss : 1.5111451148986816
outputs_pos.loss : 1.4669098854064941
outputs_pos.loss : 0.9406429529190063
Epoch 01085: adjusting learning rate of group 0 to 2.4281e-06.
outputs_pos.loss : 0.9824318289756775
outputs_pos.loss : 0.8932746052742004
outputs_pos.loss : 1.4829508066177368
outputs_pos.loss : 1.289472222328186
outputs_pos.loss : 1.1791845560073853
outputs_pos.loss : 0.9219425320625305
outputs_pos.loss : 0.9809333682060242
outputs_pos.loss : 1.1938222646713257
Epoch 01086: adjusting learning rate of group 0 to 2.4280e-06.
outputs_pos.loss : 1.3960552215576172
outputs_pos.loss : 0.9612972140312195
outputs_pos.loss : 0.9983426332473755
outputs_pos.loss : 1.2205220460891724
outputs_pos.loss : 1.0999863147735596
outputs_pos.loss : 1.3555660247802734
outputs_pos.loss : 1.5042001008987427
outputs_pos.loss : 0.910810649394989
Epoch 01087: adjusting learning rate of group 0 to 2.4278e-06.
outputs_pos.loss : 1.0291892290115356
outputs_pos.loss : 0.8378521800041199
outputs_pos.loss : 1.248486876487732
outputs_pos.loss : 1.0811930894851685
outputs_pos.loss : 0.6376701593399048
outputs_pos.loss : 1.1764817237854004
outputs_pos.loss : 1.1275655031204224
outputs_pos.loss : 0.9857622385025024
Epoch 01088: adjusting learning rate of group 0 to 2.4277e-06.
outputs_pos.loss : 0.8540011048316956
outputs_pos.loss : 1.314879298210144
outputs_pos.loss : 1.2871164083480835
outputs_pos.loss : 0.9020229578018188
outputs_pos.loss : 1.57555091381073
outputs_pos.loss : 1.0986493825912476
outputs_pos.loss : 1.1640043258666992
outputs_pos.loss : 1.4233288764953613
Epoch 01089: adjusting learning rate of group 0 to 2.4276e-06.
outputs_pos.loss : 0.9399071931838989
outputs_pos.loss : 1.2381179332733154
outputs_pos.loss : 1.0643824338912964
outputs_pos.loss : 1.223259449005127
outputs_pos.loss : 1.5191279649734497
outputs_pos.loss : 1.3099009990692139
outputs_pos.loss : 0.9908064603805542
outputs_pos.loss : 1.2334768772125244
Epoch 01090: adjusting learning rate of group 0 to 2.4274e-06.
outputs_pos.loss : 1.8191198110580444
outputs_pos.loss : 0.8469898700714111
outputs_pos.loss : 0.7794628143310547
outputs_pos.loss : 1.1506152153015137
outputs_pos.loss : 0.8230745792388916
outputs_pos.loss : 1.0199205875396729
outputs_pos.loss : 1.0948145389556885
outputs_pos.loss : 1.4482899904251099
Epoch 01091: adjusting learning rate of group 0 to 2.4273e-06.
outputs_pos.loss : 1.8050305843353271
outputs_pos.loss : 0.8375400304794312
outputs_pos.loss : 1.2917754650115967
outputs_pos.loss : 1.0764307975769043
outputs_pos.loss : 1.0101057291030884
outputs_pos.loss : 1.0722296237945557
outputs_pos.loss : 0.9740166664123535
outputs_pos.loss : 1.094101071357727
Epoch 01092: adjusting learning rate of group 0 to 2.4272e-06.
outputs_pos.loss : 1.2634433507919312
outputs_pos.loss : 1.0166300535202026
outputs_pos.loss : 0.950912594795227
outputs_pos.loss : 0.8587350249290466
outputs_pos.loss : 0.802802324295044
outputs_pos.loss : 0.948300302028656
outputs_pos.loss : 0.9580516219139099
outputs_pos.loss : 1.5340852737426758
Epoch 01093: adjusting learning rate of group 0 to 2.4270e-06.
outputs_pos.loss : 0.916631281375885
outputs_pos.loss : 1.3907307386398315
outputs_pos.loss : 0.7333588004112244
outputs_pos.loss : 0.9274033904075623
outputs_pos.loss : 0.9955954551696777
outputs_pos.loss : 1.0288565158843994
outputs_pos.loss : 0.8512347340583801
outputs_pos.loss : 1.174923300743103
Epoch 01094: adjusting learning rate of group 0 to 2.4269e-06.
outputs_pos.loss : 1.0978612899780273
outputs_pos.loss : 1.1973100900650024
outputs_pos.loss : 0.8622352480888367
outputs_pos.loss : 1.0151283740997314
outputs_pos.loss : 1.4446481466293335
outputs_pos.loss : 1.0237998962402344
outputs_pos.loss : 1.360460877418518
outputs_pos.loss : 0.8390139937400818
Epoch 01095: adjusting learning rate of group 0 to 2.4268e-06.
outputs_pos.loss : 1.1753613948822021
outputs_pos.loss : 0.9123978614807129
outputs_pos.loss : 1.2459697723388672
outputs_pos.loss : 1.5115532875061035
outputs_pos.loss : 1.2356531620025635
outputs_pos.loss : 1.1687450408935547
outputs_pos.loss : 1.055584192276001
outputs_pos.loss : 1.6261262893676758
Epoch 01096: adjusting learning rate of group 0 to 2.4266e-06.
outputs_pos.loss : 0.9974184036254883
outputs_pos.loss : 1.0634962320327759
outputs_pos.loss : 1.41299307346344
outputs_pos.loss : 1.662516713142395
outputs_pos.loss : 1.1195993423461914
outputs_pos.loss : 0.9666571617126465
outputs_pos.loss : 0.8084880113601685
outputs_pos.loss : 0.9019671082496643
Epoch 01097: adjusting learning rate of group 0 to 2.4265e-06.
outputs_pos.loss : 1.1479123830795288
outputs_pos.loss : 1.3342036008834839
outputs_pos.loss : 1.07437002658844
outputs_pos.loss : 1.4421817064285278
outputs_pos.loss : 2.1463706493377686
outputs_pos.loss : 0.9261213541030884
outputs_pos.loss : 0.9203241467475891
outputs_pos.loss : 1.0745640993118286
Epoch 01098: adjusting learning rate of group 0 to 2.4264e-06.
outputs_pos.loss : 1.235636591911316
outputs_pos.loss : 1.2299304008483887
outputs_pos.loss : 1.1258424520492554
outputs_pos.loss : 1.1272426843643188
outputs_pos.loss : 0.8622105121612549
outputs_pos.loss : 1.3578647375106812
outputs_pos.loss : 1.2739336490631104
outputs_pos.loss : 1.105979561805725
Epoch 01099: adjusting learning rate of group 0 to 2.4262e-06.
outputs_pos.loss : 1.3056857585906982
outputs_pos.loss : 1.807457685470581
outputs_pos.loss : 1.536629557609558
outputs_pos.loss : 1.4015830755233765
outputs_pos.loss : 1.2681773900985718
outputs_pos.loss : 1.007358431816101
outputs_pos.loss : 1.263767123222351
outputs_pos.loss : 0.9785556197166443
Epoch 01100: adjusting learning rate of group 0 to 2.4261e-06.
outputs_pos.loss : 0.8530704975128174
outputs_pos.loss : 0.8992465138435364
outputs_pos.loss : 1.478513240814209
outputs_pos.loss : 1.5875463485717773
outputs_pos.loss : 0.8552435636520386
outputs_pos.loss : 1.9437592029571533
outputs_pos.loss : 1.119709849357605
outputs_pos.loss : 1.1612563133239746
Epoch 01101: adjusting learning rate of group 0 to 2.4260e-06.
outputs_pos.loss : 1.393023133277893
outputs_pos.loss : 1.1232565641403198
outputs_pos.loss : 1.395900011062622
outputs_pos.loss : 1.4668463468551636
outputs_pos.loss : 1.1873657703399658
outputs_pos.loss : 1.0889482498168945
outputs_pos.loss : 1.5333783626556396
outputs_pos.loss : 1.081963062286377
Epoch 01102: adjusting learning rate of group 0 to 2.4258e-06.
outputs_pos.loss : 1.0837360620498657
outputs_pos.loss : 0.9772399663925171
outputs_pos.loss : 1.4216946363449097
outputs_pos.loss : 1.0358736515045166
outputs_pos.loss : 1.4507051706314087
outputs_pos.loss : 0.9769229888916016
outputs_pos.loss : 1.6381464004516602
outputs_pos.loss : 1.3341141939163208
Epoch 01103: adjusting learning rate of group 0 to 2.4257e-06.
outputs_pos.loss : 1.600992202758789
outputs_pos.loss : 1.410753607749939
outputs_pos.loss : 1.0758417844772339
outputs_pos.loss : 1.1907256841659546
outputs_pos.loss : 1.1429338455200195
outputs_pos.loss : 1.095897912979126
outputs_pos.loss : 0.8986743688583374
outputs_pos.loss : 1.2650569677352905
Epoch 01104: adjusting learning rate of group 0 to 2.4256e-06.
outputs_pos.loss : 1.3453456163406372
outputs_pos.loss : 1.0006818771362305
outputs_pos.loss : 0.9099443554878235
outputs_pos.loss : 1.0808740854263306
outputs_pos.loss : 0.9329704642295837
outputs_pos.loss : 1.5208297967910767
outputs_pos.loss : 0.8034940361976624
outputs_pos.loss : 0.8015632629394531
Epoch 01105: adjusting learning rate of group 0 to 2.4254e-06.
outputs_pos.loss : 1.2840635776519775
outputs_pos.loss : 1.0479546785354614
outputs_pos.loss : 1.1966277360916138
outputs_pos.loss : 1.204309344291687
outputs_pos.loss : 0.9723634123802185
outputs_pos.loss : 1.4509085416793823
outputs_pos.loss : 1.1637226343154907
outputs_pos.loss : 1.5465468168258667
Epoch 01106: adjusting learning rate of group 0 to 2.4253e-06.
outputs_pos.loss : 1.0106444358825684
outputs_pos.loss : 1.0371242761611938
outputs_pos.loss : 0.9173476696014404
outputs_pos.loss : 0.9932188391685486
outputs_pos.loss : 1.4232280254364014
outputs_pos.loss : 1.1234108209609985
outputs_pos.loss : 1.1584128141403198
outputs_pos.loss : 1.0616923570632935
Epoch 01107: adjusting learning rate of group 0 to 2.4252e-06.
outputs_pos.loss : 1.1981828212738037
outputs_pos.loss : 1.0216232538223267
outputs_pos.loss : 1.1242547035217285
outputs_pos.loss : 1.3394221067428589
outputs_pos.loss : 0.9619181752204895
outputs_pos.loss : 1.228515863418579
outputs_pos.loss : 0.700141191482544
outputs_pos.loss : 1.4618812799453735
Epoch 01108: adjusting learning rate of group 0 to 2.4250e-06.
outputs_pos.loss : 0.9290987849235535
outputs_pos.loss : 1.1673173904418945
outputs_pos.loss : 1.5176831483840942
outputs_pos.loss : 1.4185234308242798
outputs_pos.loss : 1.487210988998413
outputs_pos.loss : 1.0673627853393555
outputs_pos.loss : 1.4274202585220337
outputs_pos.loss : 0.7679402828216553
Epoch 01109: adjusting learning rate of group 0 to 2.4249e-06.
outputs_pos.loss : 1.1490380764007568
outputs_pos.loss : 0.9091830849647522
outputs_pos.loss : 1.0726077556610107
outputs_pos.loss : 1.4082982540130615
outputs_pos.loss : 1.182929515838623
outputs_pos.loss : 1.5560529232025146
outputs_pos.loss : 1.2231817245483398
outputs_pos.loss : 1.1873805522918701
Epoch 01110: adjusting learning rate of group 0 to 2.4248e-06.
outputs_pos.loss : 1.3455150127410889
outputs_pos.loss : 0.8493788838386536
outputs_pos.loss : 0.9966812133789062
outputs_pos.loss : 1.2707421779632568
outputs_pos.loss : 0.7179848551750183
outputs_pos.loss : 1.1033532619476318
outputs_pos.loss : 1.2795072793960571
outputs_pos.loss : 1.0985219478607178
Epoch 01111: adjusting learning rate of group 0 to 2.4246e-06.
outputs_pos.loss : 0.9753915071487427
outputs_pos.loss : 1.0588550567626953
outputs_pos.loss : 1.2697550058364868
outputs_pos.loss : 0.7087774276733398
outputs_pos.loss : 1.3157103061676025
outputs_pos.loss : 1.0069818496704102
outputs_pos.loss : 1.4121959209442139
outputs_pos.loss : 1.2346148490905762
Epoch 01112: adjusting learning rate of group 0 to 2.4245e-06.
outputs_pos.loss : 0.911334753036499
outputs_pos.loss : 0.8988581895828247
outputs_pos.loss : 1.831473708152771
outputs_pos.loss : 1.6582509279251099
outputs_pos.loss : 1.25613534450531
outputs_pos.loss : 1.3792318105697632
outputs_pos.loss : 1.032673716545105
outputs_pos.loss : 1.171012282371521
Epoch 01113: adjusting learning rate of group 0 to 2.4244e-06.
outputs_pos.loss : 0.8220458626747131
outputs_pos.loss : 0.9236593246459961
outputs_pos.loss : 1.0781595706939697
outputs_pos.loss : 0.787706196308136
outputs_pos.loss : 0.9571258425712585
outputs_pos.loss : 1.3215227127075195
outputs_pos.loss : 0.9986810684204102
outputs_pos.loss : 1.2899885177612305
Epoch 01114: adjusting learning rate of group 0 to 2.4242e-06.
outputs_pos.loss : 1.1128989458084106
outputs_pos.loss : 1.1518336534500122
outputs_pos.loss : 0.928597092628479
outputs_pos.loss : 0.937032163143158
outputs_pos.loss : 0.820439875125885
outputs_pos.loss : 0.7024584412574768
outputs_pos.loss : 0.9194769263267517
outputs_pos.loss : 1.0388975143432617
Epoch 01115: adjusting learning rate of group 0 to 2.4241e-06.
outputs_pos.loss : 1.0513920783996582
outputs_pos.loss : 1.114687442779541
outputs_pos.loss : 1.1496931314468384
outputs_pos.loss : 1.2606183290481567
outputs_pos.loss : 1.3287101984024048
outputs_pos.loss : 0.8207579255104065
outputs_pos.loss : 1.5185730457305908
outputs_pos.loss : 1.0633490085601807
Epoch 01116: adjusting learning rate of group 0 to 2.4240e-06.
outputs_pos.loss : 1.4789533615112305
outputs_pos.loss : 1.1905128955841064
outputs_pos.loss : 0.7908865809440613
outputs_pos.loss : 1.0867257118225098
outputs_pos.loss : 1.8298516273498535
outputs_pos.loss : 1.0487847328186035
outputs_pos.loss : 1.7112770080566406
outputs_pos.loss : 0.9102574586868286
Epoch 01117: adjusting learning rate of group 0 to 2.4238e-06.
outputs_pos.loss : 1.078033685684204
outputs_pos.loss : 1.1628257036209106
outputs_pos.loss : 1.2978912591934204
outputs_pos.loss : 1.2763426303863525
outputs_pos.loss : 0.9915819764137268
outputs_pos.loss : 1.0600488185882568
outputs_pos.loss : 1.985526442527771
outputs_pos.loss : 1.4619213342666626
Epoch 01118: adjusting learning rate of group 0 to 2.4237e-06.
outputs_pos.loss : 0.8328962922096252
outputs_pos.loss : 1.3786654472351074
outputs_pos.loss : 1.717164158821106
outputs_pos.loss : 1.216056227684021
outputs_pos.loss : 1.3806415796279907
outputs_pos.loss : 1.1402230262756348
outputs_pos.loss : 0.9109638333320618
outputs_pos.loss : 1.2563894987106323
Epoch 01119: adjusting learning rate of group 0 to 2.4236e-06.
outputs_pos.loss : 1.280198097229004
outputs_pos.loss : 0.8600154519081116
outputs_pos.loss : 0.8637678027153015
outputs_pos.loss : 0.8983874320983887
outputs_pos.loss : 1.1458251476287842
outputs_pos.loss : 1.0576282739639282
outputs_pos.loss : 0.9349380135536194
outputs_pos.loss : 1.3422869443893433
Epoch 01120: adjusting learning rate of group 0 to 2.4234e-06.
outputs_pos.loss : 1.5843086242675781
outputs_pos.loss : 0.9808110594749451
outputs_pos.loss : 1.2105183601379395
outputs_pos.loss : 1.9128741025924683
outputs_pos.loss : 1.1585590839385986
outputs_pos.loss : 1.230667233467102
outputs_pos.loss : 1.2316429615020752
outputs_pos.loss : 0.9251647591590881
Epoch 01121: adjusting learning rate of group 0 to 2.4233e-06.
outputs_pos.loss : 1.0524898767471313
outputs_pos.loss : 0.9804062843322754
outputs_pos.loss : 0.916785478591919
outputs_pos.loss : 1.4277522563934326
outputs_pos.loss : 1.3940798044204712
outputs_pos.loss : 1.5631276369094849
outputs_pos.loss : 0.6888989806175232
outputs_pos.loss : 1.1576122045516968
Epoch 01122: adjusting learning rate of group 0 to 2.4231e-06.
outputs_pos.loss : 1.1987836360931396
outputs_pos.loss : 1.2421058416366577
outputs_pos.loss : 1.3197325468063354
outputs_pos.loss : 1.1212602853775024
outputs_pos.loss : 1.068023443222046
outputs_pos.loss : 0.7888398170471191
outputs_pos.loss : 1.5016264915466309
outputs_pos.loss : 1.122873067855835
Epoch 01123: adjusting learning rate of group 0 to 2.4230e-06.
outputs_pos.loss : 1.3144400119781494
outputs_pos.loss : 1.3327358961105347
outputs_pos.loss : 1.1301825046539307
outputs_pos.loss : 1.5838024616241455
outputs_pos.loss : 0.9064653515815735
outputs_pos.loss : 1.0260847806930542
outputs_pos.loss : 1.083878755569458
outputs_pos.loss : 1.5530906915664673
Epoch 01124: adjusting learning rate of group 0 to 2.4229e-06.
outputs_pos.loss : 1.4654359817504883
outputs_pos.loss : 1.2093838453292847
outputs_pos.loss : 1.4671317338943481
outputs_pos.loss : 1.4330235719680786
outputs_pos.loss : 1.0287580490112305
outputs_pos.loss : 1.094328761100769
outputs_pos.loss : 1.144029140472412
outputs_pos.loss : 1.2848016023635864
Epoch 01125: adjusting learning rate of group 0 to 2.4227e-06.
outputs_pos.loss : 1.6609609127044678
outputs_pos.loss : 0.9571914672851562
outputs_pos.loss : 1.0536857843399048
outputs_pos.loss : 1.1562188863754272
outputs_pos.loss : 0.926060676574707
outputs_pos.loss : 1.0141236782073975
outputs_pos.loss : 1.7312448024749756
outputs_pos.loss : 0.8294950723648071
Epoch 01126: adjusting learning rate of group 0 to 2.4226e-06.
outputs_pos.loss : 1.2581007480621338
outputs_pos.loss : 1.0101945400238037
outputs_pos.loss : 1.1526306867599487
outputs_pos.loss : 1.0889843702316284
outputs_pos.loss : 1.0980697870254517
outputs_pos.loss : 1.2328786849975586
outputs_pos.loss : 1.6948163509368896
outputs_pos.loss : 0.8122366070747375
Epoch 01127: adjusting learning rate of group 0 to 2.4225e-06.
outputs_pos.loss : 1.3534356355667114
outputs_pos.loss : 0.9406245946884155
outputs_pos.loss : 1.421004056930542
outputs_pos.loss : 1.4802746772766113
outputs_pos.loss : 0.9853180050849915
outputs_pos.loss : 1.5782266855239868
outputs_pos.loss : 0.7831792235374451
outputs_pos.loss : 1.0723408460617065
Epoch 01128: adjusting learning rate of group 0 to 2.4223e-06.
outputs_pos.loss : 1.3809195756912231
outputs_pos.loss : 1.3641259670257568
outputs_pos.loss : 1.266455888748169
outputs_pos.loss : 1.174890160560608
outputs_pos.loss : 0.9215275645256042
outputs_pos.loss : 1.1006498336791992
outputs_pos.loss : 1.6012506484985352
outputs_pos.loss : 1.4934381246566772
Epoch 01129: adjusting learning rate of group 0 to 2.4222e-06.
outputs_pos.loss : 1.7187342643737793
outputs_pos.loss : 1.3142871856689453
outputs_pos.loss : 1.3264442682266235
outputs_pos.loss : 1.1774934530258179
outputs_pos.loss : 1.34627103805542
outputs_pos.loss : 1.0281096696853638
outputs_pos.loss : 1.5446386337280273
outputs_pos.loss : 1.0571988821029663
Epoch 01130: adjusting learning rate of group 0 to 2.4221e-06.
outputs_pos.loss : 0.9948633909225464
outputs_pos.loss : 1.2801568508148193
outputs_pos.loss : 1.1899603605270386
outputs_pos.loss : 0.8815141916275024
outputs_pos.loss : 1.385810375213623
outputs_pos.loss : 1.0817363262176514
outputs_pos.loss : 1.646730661392212
outputs_pos.loss : 1.5319690704345703
Epoch 01131: adjusting learning rate of group 0 to 2.4219e-06.
outputs_pos.loss : 1.3161221742630005
outputs_pos.loss : 1.2513904571533203
outputs_pos.loss : 1.6116613149642944
outputs_pos.loss : 1.1390210390090942
outputs_pos.loss : 1.3241215944290161
outputs_pos.loss : 1.4035608768463135
outputs_pos.loss : 1.1737041473388672
outputs_pos.loss : 1.2079155445098877
Epoch 01132: adjusting learning rate of group 0 to 2.4218e-06.
outputs_pos.loss : 1.2323764562606812
outputs_pos.loss : 1.5892860889434814
outputs_pos.loss : 1.393432378768921
outputs_pos.loss : 0.8317753672599792
outputs_pos.loss : 0.8425045609474182
outputs_pos.loss : 1.006433367729187
outputs_pos.loss : 1.21061372756958
outputs_pos.loss : 1.3162246942520142
Epoch 01133: adjusting learning rate of group 0 to 2.4216e-06.
outputs_pos.loss : 1.4950072765350342
outputs_pos.loss : 1.2549954652786255
outputs_pos.loss : 0.904657781124115
outputs_pos.loss : 1.2638250589370728
outputs_pos.loss : 0.9042340517044067
outputs_pos.loss : 1.438162922859192
outputs_pos.loss : 1.465113639831543
outputs_pos.loss : 1.1449559926986694
Epoch 01134: adjusting learning rate of group 0 to 2.4215e-06.
outputs_pos.loss : 1.0627244710922241
outputs_pos.loss : 1.0006681680679321
outputs_pos.loss : 0.9467154145240784
outputs_pos.loss : 1.0534557104110718
outputs_pos.loss : 1.2708758115768433
outputs_pos.loss : 1.5216866731643677
outputs_pos.loss : 1.4527206420898438
outputs_pos.loss : 1.1535216569900513
Epoch 01135: adjusting learning rate of group 0 to 2.4214e-06.
outputs_pos.loss : 1.3427197933197021
outputs_pos.loss : 1.164083480834961
outputs_pos.loss : 0.8558664321899414
outputs_pos.loss : 1.1743234395980835
outputs_pos.loss : 1.3762049674987793
outputs_pos.loss : 1.608275055885315
outputs_pos.loss : 0.7595237493515015
outputs_pos.loss : 1.8086907863616943
Epoch 01136: adjusting learning rate of group 0 to 2.4212e-06.
outputs_pos.loss : 1.2259283065795898
outputs_pos.loss : 1.1380188465118408
outputs_pos.loss : 1.236624836921692
outputs_pos.loss : 1.570137858390808
outputs_pos.loss : 1.626097559928894
outputs_pos.loss : 1.0060421228408813
outputs_pos.loss : 1.4343239068984985
outputs_pos.loss : 1.7691395282745361
Epoch 01137: adjusting learning rate of group 0 to 2.4211e-06.
outputs_pos.loss : 1.0277122259140015
outputs_pos.loss : 1.1861094236373901
outputs_pos.loss : 0.8715007901191711
outputs_pos.loss : 1.139378547668457
outputs_pos.loss : 0.9133074879646301
outputs_pos.loss : 0.9184162616729736
outputs_pos.loss : 1.076328158378601
outputs_pos.loss : 1.164232850074768
Epoch 01138: adjusting learning rate of group 0 to 2.4210e-06.
outputs_pos.loss : 1.0260735750198364
outputs_pos.loss : 0.8152247667312622
outputs_pos.loss : 0.7555899620056152
outputs_pos.loss : 0.9342284202575684
outputs_pos.loss : 1.3868623971939087
outputs_pos.loss : 1.178876280784607
outputs_pos.loss : 1.2842824459075928
outputs_pos.loss : 1.2403861284255981
Epoch 01139: adjusting learning rate of group 0 to 2.4208e-06.
outputs_pos.loss : 1.411376714706421
outputs_pos.loss : 0.9706389307975769
outputs_pos.loss : 0.9960085153579712
outputs_pos.loss : 0.8228317499160767
outputs_pos.loss : 1.4716490507125854
outputs_pos.loss : 1.074958086013794
outputs_pos.loss : 1.283475399017334
outputs_pos.loss : 1.1201503276824951
Epoch 01140: adjusting learning rate of group 0 to 2.4207e-06.
outputs_pos.loss : 1.157808542251587
outputs_pos.loss : 1.1864806413650513
outputs_pos.loss : 0.9163116216659546
outputs_pos.loss : 1.2905913591384888
outputs_pos.loss : 1.1479109525680542
outputs_pos.loss : 1.3594950437545776
outputs_pos.loss : 0.9567175507545471
outputs_pos.loss : 1.1786563396453857
Epoch 01141: adjusting learning rate of group 0 to 2.4205e-06.
outputs_pos.loss : 1.1985609531402588
outputs_pos.loss : 1.135772705078125
outputs_pos.loss : 1.1378248929977417
outputs_pos.loss : 0.962086021900177
outputs_pos.loss : 1.3199899196624756
outputs_pos.loss : 1.1934009790420532
outputs_pos.loss : 1.2671880722045898
outputs_pos.loss : 1.0849045515060425
Epoch 01142: adjusting learning rate of group 0 to 2.4204e-06.
outputs_pos.loss : 0.9542281627655029
outputs_pos.loss : 1.055419683456421
outputs_pos.loss : 1.4785901308059692
outputs_pos.loss : 0.9541882276535034
outputs_pos.loss : 1.3814321756362915
outputs_pos.loss : 1.2442221641540527
outputs_pos.loss : 1.1652486324310303
outputs_pos.loss : 1.3069288730621338
Epoch 01143: adjusting learning rate of group 0 to 2.4203e-06.
outputs_pos.loss : 0.9983861446380615
outputs_pos.loss : 0.8910449743270874
outputs_pos.loss : 1.1896909475326538
outputs_pos.loss : 1.194572925567627
outputs_pos.loss : 1.5129250288009644
outputs_pos.loss : 0.9866606593132019
outputs_pos.loss : 1.5716367959976196
outputs_pos.loss : 1.364846110343933
Epoch 01144: adjusting learning rate of group 0 to 2.4201e-06.
outputs_pos.loss : 0.7975320219993591
outputs_pos.loss : 1.0667827129364014
outputs_pos.loss : 0.8373669981956482
outputs_pos.loss : 1.3227516412734985
outputs_pos.loss : 1.2208471298217773
outputs_pos.loss : 1.0363718271255493
outputs_pos.loss : 1.2973250150680542
outputs_pos.loss : 0.9509373903274536
Epoch 01145: adjusting learning rate of group 0 to 2.4200e-06.
outputs_pos.loss : 1.5936541557312012
outputs_pos.loss : 1.5386518239974976
outputs_pos.loss : 1.1253571510314941
outputs_pos.loss : 1.0281857252120972
outputs_pos.loss : 1.1349425315856934
outputs_pos.loss : 1.1172268390655518
outputs_pos.loss : 0.709750235080719
outputs_pos.loss : 1.3445920944213867
Epoch 01146: adjusting learning rate of group 0 to 2.4199e-06.
outputs_pos.loss : 0.6427874565124512
outputs_pos.loss : 1.0615121126174927
outputs_pos.loss : 1.137291431427002
outputs_pos.loss : 1.0844749212265015
outputs_pos.loss : 0.940030038356781
outputs_pos.loss : 1.379091739654541
outputs_pos.loss : 0.9197964668273926
outputs_pos.loss : 1.350817322731018
Epoch 01147: adjusting learning rate of group 0 to 2.4197e-06.
outputs_pos.loss : 0.8788073658943176
outputs_pos.loss : 0.9840700626373291
outputs_pos.loss : 0.947846531867981
outputs_pos.loss : 1.2491240501403809
outputs_pos.loss : 1.3103151321411133
outputs_pos.loss : 1.3442976474761963
outputs_pos.loss : 0.9001854062080383
outputs_pos.loss : 1.1925947666168213
Epoch 01148: adjusting learning rate of group 0 to 2.4196e-06.
outputs_pos.loss : 1.1135162115097046
outputs_pos.loss : 1.2071197032928467
outputs_pos.loss : 1.3947151899337769
outputs_pos.loss : 0.9284359216690063
outputs_pos.loss : 0.8968994617462158
outputs_pos.loss : 0.9392390847206116
outputs_pos.loss : 0.8883229494094849
outputs_pos.loss : 1.224412441253662
Epoch 01149: adjusting learning rate of group 0 to 2.4194e-06.
outputs_pos.loss : 1.3260595798492432
outputs_pos.loss : 1.048028826713562
outputs_pos.loss : 1.4345839023590088
outputs_pos.loss : 1.3723909854888916
outputs_pos.loss : 1.1749573945999146
outputs_pos.loss : 1.3055896759033203
outputs_pos.loss : 1.1052249670028687
outputs_pos.loss : 1.2814960479736328
Epoch 01150: adjusting learning rate of group 0 to 2.4193e-06.
outputs_pos.loss : 1.1059536933898926
outputs_pos.loss : 1.4525189399719238
outputs_pos.loss : 1.3362330198287964
outputs_pos.loss : 0.7757177352905273
outputs_pos.loss : 1.2632331848144531
outputs_pos.loss : 1.3830771446228027
outputs_pos.loss : 1.4363925457000732
outputs_pos.loss : 0.9953855872154236
Epoch 01151: adjusting learning rate of group 0 to 2.4192e-06.
outputs_pos.loss : 1.745973825454712
outputs_pos.loss : 1.358920693397522
outputs_pos.loss : 1.6579254865646362
outputs_pos.loss : 0.7092925310134888
outputs_pos.loss : 1.533572793006897
outputs_pos.loss : 1.1002495288848877
outputs_pos.loss : 1.222408652305603
outputs_pos.loss : 0.9832091927528381
Epoch 01152: adjusting learning rate of group 0 to 2.4190e-06.
outputs_pos.loss : 1.157855749130249
outputs_pos.loss : 1.1152454614639282
outputs_pos.loss : 1.1085610389709473
outputs_pos.loss : 0.9935725331306458
outputs_pos.loss : 1.336763858795166
outputs_pos.loss : 0.9275791645050049
outputs_pos.loss : 1.0650001764297485
outputs_pos.loss : 1.419723629951477
Epoch 01153: adjusting learning rate of group 0 to 2.4189e-06.
outputs_pos.loss : 1.3050072193145752
outputs_pos.loss : 1.1684163808822632
outputs_pos.loss : 1.2247166633605957
outputs_pos.loss : 1.0795233249664307
outputs_pos.loss : 1.4167039394378662
outputs_pos.loss : 1.2369282245635986
outputs_pos.loss : 1.2618194818496704
outputs_pos.loss : 1.1413744688034058
Epoch 01154: adjusting learning rate of group 0 to 2.4187e-06.
outputs_pos.loss : 0.8844176530838013
outputs_pos.loss : 1.7601109743118286
outputs_pos.loss : 1.5554951429367065
outputs_pos.loss : 0.9716603755950928
outputs_pos.loss : 0.8779782056808472
outputs_pos.loss : 1.391702651977539
outputs_pos.loss : 1.2785760164260864
outputs_pos.loss : 0.9003149271011353
Epoch 01155: adjusting learning rate of group 0 to 2.4186e-06.
outputs_pos.loss : 1.338484525680542
outputs_pos.loss : 1.1983661651611328
outputs_pos.loss : 0.8148018717765808
outputs_pos.loss : 1.4146009683609009
outputs_pos.loss : 1.245369553565979
outputs_pos.loss : 0.9178561568260193
outputs_pos.loss : 1.2061702013015747
outputs_pos.loss : 0.9742743968963623
Epoch 01156: adjusting learning rate of group 0 to 2.4185e-06.
outputs_pos.loss : 1.348934531211853
outputs_pos.loss : 1.116214394569397
outputs_pos.loss : 2.036316394805908
outputs_pos.loss : 1.1173112392425537
outputs_pos.loss : 1.2822504043579102
outputs_pos.loss : 1.2263280153274536
outputs_pos.loss : 1.0654332637786865
outputs_pos.loss : 1.3682795763015747
Epoch 01157: adjusting learning rate of group 0 to 2.4183e-06.
outputs_pos.loss : 0.9814961552619934
outputs_pos.loss : 0.9019523859024048
outputs_pos.loss : 1.5742096900939941
outputs_pos.loss : 1.4362266063690186
outputs_pos.loss : 0.804754912853241
outputs_pos.loss : 1.1052749156951904
outputs_pos.loss : 1.0086232423782349
outputs_pos.loss : 1.398339867591858
Epoch 01158: adjusting learning rate of group 0 to 2.4182e-06.
outputs_pos.loss : 0.8355761170387268
outputs_pos.loss : 1.284946322441101
outputs_pos.loss : 1.3329534530639648
outputs_pos.loss : 1.0396299362182617
outputs_pos.loss : 1.3449395895004272
outputs_pos.loss : 0.6796544194221497
outputs_pos.loss : 1.1832554340362549
outputs_pos.loss : 1.2776976823806763
Epoch 01159: adjusting learning rate of group 0 to 2.4181e-06.
outputs_pos.loss : 1.7305643558502197
outputs_pos.loss : 1.2307319641113281
outputs_pos.loss : 1.2129366397857666
outputs_pos.loss : 0.9578769207000732
outputs_pos.loss : 0.7036914229393005
outputs_pos.loss : 0.8387444019317627
outputs_pos.loss : 1.0019810199737549
outputs_pos.loss : 1.360230803489685
Epoch 01160: adjusting learning rate of group 0 to 2.4179e-06.
outputs_pos.loss : 1.953480839729309
outputs_pos.loss : 1.543533205986023
outputs_pos.loss : 1.205148458480835
outputs_pos.loss : 1.348328948020935
outputs_pos.loss : 0.7439562082290649
outputs_pos.loss : 0.8785892128944397
outputs_pos.loss : 0.9721392393112183
outputs_pos.loss : 1.4853192567825317
Epoch 01161: adjusting learning rate of group 0 to 2.4178e-06.
outputs_pos.loss : 1.2848509550094604
outputs_pos.loss : 0.9897597432136536
outputs_pos.loss : 1.3888742923736572
outputs_pos.loss : 1.0269473791122437
outputs_pos.loss : 1.258676290512085
outputs_pos.loss : 0.9492043852806091
outputs_pos.loss : 1.558506727218628
outputs_pos.loss : 1.2341967821121216
Epoch 01162: adjusting learning rate of group 0 to 2.4176e-06.
outputs_pos.loss : 1.2888463735580444
outputs_pos.loss : 1.135429859161377
outputs_pos.loss : 1.1945559978485107
outputs_pos.loss : 0.8561123013496399
outputs_pos.loss : 0.909360408782959
outputs_pos.loss : 1.9447112083435059
outputs_pos.loss : 1.2413824796676636
outputs_pos.loss : 0.8818156123161316
Epoch 01163: adjusting learning rate of group 0 to 2.4175e-06.
outputs_pos.loss : 1.120894432067871
outputs_pos.loss : 1.3338565826416016
outputs_pos.loss : 1.2741429805755615
outputs_pos.loss : 1.4867933988571167
outputs_pos.loss : 0.7961541414260864
outputs_pos.loss : 1.1570730209350586
outputs_pos.loss : 1.1731711626052856
outputs_pos.loss : 1.7449146509170532
Epoch 01164: adjusting learning rate of group 0 to 2.4174e-06.
outputs_pos.loss : 1.242648720741272
outputs_pos.loss : 1.130287528038025
outputs_pos.loss : 1.028774380683899
outputs_pos.loss : 1.1179951429367065
outputs_pos.loss : 1.0072590112686157
outputs_pos.loss : 1.179120659828186
outputs_pos.loss : 0.9878355264663696
outputs_pos.loss : 1.7079112529754639
Epoch 01165: adjusting learning rate of group 0 to 2.4172e-06.
outputs_pos.loss : 1.4011436700820923
outputs_pos.loss : 1.549338698387146
outputs_pos.loss : 1.4198229312896729
outputs_pos.loss : 1.0816466808319092
outputs_pos.loss : 1.2432414293289185
outputs_pos.loss : 1.6209667921066284
outputs_pos.loss : 1.25514817237854
outputs_pos.loss : 1.0971537828445435
Epoch 01166: adjusting learning rate of group 0 to 2.4171e-06.
outputs_pos.loss : 1.4822968244552612
outputs_pos.loss : 1.2943427562713623
outputs_pos.loss : 1.1083757877349854
outputs_pos.loss : 1.0257937908172607
outputs_pos.loss : 0.8683947920799255
outputs_pos.loss : 1.5524580478668213
outputs_pos.loss : 1.2880715131759644
outputs_pos.loss : 0.8845481276512146
Epoch 01167: adjusting learning rate of group 0 to 2.4169e-06.
outputs_pos.loss : 1.1661174297332764
outputs_pos.loss : 0.9269629120826721
outputs_pos.loss : 1.1751973628997803
outputs_pos.loss : 1.4452067613601685
outputs_pos.loss : 1.4113401174545288
outputs_pos.loss : 1.4331244230270386
outputs_pos.loss : 1.2243472337722778
outputs_pos.loss : 1.3879878520965576
Epoch 01168: adjusting learning rate of group 0 to 2.4168e-06.
outputs_pos.loss : 1.2592475414276123
outputs_pos.loss : 0.9014670848846436
outputs_pos.loss : 1.0273597240447998
outputs_pos.loss : 1.2826266288757324
outputs_pos.loss : 1.1023057699203491
outputs_pos.loss : 1.3022469282150269
outputs_pos.loss : 0.6268377304077148
outputs_pos.loss : 0.9697993993759155
Epoch 01169: adjusting learning rate of group 0 to 2.4166e-06.
outputs_pos.loss : 0.9625023007392883
outputs_pos.loss : 1.1694427728652954
outputs_pos.loss : 1.1986708641052246
outputs_pos.loss : 1.448569416999817
outputs_pos.loss : 0.7784278988838196
outputs_pos.loss : 1.95802640914917
outputs_pos.loss : 1.2820922136306763
outputs_pos.loss : 1.0869619846343994
Epoch 01170: adjusting learning rate of group 0 to 2.4165e-06.
outputs_pos.loss : 1.1248584985733032
outputs_pos.loss : 1.0357753038406372
outputs_pos.loss : 1.2684729099273682
outputs_pos.loss : 1.3467825651168823
outputs_pos.loss : 1.329971432685852
outputs_pos.loss : 0.7469794154167175
outputs_pos.loss : 1.333622694015503
outputs_pos.loss : 1.459421992301941
Epoch 01171: adjusting learning rate of group 0 to 2.4164e-06.
outputs_pos.loss : 1.489472508430481
outputs_pos.loss : 1.2473526000976562
outputs_pos.loss : 1.489578127861023
outputs_pos.loss : 1.1350024938583374
outputs_pos.loss : 1.2453982830047607
outputs_pos.loss : 1.0699646472930908
outputs_pos.loss : 1.306420922279358
outputs_pos.loss : 1.48117995262146
Epoch 01172: adjusting learning rate of group 0 to 2.4162e-06.
outputs_pos.loss : 0.7729845643043518
outputs_pos.loss : 1.4644161462783813
outputs_pos.loss : 1.2194591760635376
outputs_pos.loss : 1.126021385192871
outputs_pos.loss : 1.1879123449325562
outputs_pos.loss : 1.057704210281372
outputs_pos.loss : 1.8218854665756226
outputs_pos.loss : 1.1778455972671509
Epoch 01173: adjusting learning rate of group 0 to 2.4161e-06.
outputs_pos.loss : 1.0701491832733154
outputs_pos.loss : 1.1224805116653442
outputs_pos.loss : 1.437095284461975
outputs_pos.loss : 1.1056238412857056
outputs_pos.loss : 1.382775902748108
outputs_pos.loss : 1.3119745254516602
outputs_pos.loss : 1.24769127368927
outputs_pos.loss : 0.6034764051437378
Epoch 01174: adjusting learning rate of group 0 to 2.4159e-06.
outputs_pos.loss : 0.822238028049469
outputs_pos.loss : 0.8640180230140686
outputs_pos.loss : 0.9013693332672119
outputs_pos.loss : 0.9490710496902466
outputs_pos.loss : 1.0512455701828003
outputs_pos.loss : 0.8998322486877441
outputs_pos.loss : 1.0687313079833984
outputs_pos.loss : 0.8318483233451843
Epoch 01175: adjusting learning rate of group 0 to 2.4158e-06.
outputs_pos.loss : 1.2245402336120605
outputs_pos.loss : 1.1038438081741333
outputs_pos.loss : 1.532862901687622
outputs_pos.loss : 1.2091444730758667
outputs_pos.loss : 1.3948817253112793
outputs_pos.loss : 1.1674761772155762
outputs_pos.loss : 1.0355452299118042
outputs_pos.loss : 1.1670050621032715
Epoch 01176: adjusting learning rate of group 0 to 2.4157e-06.
outputs_pos.loss : 1.0169786214828491
outputs_pos.loss : 1.306066632270813
outputs_pos.loss : 1.2346400022506714
outputs_pos.loss : 1.6802362203598022
outputs_pos.loss : 1.2015198469161987
outputs_pos.loss : 1.0839637517929077
outputs_pos.loss : 1.1581465005874634
outputs_pos.loss : 1.1368566751480103
Epoch 01177: adjusting learning rate of group 0 to 2.4155e-06.
outputs_pos.loss : 1.0885356664657593
outputs_pos.loss : 1.059563398361206
outputs_pos.loss : 0.8867143392562866
outputs_pos.loss : 1.1470612287521362
outputs_pos.loss : 1.4571707248687744
outputs_pos.loss : 0.7150084376335144
outputs_pos.loss : 1.3528296947479248
outputs_pos.loss : 0.99728924036026
Epoch 01178: adjusting learning rate of group 0 to 2.4154e-06.
outputs_pos.loss : 1.2896770238876343
outputs_pos.loss : 1.0899379253387451
outputs_pos.loss : 0.70000159740448
outputs_pos.loss : 1.2480415105819702
outputs_pos.loss : 0.9438698291778564
outputs_pos.loss : 1.3363736867904663
outputs_pos.loss : 1.1313599348068237
outputs_pos.loss : 1.3460530042648315
Epoch 01179: adjusting learning rate of group 0 to 2.4152e-06.
outputs_pos.loss : 0.6212927103042603
outputs_pos.loss : 1.416445016860962
outputs_pos.loss : 0.8497152924537659
outputs_pos.loss : 1.0335098505020142
outputs_pos.loss : 1.1839001178741455
outputs_pos.loss : 1.092631220817566
outputs_pos.loss : 1.097036361694336
outputs_pos.loss : 1.1000096797943115
Epoch 01180: adjusting learning rate of group 0 to 2.4151e-06.
outputs_pos.loss : 0.9385886192321777
outputs_pos.loss : 1.356696605682373
outputs_pos.loss : 0.8432360291481018
outputs_pos.loss : 1.6165331602096558
outputs_pos.loss : 1.0642526149749756
outputs_pos.loss : 1.0528141260147095
outputs_pos.loss : 1.3682154417037964
outputs_pos.loss : 0.9384971261024475
Epoch 01181: adjusting learning rate of group 0 to 2.4149e-06.
outputs_pos.loss : 1.3598957061767578
outputs_pos.loss : 1.1438788175582886
outputs_pos.loss : 1.489280343055725
outputs_pos.loss : 1.2518203258514404
outputs_pos.loss : 1.6016467809677124
outputs_pos.loss : 1.3442977666854858
outputs_pos.loss : 1.3093445301055908
outputs_pos.loss : 0.8794214725494385
Epoch 01182: adjusting learning rate of group 0 to 2.4148e-06.
outputs_pos.loss : 1.231045126914978
outputs_pos.loss : 0.8135581016540527
outputs_pos.loss : 0.9118571877479553
outputs_pos.loss : 0.6492232084274292
outputs_pos.loss : 1.1876120567321777
outputs_pos.loss : 1.7817769050598145
outputs_pos.loss : 1.1748021841049194
outputs_pos.loss : 1.8291205167770386
Epoch 01183: adjusting learning rate of group 0 to 2.4147e-06.
outputs_pos.loss : 1.1691292524337769
outputs_pos.loss : 0.9041152596473694
outputs_pos.loss : 1.1629172563552856
outputs_pos.loss : 0.9694806337356567
outputs_pos.loss : 0.7964505553245544
outputs_pos.loss : 1.7373067140579224
outputs_pos.loss : 1.0244337320327759
outputs_pos.loss : 0.965824544429779
Epoch 01184: adjusting learning rate of group 0 to 2.4145e-06.
outputs_pos.loss : 0.696023166179657
outputs_pos.loss : 1.6349079608917236
outputs_pos.loss : 1.3149306774139404
outputs_pos.loss : 1.350847601890564
outputs_pos.loss : 1.6423157453536987
outputs_pos.loss : 0.8566915392875671
outputs_pos.loss : 1.6287987232208252
outputs_pos.loss : 1.4243141412734985
Epoch 01185: adjusting learning rate of group 0 to 2.4144e-06.
outputs_pos.loss : 1.179708480834961
outputs_pos.loss : 1.1323975324630737
outputs_pos.loss : 0.794255256652832
outputs_pos.loss : 0.7959727048873901
outputs_pos.loss : 1.6533597707748413
outputs_pos.loss : 1.1570748090744019
outputs_pos.loss : 1.463967204093933
outputs_pos.loss : 0.7646689414978027
Epoch 01186: adjusting learning rate of group 0 to 2.4142e-06.
outputs_pos.loss : 1.2813324928283691
outputs_pos.loss : 0.8832253217697144
outputs_pos.loss : 1.5446325540542603
outputs_pos.loss : 1.2043423652648926
outputs_pos.loss : 1.398958444595337
outputs_pos.loss : 0.8181928992271423
outputs_pos.loss : 1.0132261514663696
outputs_pos.loss : 1.0936081409454346
Epoch 01187: adjusting learning rate of group 0 to 2.4141e-06.
outputs_pos.loss : 1.3648836612701416
outputs_pos.loss : 1.209131121635437
outputs_pos.loss : 1.4297175407409668
outputs_pos.loss : 1.4268234968185425
outputs_pos.loss : 1.0119893550872803
outputs_pos.loss : 0.8742536306381226
outputs_pos.loss : 1.2764477729797363
outputs_pos.loss : 1.4723304510116577
Epoch 01188: adjusting learning rate of group 0 to 2.4139e-06.
outputs_pos.loss : 1.1980085372924805
outputs_pos.loss : 1.773825764656067
outputs_pos.loss : 1.165347933769226
outputs_pos.loss : 0.9179808497428894
outputs_pos.loss : 1.0873730182647705
outputs_pos.loss : 1.0955091714859009
outputs_pos.loss : 1.404176950454712
outputs_pos.loss : 0.7743499279022217
Epoch 01189: adjusting learning rate of group 0 to 2.4138e-06.
outputs_pos.loss : 1.0864158868789673
outputs_pos.loss : 1.1364905834197998
outputs_pos.loss : 1.3114691972732544
outputs_pos.loss : 1.4619032144546509
outputs_pos.loss : 0.9371193051338196
outputs_pos.loss : 1.5095713138580322
outputs_pos.loss : 1.2943921089172363
outputs_pos.loss : 2.2323553562164307
Epoch 01190: adjusting learning rate of group 0 to 2.4137e-06.
outputs_pos.loss : 1.234711766242981
outputs_pos.loss : 2.085866689682007
outputs_pos.loss : 1.1839447021484375
outputs_pos.loss : 1.6470301151275635
outputs_pos.loss : 1.4714252948760986
outputs_pos.loss : 1.0626442432403564
outputs_pos.loss : 0.9910234212875366
outputs_pos.loss : 0.9566326141357422
Epoch 01191: adjusting learning rate of group 0 to 2.4135e-06.
outputs_pos.loss : 0.9179838299751282
outputs_pos.loss : 0.8408660292625427
outputs_pos.loss : 1.2359055280685425
outputs_pos.loss : 0.7720578908920288
outputs_pos.loss : 1.3189949989318848
outputs_pos.loss : 1.205098032951355
outputs_pos.loss : 1.0798205137252808
outputs_pos.loss : 1.3800709247589111
Epoch 01192: adjusting learning rate of group 0 to 2.4134e-06.
outputs_pos.loss : 1.3193873167037964
outputs_pos.loss : 1.0809955596923828
outputs_pos.loss : 1.6271179914474487
outputs_pos.loss : 1.2462904453277588
outputs_pos.loss : 1.10069739818573
outputs_pos.loss : 1.562293529510498
outputs_pos.loss : 1.2099401950836182
outputs_pos.loss : 1.0717706680297852
Epoch 01193: adjusting learning rate of group 0 to 2.4132e-06.
outputs_pos.loss : 1.0149400234222412
outputs_pos.loss : 1.1641881465911865
outputs_pos.loss : 1.024912714958191
outputs_pos.loss : 1.2146779298782349
outputs_pos.loss : 1.2586268186569214
outputs_pos.loss : 1.0800038576126099
outputs_pos.loss : 0.9784641265869141
outputs_pos.loss : 1.5443419218063354
Epoch 01194: adjusting learning rate of group 0 to 2.4131e-06.
outputs_pos.loss : 1.2112876176834106
outputs_pos.loss : 1.0779684782028198
outputs_pos.loss : 1.2049561738967896
outputs_pos.loss : 1.2911231517791748
outputs_pos.loss : 1.5573476552963257
outputs_pos.loss : 1.364857792854309
outputs_pos.loss : 0.5609215497970581
outputs_pos.loss : 0.7748172283172607
Epoch 01195: adjusting learning rate of group 0 to 2.4129e-06.
outputs_pos.loss : 1.098952293395996
outputs_pos.loss : 1.7285126447677612
outputs_pos.loss : 1.3413567543029785
outputs_pos.loss : 1.0316400527954102
outputs_pos.loss : 1.1723980903625488
outputs_pos.loss : 0.8601160645484924
outputs_pos.loss : 0.9389318227767944
outputs_pos.loss : 1.1593369245529175
Epoch 01196: adjusting learning rate of group 0 to 2.4128e-06.
outputs_pos.loss : 1.0851962566375732
outputs_pos.loss : 1.5097947120666504
outputs_pos.loss : 0.8273134231567383
outputs_pos.loss : 1.0554293394088745
outputs_pos.loss : 1.3139499425888062
outputs_pos.loss : 0.7892959117889404
outputs_pos.loss : 1.2229790687561035
outputs_pos.loss : 0.9076048731803894
Epoch 01197: adjusting learning rate of group 0 to 2.4127e-06.
outputs_pos.loss : 1.44352388381958
outputs_pos.loss : 1.4005955457687378
outputs_pos.loss : 1.1326754093170166
outputs_pos.loss : 1.1035611629486084
outputs_pos.loss : 0.7181279063224792
outputs_pos.loss : 1.2286913394927979
outputs_pos.loss : 0.8434706330299377
outputs_pos.loss : 0.8905779123306274
Epoch 01198: adjusting learning rate of group 0 to 2.4125e-06.
outputs_pos.loss : 1.1048673391342163
outputs_pos.loss : 0.8530353307723999
outputs_pos.loss : 1.489115834236145
outputs_pos.loss : 1.24159574508667
outputs_pos.loss : 0.8883571624755859
outputs_pos.loss : 0.9331382513046265
outputs_pos.loss : 1.2125343084335327
outputs_pos.loss : 0.7802028059959412
Epoch 01199: adjusting learning rate of group 0 to 2.4124e-06.
outputs_pos.loss : 1.386165738105774
outputs_pos.loss : 1.0821861028671265
outputs_pos.loss : 1.0338741540908813
outputs_pos.loss : 1.7902510166168213
outputs_pos.loss : 0.6621841788291931
outputs_pos.loss : 0.9336881041526794
outputs_pos.loss : 1.2486110925674438
outputs_pos.loss : 1.9077367782592773
Epoch 01200: adjusting learning rate of group 0 to 2.4122e-06.
outputs_pos.loss : 1.163163423538208
outputs_pos.loss : 0.9781231880187988
outputs_pos.loss : 1.7945480346679688
outputs_pos.loss : 1.5706130266189575
outputs_pos.loss : 1.0575734376907349
outputs_pos.loss : 1.7168792486190796
outputs_pos.loss : 1.1482073068618774
outputs_pos.loss : 0.9394269585609436
Epoch 01201: adjusting learning rate of group 0 to 2.4121e-06.
outputs_pos.loss : 1.379085659980774
outputs_pos.loss : 1.0889689922332764
outputs_pos.loss : 0.6774711012840271
outputs_pos.loss : 1.3445168733596802
outputs_pos.loss : 1.064564824104309
outputs_pos.loss : 0.9884995222091675
outputs_pos.loss : 1.2369548082351685
outputs_pos.loss : 0.9324357509613037
Epoch 01202: adjusting learning rate of group 0 to 2.4119e-06.
outputs_pos.loss : 1.5303151607513428
outputs_pos.loss : 1.3884896039962769
outputs_pos.loss : 1.3671823740005493
outputs_pos.loss : 1.2371340990066528
outputs_pos.loss : 0.8705530762672424
outputs_pos.loss : 1.120996356010437
outputs_pos.loss : 0.9850658774375916
outputs_pos.loss : 1.7677640914916992
Epoch 01203: adjusting learning rate of group 0 to 2.4118e-06.
outputs_pos.loss : 1.199993371963501
outputs_pos.loss : 0.9869778156280518
outputs_pos.loss : 0.8777719140052795
outputs_pos.loss : 1.7583518028259277
outputs_pos.loss : 1.0312670469284058
outputs_pos.loss : 1.5344780683517456
outputs_pos.loss : 1.1029186248779297
outputs_pos.loss : 1.5389448404312134
Epoch 01204: adjusting learning rate of group 0 to 2.4116e-06.
outputs_pos.loss : 1.18666410446167
outputs_pos.loss : 1.1657477617263794
outputs_pos.loss : 1.2381218671798706
outputs_pos.loss : 0.946694016456604
outputs_pos.loss : 1.6902118921279907
outputs_pos.loss : 1.0904921293258667
outputs_pos.loss : 1.267409324645996
outputs_pos.loss : 1.473036766052246
Epoch 01205: adjusting learning rate of group 0 to 2.4115e-06.
outputs_pos.loss : 1.0464767217636108
outputs_pos.loss : 1.4127916097640991
outputs_pos.loss : 1.248763084411621
outputs_pos.loss : 1.124294638633728
outputs_pos.loss : 1.3423575162887573
outputs_pos.loss : 1.5389742851257324
outputs_pos.loss : 0.9158851504325867
outputs_pos.loss : 1.1088061332702637
Epoch 01206: adjusting learning rate of group 0 to 2.4114e-06.
outputs_pos.loss : 1.1563888788223267
outputs_pos.loss : 0.9758531451225281
outputs_pos.loss : 1.3147225379943848
outputs_pos.loss : 0.942914605140686
outputs_pos.loss : 1.023371934890747
outputs_pos.loss : 1.3874374628067017
outputs_pos.loss : 1.3307969570159912
outputs_pos.loss : 0.8806753158569336
Epoch 01207: adjusting learning rate of group 0 to 2.4112e-06.
outputs_pos.loss : 2.2804789543151855
outputs_pos.loss : 1.284597396850586
outputs_pos.loss : 1.066467046737671
outputs_pos.loss : 0.8142218589782715
outputs_pos.loss : 1.523463487625122
outputs_pos.loss : 1.0784345865249634
outputs_pos.loss : 0.853486955165863
outputs_pos.loss : 0.834937572479248
Epoch 01208: adjusting learning rate of group 0 to 2.4111e-06.
outputs_pos.loss : 0.7117204070091248
outputs_pos.loss : 1.0054196119308472
outputs_pos.loss : 0.8602432012557983
outputs_pos.loss : 1.1074692010879517
outputs_pos.loss : 1.2400681972503662
outputs_pos.loss : 0.9492138624191284
outputs_pos.loss : 1.3018085956573486
outputs_pos.loss : 1.350897192955017
Epoch 01209: adjusting learning rate of group 0 to 2.4109e-06.
outputs_pos.loss : 1.244044303894043
outputs_pos.loss : 0.9658728241920471
outputs_pos.loss : 0.9184220433235168
outputs_pos.loss : 0.9893965721130371
outputs_pos.loss : 1.0035656690597534
outputs_pos.loss : 1.325005054473877
outputs_pos.loss : 1.7198083400726318
outputs_pos.loss : 1.0577472448349
Epoch 01210: adjusting learning rate of group 0 to 2.4108e-06.
outputs_pos.loss : 1.7547260522842407
outputs_pos.loss : 1.46945059299469
outputs_pos.loss : 1.1086453199386597
outputs_pos.loss : 1.5025337934494019
outputs_pos.loss : 1.3043054342269897
outputs_pos.loss : 1.5664838552474976
outputs_pos.loss : 1.1456011533737183
outputs_pos.loss : 1.5399889945983887
Epoch 01211: adjusting learning rate of group 0 to 2.4106e-06.
outputs_pos.loss : 1.5693186521530151
outputs_pos.loss : 0.968086302280426
outputs_pos.loss : 1.1267671585083008
outputs_pos.loss : 1.2414807081222534
outputs_pos.loss : 1.2550081014633179
outputs_pos.loss : 1.2467814683914185
outputs_pos.loss : 0.9432883262634277
outputs_pos.loss : 1.3337239027023315
Epoch 01212: adjusting learning rate of group 0 to 2.4105e-06.
outputs_pos.loss : 1.2918063402175903
outputs_pos.loss : 1.1591228246688843
outputs_pos.loss : 1.1680760383605957
outputs_pos.loss : 1.0212953090667725
outputs_pos.loss : 0.9961107969284058
outputs_pos.loss : 0.913735032081604
outputs_pos.loss : 1.4169306755065918
outputs_pos.loss : 1.0640250444412231
Epoch 01213: adjusting learning rate of group 0 to 2.4103e-06.
outputs_pos.loss : 1.507759690284729
outputs_pos.loss : 0.9159983396530151
outputs_pos.loss : 1.118308663368225
outputs_pos.loss : 1.2733608484268188
outputs_pos.loss : 1.0687094926834106
outputs_pos.loss : 0.9651454091072083
outputs_pos.loss : 1.064408779144287
outputs_pos.loss : 0.9989545941352844
Epoch 01214: adjusting learning rate of group 0 to 2.4102e-06.
outputs_pos.loss : 1.2845919132232666
outputs_pos.loss : 1.0936256647109985
outputs_pos.loss : 1.038835883140564
outputs_pos.loss : 1.2202495336532593
outputs_pos.loss : 0.9566038250923157
outputs_pos.loss : 1.1916109323501587
outputs_pos.loss : 0.9927425384521484
outputs_pos.loss : 0.838990330696106
Epoch 01215: adjusting learning rate of group 0 to 2.4100e-06.
outputs_pos.loss : 0.9272434711456299
outputs_pos.loss : 1.2834614515304565
outputs_pos.loss : 1.12547767162323
outputs_pos.loss : 0.991066575050354
outputs_pos.loss : 1.2765109539031982
outputs_pos.loss : 0.9762871861457825
outputs_pos.loss : 1.1299312114715576
outputs_pos.loss : 1.4704375267028809
Epoch 01216: adjusting learning rate of group 0 to 2.4099e-06.
outputs_pos.loss : 1.188189148902893
outputs_pos.loss : 1.0045416355133057
outputs_pos.loss : 1.0343722105026245
outputs_pos.loss : 1.202333688735962
outputs_pos.loss : 1.1230535507202148
outputs_pos.loss : 1.5969465970993042
outputs_pos.loss : 1.2742191553115845
outputs_pos.loss : 0.875120222568512
Epoch 01217: adjusting learning rate of group 0 to 2.4097e-06.
outputs_pos.loss : 1.4055289030075073
outputs_pos.loss : 1.003330945968628
outputs_pos.loss : 1.0610971450805664
outputs_pos.loss : 1.6597776412963867
outputs_pos.loss : 1.0286558866500854
outputs_pos.loss : 1.2721598148345947
outputs_pos.loss : 0.9471440315246582
outputs_pos.loss : 1.8503446578979492
Epoch 01218: adjusting learning rate of group 0 to 2.4096e-06.
outputs_pos.loss : 1.3555147647857666
outputs_pos.loss : 1.0197237730026245
outputs_pos.loss : 0.9533668756484985
outputs_pos.loss : 0.933002769947052
outputs_pos.loss : 0.9327238202095032
outputs_pos.loss : 1.2549378871917725
outputs_pos.loss : 1.1726189851760864
outputs_pos.loss : 1.2547328472137451
Epoch 01219: adjusting learning rate of group 0 to 2.4095e-06.
outputs_pos.loss : 0.9173378944396973
outputs_pos.loss : 1.6767982244491577
outputs_pos.loss : 1.1938047409057617
outputs_pos.loss : 1.5544203519821167
outputs_pos.loss : 1.1186782121658325
outputs_pos.loss : 1.525704264640808
outputs_pos.loss : 1.2736105918884277
outputs_pos.loss : 1.1649214029312134
Epoch 01220: adjusting learning rate of group 0 to 2.4093e-06.
outputs_pos.loss : 1.1022793054580688
outputs_pos.loss : 0.892329216003418
outputs_pos.loss : 1.5740700960159302
outputs_pos.loss : 1.3898487091064453
outputs_pos.loss : 0.8430889248847961
outputs_pos.loss : 1.5622692108154297
outputs_pos.loss : 0.921747088432312
outputs_pos.loss : 1.1624126434326172
Epoch 01221: adjusting learning rate of group 0 to 2.4092e-06.
outputs_pos.loss : 1.0912679433822632
outputs_pos.loss : 1.639090895652771
outputs_pos.loss : 1.0933998823165894
outputs_pos.loss : 1.491546392440796
outputs_pos.loss : 1.2174721956253052
outputs_pos.loss : 1.397531270980835
outputs_pos.loss : 0.8923554420471191
outputs_pos.loss : 1.0232497453689575
Epoch 01222: adjusting learning rate of group 0 to 2.4090e-06.
outputs_pos.loss : 1.2435057163238525
outputs_pos.loss : 1.423376441001892
outputs_pos.loss : 1.1734426021575928
outputs_pos.loss : 1.1668652296066284
outputs_pos.loss : 0.8187979459762573
outputs_pos.loss : 1.0019620656967163
outputs_pos.loss : 0.9876519441604614
outputs_pos.loss : 1.121373176574707
Epoch 01223: adjusting learning rate of group 0 to 2.4089e-06.
outputs_pos.loss : 1.176588773727417
outputs_pos.loss : 1.8621456623077393
outputs_pos.loss : 1.3361015319824219
outputs_pos.loss : 1.103240728378296
outputs_pos.loss : 0.9744601249694824
outputs_pos.loss : 0.8680605292320251
outputs_pos.loss : 1.1323328018188477
outputs_pos.loss : 1.339440941810608
Epoch 01224: adjusting learning rate of group 0 to 2.4087e-06.
outputs_pos.loss : 1.4201195240020752
outputs_pos.loss : 1.4701861143112183
outputs_pos.loss : 1.0988281965255737
outputs_pos.loss : 1.1701287031173706
outputs_pos.loss : 1.054845929145813
outputs_pos.loss : 1.019203782081604
outputs_pos.loss : 1.2914611101150513
outputs_pos.loss : 1.3822475671768188
Epoch 01225: adjusting learning rate of group 0 to 2.4086e-06.
outputs_pos.loss : 1.4284687042236328
outputs_pos.loss : 1.4291709661483765
outputs_pos.loss : 1.3968743085861206
outputs_pos.loss : 0.9280180931091309
outputs_pos.loss : 1.1983989477157593
outputs_pos.loss : 1.7238737344741821
outputs_pos.loss : 1.2501887083053589
outputs_pos.loss : 1.1695727109909058
Epoch 01226: adjusting learning rate of group 0 to 2.4084e-06.
outputs_pos.loss : 1.2733203172683716
outputs_pos.loss : 1.596494197845459
outputs_pos.loss : 0.7222517728805542
outputs_pos.loss : 1.5080283880233765
outputs_pos.loss : 0.863173246383667
outputs_pos.loss : 0.9092338681221008
outputs_pos.loss : 1.1358616352081299
outputs_pos.loss : 1.0465081930160522
Epoch 01227: adjusting learning rate of group 0 to 2.4083e-06.
outputs_pos.loss : 1.798491358757019
outputs_pos.loss : 1.5913044214248657
outputs_pos.loss : 1.0227199792861938
outputs_pos.loss : 1.4510431289672852
outputs_pos.loss : 1.1921900510787964
outputs_pos.loss : 1.033207893371582
outputs_pos.loss : 1.3332873582839966
outputs_pos.loss : 1.3873449563980103
Epoch 01228: adjusting learning rate of group 0 to 2.4081e-06.
outputs_pos.loss : 0.9616818428039551
outputs_pos.loss : 0.8616044521331787
outputs_pos.loss : 1.3356738090515137
outputs_pos.loss : 1.3680132627487183
outputs_pos.loss : 1.0389121770858765
outputs_pos.loss : 1.3391096591949463
outputs_pos.loss : 1.0925004482269287
outputs_pos.loss : 1.3650926351547241
Epoch 01229: adjusting learning rate of group 0 to 2.4080e-06.
outputs_pos.loss : 1.1623350381851196
outputs_pos.loss : 0.9985104203224182
outputs_pos.loss : 1.0655641555786133
outputs_pos.loss : 1.3465977907180786
outputs_pos.loss : 0.9891777038574219
outputs_pos.loss : 1.456936001777649
outputs_pos.loss : 0.8206800222396851
outputs_pos.loss : 1.2089588642120361
Epoch 01230: adjusting learning rate of group 0 to 2.4078e-06.
outputs_pos.loss : 1.4073026180267334
outputs_pos.loss : 1.0233469009399414
outputs_pos.loss : 1.3078694343566895
outputs_pos.loss : 0.7108408808708191
outputs_pos.loss : 0.878514289855957
outputs_pos.loss : 1.2461938858032227
outputs_pos.loss : 1.1528868675231934
outputs_pos.loss : 1.2845929861068726
Epoch 01231: adjusting learning rate of group 0 to 2.4077e-06.
outputs_pos.loss : 2.037111759185791
outputs_pos.loss : 1.0424021482467651
outputs_pos.loss : 0.7927760481834412
outputs_pos.loss : 1.0718528032302856
outputs_pos.loss : 0.9641796946525574
outputs_pos.loss : 0.830670952796936
outputs_pos.loss : 0.9330923557281494
outputs_pos.loss : 1.3272966146469116
Epoch 01232: adjusting learning rate of group 0 to 2.4075e-06.
outputs_pos.loss : 1.0616406202316284
outputs_pos.loss : 1.1961604356765747
outputs_pos.loss : 1.4981392621994019
outputs_pos.loss : 1.6727886199951172
outputs_pos.loss : 1.4096009731292725
outputs_pos.loss : 1.4437049627304077
outputs_pos.loss : 1.1148693561553955
outputs_pos.loss : 0.9391195774078369
Epoch 01233: adjusting learning rate of group 0 to 2.4074e-06.
outputs_pos.loss : 0.7897325158119202
outputs_pos.loss : 1.1496429443359375
outputs_pos.loss : 1.3301279544830322
outputs_pos.loss : 1.1539461612701416
outputs_pos.loss : 1.1033374071121216
outputs_pos.loss : 1.3793666362762451
outputs_pos.loss : 0.918476939201355
outputs_pos.loss : 1.115715742111206
Epoch 01234: adjusting learning rate of group 0 to 2.4072e-06.
outputs_pos.loss : 1.309316635131836
outputs_pos.loss : 1.19810950756073
outputs_pos.loss : 1.5821431875228882
outputs_pos.loss : 1.1212273836135864
outputs_pos.loss : 0.690338134765625
outputs_pos.loss : 1.8702274560928345
outputs_pos.loss : 1.1287002563476562
outputs_pos.loss : 0.8623948693275452
Epoch 01235: adjusting learning rate of group 0 to 2.4071e-06.
outputs_pos.loss : 1.2165271043777466
outputs_pos.loss : 1.1710255146026611
outputs_pos.loss : 1.064632534980774
outputs_pos.loss : 1.1190495491027832
outputs_pos.loss : 1.0741227865219116
outputs_pos.loss : 1.1048821210861206
outputs_pos.loss : 1.0417574644088745
outputs_pos.loss : 1.0518187284469604
Epoch 01236: adjusting learning rate of group 0 to 2.4069e-06.
outputs_pos.loss : 1.4298012256622314
outputs_pos.loss : 1.3267818689346313
outputs_pos.loss : 1.6803139448165894
outputs_pos.loss : 0.8781126141548157
outputs_pos.loss : 1.253763198852539
outputs_pos.loss : 1.0911500453948975
outputs_pos.loss : 0.8526396155357361
outputs_pos.loss : 1.9675050973892212
Epoch 01237: adjusting learning rate of group 0 to 2.4068e-06.
outputs_pos.loss : 1.3911327123641968
outputs_pos.loss : 1.2030969858169556
outputs_pos.loss : 0.9409111142158508
outputs_pos.loss : 0.5088393092155457
outputs_pos.loss : 1.2605241537094116
outputs_pos.loss : 1.3698821067810059
outputs_pos.loss : 1.2102817296981812
outputs_pos.loss : 0.9075996279716492
Epoch 01238: adjusting learning rate of group 0 to 2.4066e-06.
outputs_pos.loss : 1.8544644117355347
outputs_pos.loss : 1.193023443222046
outputs_pos.loss : 1.2472443580627441
outputs_pos.loss : 1.1304371356964111
outputs_pos.loss : 0.983913242816925
outputs_pos.loss : 0.7610881328582764
outputs_pos.loss : 1.117309331893921
outputs_pos.loss : 1.231208086013794
Epoch 01239: adjusting learning rate of group 0 to 2.4065e-06.
outputs_pos.loss : 1.0924530029296875
outputs_pos.loss : 1.3880932331085205
outputs_pos.loss : 1.045837640762329
outputs_pos.loss : 1.4347296953201294
outputs_pos.loss : 1.0448403358459473
outputs_pos.loss : 0.6647014021873474
outputs_pos.loss : 1.8406710624694824
outputs_pos.loss : 1.2297747135162354
Epoch 01240: adjusting learning rate of group 0 to 2.4063e-06.
outputs_pos.loss : 0.999789834022522
outputs_pos.loss : 0.8293383717536926
outputs_pos.loss : 1.0324814319610596
outputs_pos.loss : 0.9947172999382019
outputs_pos.loss : 1.603270173072815
outputs_pos.loss : 0.726020097732544
outputs_pos.loss : 0.9056283235549927
outputs_pos.loss : 1.1809806823730469
Epoch 01241: adjusting learning rate of group 0 to 2.4062e-06.
outputs_pos.loss : 0.965625524520874
outputs_pos.loss : 1.3832406997680664
outputs_pos.loss : 1.6982852220535278
outputs_pos.loss : 1.4722892045974731
outputs_pos.loss : 1.4574564695358276
outputs_pos.loss : 0.680540919303894
outputs_pos.loss : 0.9136171340942383
outputs_pos.loss : 1.1845862865447998
Epoch 01242: adjusting learning rate of group 0 to 2.4060e-06.
outputs_pos.loss : 1.4055471420288086
outputs_pos.loss : 1.392964243888855
outputs_pos.loss : 1.2694543600082397
outputs_pos.loss : 0.9461665153503418
outputs_pos.loss : 0.7886841297149658
outputs_pos.loss : 1.3722745180130005
outputs_pos.loss : 1.034136414527893
outputs_pos.loss : 1.311004877090454
Epoch 01243: adjusting learning rate of group 0 to 2.4059e-06.
outputs_pos.loss : 1.2446212768554688
outputs_pos.loss : 1.3717964887619019
outputs_pos.loss : 0.6065585017204285
outputs_pos.loss : 1.2057815790176392
outputs_pos.loss : 1.4994151592254639
outputs_pos.loss : 1.0784666538238525
outputs_pos.loss : 1.435529351234436
outputs_pos.loss : 0.9851354956626892
Epoch 01244: adjusting learning rate of group 0 to 2.4057e-06.
outputs_pos.loss : 1.0567188262939453
outputs_pos.loss : 1.1508406400680542
outputs_pos.loss : 1.2280256748199463
outputs_pos.loss : 1.1781353950500488
outputs_pos.loss : 1.0604969263076782
outputs_pos.loss : 0.8379311561584473
outputs_pos.loss : 1.1943639516830444
outputs_pos.loss : 0.9717946648597717
Epoch 01245: adjusting learning rate of group 0 to 2.4056e-06.
outputs_pos.loss : 1.4489926099777222
outputs_pos.loss : 1.2697327136993408
outputs_pos.loss : 1.2712455987930298
outputs_pos.loss : 1.4226793050765991
outputs_pos.loss : 1.0168230533599854
outputs_pos.loss : 1.0910389423370361
outputs_pos.loss : 1.0800600051879883
outputs_pos.loss : 0.8594924807548523
Epoch 01246: adjusting learning rate of group 0 to 2.4054e-06.
outputs_pos.loss : 0.9333116412162781
outputs_pos.loss : 0.9349970817565918
outputs_pos.loss : 1.056965947151184
outputs_pos.loss : 1.469103455543518
outputs_pos.loss : 1.7403132915496826
outputs_pos.loss : 1.058239221572876
outputs_pos.loss : 1.0654258728027344
outputs_pos.loss : 1.426700234413147
Epoch 01247: adjusting learning rate of group 0 to 2.4053e-06.
outputs_pos.loss : 1.1317169666290283
outputs_pos.loss : 1.055740237236023
outputs_pos.loss : 1.1518794298171997
outputs_pos.loss : 1.3370846509933472
outputs_pos.loss : 1.1747148036956787
outputs_pos.loss : 0.9260079264640808
outputs_pos.loss : 1.1087491512298584
outputs_pos.loss : 1.2637274265289307
Epoch 01248: adjusting learning rate of group 0 to 2.4051e-06.
outputs_pos.loss : 0.9205582737922668
outputs_pos.loss : 1.1937177181243896
outputs_pos.loss : 1.345197319984436
outputs_pos.loss : 1.589286208152771
outputs_pos.loss : 0.9814855456352234
outputs_pos.loss : 1.0329731702804565
outputs_pos.loss : 0.8668352961540222
outputs_pos.loss : 1.7847543954849243
Epoch 01249: adjusting learning rate of group 0 to 2.4050e-06.
outputs_pos.loss : 1.4111356735229492
outputs_pos.loss : 1.3248891830444336
outputs_pos.loss : 0.878351628780365
outputs_pos.loss : 1.2403255701065063
outputs_pos.loss : 1.5805904865264893
outputs_pos.loss : 1.1755452156066895
outputs_pos.loss : 0.9656683206558228
outputs_pos.loss : 1.1111050844192505
Epoch 01250: adjusting learning rate of group 0 to 2.4048e-06.
outputs_pos.loss : 1.0646419525146484
outputs_pos.loss : 1.5698736906051636
outputs_pos.loss : 1.3488432168960571
outputs_pos.loss : 1.1574007272720337
outputs_pos.loss : 1.3267157077789307
outputs_pos.loss : 0.7818536162376404
outputs_pos.loss : 1.556004524230957
outputs_pos.loss : 1.109346866607666
Epoch 01251: adjusting learning rate of group 0 to 2.4047e-06.
outputs_pos.loss : 0.9168913960456848
outputs_pos.loss : 0.936404824256897
outputs_pos.loss : 1.0545634031295776
outputs_pos.loss : 0.8812187910079956
outputs_pos.loss : 1.5610017776489258
outputs_pos.loss : 1.1126091480255127
outputs_pos.loss : 0.9891011118888855
outputs_pos.loss : 1.4439910650253296
Epoch 01252: adjusting learning rate of group 0 to 2.4045e-06.
outputs_pos.loss : 1.2328113317489624
outputs_pos.loss : 1.0038007497787476
outputs_pos.loss : 1.3073064088821411
outputs_pos.loss : 0.8313443660736084
outputs_pos.loss : 0.926823616027832
outputs_pos.loss : 0.9848845601081848
outputs_pos.loss : 1.0581895112991333
outputs_pos.loss : 1.0652143955230713
Epoch 01253: adjusting learning rate of group 0 to 2.4044e-06.
outputs_pos.loss : 1.2901378870010376
outputs_pos.loss : 1.1541465520858765
outputs_pos.loss : 1.2761582136154175
outputs_pos.loss : 1.0627195835113525
outputs_pos.loss : 0.869890570640564
outputs_pos.loss : 1.2465462684631348
outputs_pos.loss : 1.377386450767517
outputs_pos.loss : 1.2471106052398682
Epoch 01254: adjusting learning rate of group 0 to 2.4042e-06.
outputs_pos.loss : 1.216840147972107
outputs_pos.loss : 1.3350509405136108
outputs_pos.loss : 1.3128184080123901
outputs_pos.loss : 1.046128273010254
outputs_pos.loss : 1.1608425378799438
outputs_pos.loss : 0.8718081116676331
outputs_pos.loss : 1.212501883506775
outputs_pos.loss : 0.9466826915740967
Epoch 01255: adjusting learning rate of group 0 to 2.4041e-06.
outputs_pos.loss : 1.4557888507843018
outputs_pos.loss : 1.0547951459884644
outputs_pos.loss : 1.1826835870742798
outputs_pos.loss : 1.01272451877594
outputs_pos.loss : 1.080557107925415
outputs_pos.loss : 1.0468705892562866
outputs_pos.loss : 1.4751784801483154
outputs_pos.loss : 0.9679518938064575
Epoch 01256: adjusting learning rate of group 0 to 2.4039e-06.
outputs_pos.loss : 1.446058750152588
outputs_pos.loss : 1.0274802446365356
outputs_pos.loss : 0.5406965613365173
outputs_pos.loss : 1.526238203048706
outputs_pos.loss : 1.0884233713150024
outputs_pos.loss : 1.508548378944397
outputs_pos.loss : 1.445178747177124
outputs_pos.loss : 1.0150423049926758
Epoch 01257: adjusting learning rate of group 0 to 2.4038e-06.
outputs_pos.loss : 0.8566617369651794
outputs_pos.loss : 0.9331543445587158
outputs_pos.loss : 1.1050435304641724
outputs_pos.loss : 0.9000861048698425
outputs_pos.loss : 1.1997538805007935
outputs_pos.loss : 1.0646953582763672
outputs_pos.loss : 1.0730483531951904
outputs_pos.loss : 0.9404876232147217
Epoch 01258: adjusting learning rate of group 0 to 2.4036e-06.
outputs_pos.loss : 1.1981772184371948
outputs_pos.loss : 1.2455404996871948
outputs_pos.loss : 1.4696948528289795
outputs_pos.loss : 1.338692545890808
outputs_pos.loss : 0.9739890098571777
outputs_pos.loss : 1.3139971494674683
outputs_pos.loss : 0.9455121755599976
outputs_pos.loss : 1.6190985441207886
Epoch 01259: adjusting learning rate of group 0 to 2.4035e-06.
outputs_pos.loss : 1.328202724456787
outputs_pos.loss : 0.9221517443656921
outputs_pos.loss : 1.431965708732605
outputs_pos.loss : 0.8860656023025513
outputs_pos.loss : 1.0124568939208984
outputs_pos.loss : 1.1075282096862793
outputs_pos.loss : 1.3055164813995361
outputs_pos.loss : 0.8905656933784485
Epoch 01260: adjusting learning rate of group 0 to 2.4033e-06.
outputs_pos.loss : 1.0647727251052856
outputs_pos.loss : 1.4939401149749756
outputs_pos.loss : 1.169704556465149
outputs_pos.loss : 1.1143999099731445
outputs_pos.loss : 1.0584338903427124
outputs_pos.loss : 1.1514629125595093
outputs_pos.loss : 1.1512906551361084
outputs_pos.loss : 0.9147412776947021
Epoch 01261: adjusting learning rate of group 0 to 2.4032e-06.
outputs_pos.loss : 1.6884019374847412
outputs_pos.loss : 1.2615715265274048
outputs_pos.loss : 1.0621870756149292
outputs_pos.loss : 1.2936701774597168
outputs_pos.loss : 1.4734989404678345
outputs_pos.loss : 1.124911904335022
outputs_pos.loss : 1.1515603065490723
outputs_pos.loss : 1.021235704421997
Epoch 01262: adjusting learning rate of group 0 to 2.4030e-06.
outputs_pos.loss : 1.6862494945526123
outputs_pos.loss : 0.792903482913971
outputs_pos.loss : 1.23929762840271
outputs_pos.loss : 0.9047572016716003
outputs_pos.loss : 1.1856391429901123
outputs_pos.loss : 0.9016918540000916
outputs_pos.loss : 1.2688558101654053
outputs_pos.loss : 1.2396233081817627
Epoch 01263: adjusting learning rate of group 0 to 2.4029e-06.
outputs_pos.loss : 0.8718567490577698
outputs_pos.loss : 0.9593575596809387
outputs_pos.loss : 1.4492779970169067
outputs_pos.loss : 1.0841047763824463
outputs_pos.loss : 0.9843757748603821
outputs_pos.loss : 1.0363903045654297
outputs_pos.loss : 1.4782476425170898
outputs_pos.loss : 1.7233208417892456
Epoch 01264: adjusting learning rate of group 0 to 2.4027e-06.
outputs_pos.loss : 0.6866112351417542
outputs_pos.loss : 1.2067043781280518
outputs_pos.loss : 1.224827766418457
outputs_pos.loss : 1.142255187034607
outputs_pos.loss : 1.5196572542190552
outputs_pos.loss : 1.627740740776062
outputs_pos.loss : 1.3414897918701172
outputs_pos.loss : 1.1031298637390137
Epoch 01265: adjusting learning rate of group 0 to 2.4026e-06.
outputs_pos.loss : 0.7619739174842834
outputs_pos.loss : 1.4214062690734863
outputs_pos.loss : 1.173824429512024
outputs_pos.loss : 0.9480130076408386
outputs_pos.loss : 0.914139449596405
outputs_pos.loss : 1.0515971183776855
outputs_pos.loss : 1.0782456398010254
outputs_pos.loss : 1.2957888841629028
Epoch 01266: adjusting learning rate of group 0 to 2.4024e-06.
outputs_pos.loss : 1.196947455406189
outputs_pos.loss : 1.8137120008468628
outputs_pos.loss : 1.2746410369873047
outputs_pos.loss : 1.0088245868682861
outputs_pos.loss : 0.9152917265892029
outputs_pos.loss : 1.25740647315979
outputs_pos.loss : 1.2706573009490967
outputs_pos.loss : 1.405706524848938
Epoch 01267: adjusting learning rate of group 0 to 2.4023e-06.
outputs_pos.loss : 1.076593041419983
outputs_pos.loss : 1.1238598823547363
outputs_pos.loss : 1.1799418926239014
outputs_pos.loss : 1.1071580648422241
outputs_pos.loss : 1.4038797616958618
outputs_pos.loss : 1.8899736404418945
outputs_pos.loss : 1.250539779663086
outputs_pos.loss : 1.1270912885665894
Epoch 01268: adjusting learning rate of group 0 to 2.4021e-06.
outputs_pos.loss : 1.097267746925354
outputs_pos.loss : 1.3561537265777588
outputs_pos.loss : 1.3555638790130615
outputs_pos.loss : 1.0727990865707397
outputs_pos.loss : 1.3946142196655273
outputs_pos.loss : 1.7969681024551392
outputs_pos.loss : 1.1514887809753418
outputs_pos.loss : 1.42988920211792
Epoch 01269: adjusting learning rate of group 0 to 2.4020e-06.
outputs_pos.loss : 1.3755278587341309
outputs_pos.loss : 0.7046129107475281
outputs_pos.loss : 1.13360595703125
outputs_pos.loss : 0.9250199198722839
outputs_pos.loss : 1.4201058149337769
outputs_pos.loss : 1.321952223777771
outputs_pos.loss : 1.0947344303131104
outputs_pos.loss : 1.166587471961975
Epoch 01270: adjusting learning rate of group 0 to 2.4018e-06.
outputs_pos.loss : 0.9807173609733582
outputs_pos.loss : 1.1531909704208374
outputs_pos.loss : 0.9379824995994568
outputs_pos.loss : 0.9377491474151611
outputs_pos.loss : 1.0846017599105835
outputs_pos.loss : 1.026262879371643
outputs_pos.loss : 0.8991249799728394
outputs_pos.loss : 1.0880122184753418
Epoch 01271: adjusting learning rate of group 0 to 2.4017e-06.
outputs_pos.loss : 1.1349470615386963
outputs_pos.loss : 1.0261837244033813
outputs_pos.loss : 1.9084230661392212
outputs_pos.loss : 1.0225821733474731
outputs_pos.loss : 1.3072307109832764
outputs_pos.loss : 1.2320960760116577
outputs_pos.loss : 0.804159939289093
outputs_pos.loss : 0.780121386051178
Epoch 01272: adjusting learning rate of group 0 to 2.4015e-06.
outputs_pos.loss : 1.168133020401001
outputs_pos.loss : 1.0189127922058105
outputs_pos.loss : 0.9540408849716187
outputs_pos.loss : 0.9939388632774353
outputs_pos.loss : 1.0678651332855225
outputs_pos.loss : 1.6032536029815674
outputs_pos.loss : 1.1250990629196167
outputs_pos.loss : 1.1828447580337524
Epoch 01273: adjusting learning rate of group 0 to 2.4014e-06.
outputs_pos.loss : 1.3400777578353882
outputs_pos.loss : 0.6338716745376587
outputs_pos.loss : 0.8879470825195312
outputs_pos.loss : 1.5756725072860718
outputs_pos.loss : 1.0527616739273071
outputs_pos.loss : 0.9190993905067444
outputs_pos.loss : 0.8903635144233704
outputs_pos.loss : 1.6546999216079712
Epoch 01274: adjusting learning rate of group 0 to 2.4012e-06.
outputs_pos.loss : 1.1590391397476196
outputs_pos.loss : 1.3167316913604736
outputs_pos.loss : 1.1477960348129272
outputs_pos.loss : 1.2022905349731445
outputs_pos.loss : 1.25244140625
outputs_pos.loss : 1.0790218114852905
outputs_pos.loss : 0.6723678708076477
outputs_pos.loss : 1.3424017429351807
Epoch 01275: adjusting learning rate of group 0 to 2.4011e-06.
outputs_pos.loss : 1.077371597290039
outputs_pos.loss : 1.3788204193115234
outputs_pos.loss : 1.1386480331420898
outputs_pos.loss : 1.153768539428711
outputs_pos.loss : 1.635611891746521
outputs_pos.loss : 1.2069185972213745
outputs_pos.loss : 1.3192566633224487
outputs_pos.loss : 0.8487172722816467
Epoch 01276: adjusting learning rate of group 0 to 2.4009e-06.
outputs_pos.loss : 0.9047390222549438
outputs_pos.loss : 0.8990445137023926
outputs_pos.loss : 0.8095091581344604
outputs_pos.loss : 1.328428864479065
outputs_pos.loss : 1.393359661102295
outputs_pos.loss : 1.0881246328353882
outputs_pos.loss : 1.1303871870040894
outputs_pos.loss : 1.3175512552261353
Epoch 01277: adjusting learning rate of group 0 to 2.4008e-06.
outputs_pos.loss : 1.1882901191711426
outputs_pos.loss : 0.961305558681488
outputs_pos.loss : 1.2544569969177246
outputs_pos.loss : 0.855307400226593
outputs_pos.loss : 1.00246000289917
outputs_pos.loss : 1.2612062692642212
outputs_pos.loss : 1.185964584350586
outputs_pos.loss : 1.045186996459961
Epoch 01278: adjusting learning rate of group 0 to 2.4006e-06.
outputs_pos.loss : 1.1226205825805664
outputs_pos.loss : 1.0339107513427734
outputs_pos.loss : 0.9695321321487427
outputs_pos.loss : 1.3331098556518555
outputs_pos.loss : 1.3786556720733643
outputs_pos.loss : 1.2629541158676147
outputs_pos.loss : 1.189595103263855
outputs_pos.loss : 0.9630561470985413
Epoch 01279: adjusting learning rate of group 0 to 2.4004e-06.
outputs_pos.loss : 0.8125700950622559
outputs_pos.loss : 1.078842043876648
outputs_pos.loss : 0.8644953966140747
outputs_pos.loss : 0.9622629880905151
outputs_pos.loss : 1.2726960182189941
outputs_pos.loss : 1.0189615488052368
outputs_pos.loss : 1.0746268033981323
outputs_pos.loss : 1.413246512413025
Epoch 01280: adjusting learning rate of group 0 to 2.4003e-06.
outputs_pos.loss : 1.122176170349121
outputs_pos.loss : 0.8882008194923401
outputs_pos.loss : 1.3353374004364014
outputs_pos.loss : 1.4628818035125732
outputs_pos.loss : 1.2841914892196655
outputs_pos.loss : 1.71499764919281
outputs_pos.loss : 0.8608991503715515
outputs_pos.loss : 1.3063377141952515
Epoch 01281: adjusting learning rate of group 0 to 2.4001e-06.
outputs_pos.loss : 1.4333542585372925
outputs_pos.loss : 1.2163143157958984
outputs_pos.loss : 0.9046955108642578
outputs_pos.loss : 0.8341517448425293
outputs_pos.loss : 0.8157052397727966
outputs_pos.loss : 1.007045865058899
outputs_pos.loss : 1.1055980920791626
outputs_pos.loss : 1.051796793937683
Epoch 01282: adjusting learning rate of group 0 to 2.4000e-06.
outputs_pos.loss : 0.9858832359313965
outputs_pos.loss : 1.057530403137207
outputs_pos.loss : 1.057279109954834
outputs_pos.loss : 1.3764994144439697
outputs_pos.loss : 1.3245196342468262
outputs_pos.loss : 1.1386535167694092
outputs_pos.loss : 1.169417381286621
outputs_pos.loss : 0.9694851636886597
Epoch 01283: adjusting learning rate of group 0 to 2.3998e-06.
outputs_pos.loss : 1.186111330986023
outputs_pos.loss : 1.257912516593933
outputs_pos.loss : 1.4931432008743286
outputs_pos.loss : 1.4349288940429688
outputs_pos.loss : 0.941545844078064
outputs_pos.loss : 1.0698870420455933
outputs_pos.loss : 0.858242928981781
outputs_pos.loss : 0.760232150554657
Epoch 01284: adjusting learning rate of group 0 to 2.3997e-06.
outputs_pos.loss : 1.0979955196380615
outputs_pos.loss : 1.1806044578552246
outputs_pos.loss : 1.0260175466537476
outputs_pos.loss : 1.056450605392456
outputs_pos.loss : 0.7776427268981934
outputs_pos.loss : 0.9364900588989258
outputs_pos.loss : 0.979322075843811
outputs_pos.loss : 1.4610415697097778
Epoch 01285: adjusting learning rate of group 0 to 2.3995e-06.
outputs_pos.loss : 0.9404160976409912
outputs_pos.loss : 1.2846571207046509
outputs_pos.loss : 1.1486709117889404
outputs_pos.loss : 1.721420407295227
outputs_pos.loss : 1.399174690246582
outputs_pos.loss : 0.9622519612312317
outputs_pos.loss : 0.790155827999115
outputs_pos.loss : 1.3977630138397217
Epoch 01286: adjusting learning rate of group 0 to 2.3994e-06.
outputs_pos.loss : 1.0223628282546997
outputs_pos.loss : 1.1781139373779297
outputs_pos.loss : 0.9652656316757202
outputs_pos.loss : 0.9546206593513489
outputs_pos.loss : 1.325378656387329
outputs_pos.loss : 0.7304590344429016
outputs_pos.loss : 1.0033713579177856
outputs_pos.loss : 1.0264263153076172
Epoch 01287: adjusting learning rate of group 0 to 2.3992e-06.
outputs_pos.loss : 1.1834361553192139
outputs_pos.loss : 1.038820505142212
outputs_pos.loss : 1.1687217950820923
outputs_pos.loss : 1.2607336044311523
outputs_pos.loss : 1.2251535654067993
outputs_pos.loss : 0.9919346570968628
outputs_pos.loss : 1.5234503746032715
outputs_pos.loss : 1.28056800365448
Epoch 01288: adjusting learning rate of group 0 to 2.3991e-06.
outputs_pos.loss : 1.1131672859191895
outputs_pos.loss : 1.1828831434249878
outputs_pos.loss : 1.1164499521255493
outputs_pos.loss : 0.8281787037849426
outputs_pos.loss : 1.3661260604858398
outputs_pos.loss : 0.8661966919898987
outputs_pos.loss : 1.2880350351333618
outputs_pos.loss : 0.5356948375701904
Epoch 01289: adjusting learning rate of group 0 to 2.3989e-06.
outputs_pos.loss : 0.818705141544342
outputs_pos.loss : 1.1501370668411255
outputs_pos.loss : 1.2034761905670166
outputs_pos.loss : 1.0521975755691528
outputs_pos.loss : 1.0869477987289429
outputs_pos.loss : 1.2267764806747437
outputs_pos.loss : 1.284986138343811
outputs_pos.loss : 1.5335408449172974
Epoch 01290: adjusting learning rate of group 0 to 2.3987e-06.
outputs_pos.loss : 1.6421709060668945
outputs_pos.loss : 1.832582950592041
outputs_pos.loss : 1.1365418434143066
outputs_pos.loss : 1.0726200342178345
outputs_pos.loss : 1.2783104181289673
outputs_pos.loss : 1.4078526496887207
outputs_pos.loss : 1.2205183506011963
outputs_pos.loss : 1.4272615909576416
Epoch 01291: adjusting learning rate of group 0 to 2.3986e-06.
outputs_pos.loss : 0.9523499011993408
outputs_pos.loss : 1.1162887811660767
outputs_pos.loss : 1.1050395965576172
outputs_pos.loss : 0.7938836812973022
outputs_pos.loss : 1.2544955015182495
outputs_pos.loss : 0.9618943929672241
outputs_pos.loss : 1.2594976425170898
outputs_pos.loss : 1.0869061946868896
Epoch 01292: adjusting learning rate of group 0 to 2.3984e-06.
outputs_pos.loss : 1.0464191436767578
outputs_pos.loss : 0.8623886108398438
outputs_pos.loss : 1.239621877670288
outputs_pos.loss : 0.9781557321548462
outputs_pos.loss : 1.3602508306503296
outputs_pos.loss : 1.023550271987915
outputs_pos.loss : 1.7683522701263428
outputs_pos.loss : 1.048398733139038
Epoch 01293: adjusting learning rate of group 0 to 2.3983e-06.
outputs_pos.loss : 1.183457851409912
outputs_pos.loss : 1.0986878871917725
outputs_pos.loss : 1.181575059890747
outputs_pos.loss : 0.7232710123062134
outputs_pos.loss : 0.84820556640625
outputs_pos.loss : 1.333149790763855
outputs_pos.loss : 0.9675179719924927
outputs_pos.loss : 1.1128498315811157
Epoch 01294: adjusting learning rate of group 0 to 2.3981e-06.
outputs_pos.loss : 1.0977638959884644
outputs_pos.loss : 1.0305871963500977
outputs_pos.loss : 1.3410724401474
outputs_pos.loss : 1.3442622423171997
outputs_pos.loss : 1.1318289041519165
outputs_pos.loss : 1.344135046005249
outputs_pos.loss : 1.0616635084152222
outputs_pos.loss : 1.435651183128357
Epoch 01295: adjusting learning rate of group 0 to 2.3980e-06.
outputs_pos.loss : 1.2806923389434814
outputs_pos.loss : 1.3105509281158447
outputs_pos.loss : 1.075782060623169
outputs_pos.loss : 1.2194284200668335
outputs_pos.loss : 1.088281512260437
outputs_pos.loss : 0.6290156245231628
outputs_pos.loss : 1.0427331924438477
outputs_pos.loss : 0.9243392944335938
Epoch 01296: adjusting learning rate of group 0 to 2.3978e-06.
outputs_pos.loss : 0.8899976015090942
outputs_pos.loss : 0.9625499248504639
outputs_pos.loss : 1.3431025743484497
outputs_pos.loss : 1.7890641689300537
outputs_pos.loss : 0.44016605615615845
outputs_pos.loss : 1.1481456756591797
outputs_pos.loss : 0.9266897439956665
outputs_pos.loss : 1.0866657495498657
Epoch 01297: adjusting learning rate of group 0 to 2.3977e-06.
outputs_pos.loss : 1.1376038789749146
outputs_pos.loss : 1.035923719406128
outputs_pos.loss : 1.0256644487380981
outputs_pos.loss : 1.6746076345443726
outputs_pos.loss : 0.7319157719612122
outputs_pos.loss : 1.124770164489746
outputs_pos.loss : 1.0844852924346924
outputs_pos.loss : 1.0813202857971191
Epoch 01298: adjusting learning rate of group 0 to 2.3975e-06.
outputs_pos.loss : 1.3199814558029175
outputs_pos.loss : 0.9282122254371643
outputs_pos.loss : 0.8809590935707092
outputs_pos.loss : 0.7269368171691895
outputs_pos.loss : 1.786575198173523
outputs_pos.loss : 0.9180169105529785
outputs_pos.loss : 1.6781935691833496
outputs_pos.loss : 0.9497147798538208
Epoch 01299: adjusting learning rate of group 0 to 2.3973e-06.
outputs_pos.loss : 1.2778525352478027
outputs_pos.loss : 0.9047504663467407
outputs_pos.loss : 1.167008399963379
outputs_pos.loss : 1.0665801763534546
outputs_pos.loss : 1.1587406396865845
outputs_pos.loss : 1.3677359819412231
outputs_pos.loss : 1.3630763292312622
outputs_pos.loss : 1.6210750341415405
Epoch 01300: adjusting learning rate of group 0 to 2.3972e-06.
outputs_pos.loss : 1.0719029903411865
outputs_pos.loss : 0.8442606329917908
outputs_pos.loss : 1.1058429479599
outputs_pos.loss : 1.0820014476776123
outputs_pos.loss : 0.8852445483207703
outputs_pos.loss : 1.1534366607666016
outputs_pos.loss : 1.0968348979949951
outputs_pos.loss : 1.0062763690948486
Epoch 01301: adjusting learning rate of group 0 to 2.3970e-06.
outputs_pos.loss : 1.061706304550171
outputs_pos.loss : 1.3488868474960327
outputs_pos.loss : 1.140452265739441
outputs_pos.loss : 1.0256596803665161
outputs_pos.loss : 1.1399368047714233
outputs_pos.loss : 0.9857425689697266
outputs_pos.loss : 1.0758384466171265
outputs_pos.loss : 1.4764466285705566
Epoch 01302: adjusting learning rate of group 0 to 2.3969e-06.
outputs_pos.loss : 0.7741182446479797
outputs_pos.loss : 1.278356671333313
outputs_pos.loss : 1.0465139150619507
outputs_pos.loss : 0.944180965423584
outputs_pos.loss : 1.3796930313110352
outputs_pos.loss : 1.037286400794983
outputs_pos.loss : 1.1834779977798462
outputs_pos.loss : 1.134791612625122
Epoch 01303: adjusting learning rate of group 0 to 2.3967e-06.
outputs_pos.loss : 0.9241743087768555
outputs_pos.loss : 1.622255802154541
outputs_pos.loss : 0.5858123302459717
outputs_pos.loss : 1.1824781894683838
outputs_pos.loss : 0.9002506136894226
outputs_pos.loss : 0.8927386403083801
outputs_pos.loss : 1.1061488389968872
outputs_pos.loss : 0.7730860114097595
Epoch 01304: adjusting learning rate of group 0 to 2.3966e-06.
outputs_pos.loss : 0.9505355358123779
outputs_pos.loss : 1.0013389587402344
outputs_pos.loss : 1.1845757961273193
outputs_pos.loss : 0.9190823435783386
outputs_pos.loss : 0.9596386551856995
outputs_pos.loss : 1.4407908916473389
outputs_pos.loss : 0.8212437629699707
outputs_pos.loss : 0.7614600658416748
Epoch 01305: adjusting learning rate of group 0 to 2.3964e-06.
outputs_pos.loss : 1.0152908563613892
outputs_pos.loss : 1.2874788045883179
outputs_pos.loss : 1.1397215127944946
outputs_pos.loss : 1.1697289943695068
outputs_pos.loss : 1.124123454093933
outputs_pos.loss : 1.00394606590271
outputs_pos.loss : 0.9587234854698181
outputs_pos.loss : 1.2920124530792236
Epoch 01306: adjusting learning rate of group 0 to 2.3963e-06.
outputs_pos.loss : 1.3567548990249634
outputs_pos.loss : 0.8791921734809875
outputs_pos.loss : 1.0330960750579834
outputs_pos.loss : 1.014553189277649
outputs_pos.loss : 1.6337708234786987
outputs_pos.loss : 2.203097105026245
outputs_pos.loss : 1.2130876779556274
outputs_pos.loss : 1.4774632453918457
Epoch 01307: adjusting learning rate of group 0 to 2.3961e-06.
outputs_pos.loss : 1.9359265565872192
outputs_pos.loss : 1.3063641786575317
outputs_pos.loss : 0.8775163888931274
outputs_pos.loss : 1.5220277309417725
outputs_pos.loss : 1.5063945055007935
outputs_pos.loss : 0.7692648768424988
outputs_pos.loss : 1.1076457500457764
outputs_pos.loss : 0.8430745601654053
Epoch 01308: adjusting learning rate of group 0 to 2.3959e-06.
outputs_pos.loss : 1.405595064163208
outputs_pos.loss : 1.1421507596969604
outputs_pos.loss : 1.4620604515075684
outputs_pos.loss : 1.2300094366073608
outputs_pos.loss : 0.5993819236755371
outputs_pos.loss : 1.5785603523254395
outputs_pos.loss : 1.3075659275054932
outputs_pos.loss : 1.1210546493530273
Epoch 01309: adjusting learning rate of group 0 to 2.3958e-06.
outputs_pos.loss : 1.1878741979599
outputs_pos.loss : 0.4652029871940613
outputs_pos.loss : 1.0345572233200073
outputs_pos.loss : 1.0503461360931396
outputs_pos.loss : 0.9353294372558594
outputs_pos.loss : 1.3579167127609253
outputs_pos.loss : 0.918574869632721
outputs_pos.loss : 1.0721312761306763
Epoch 01310: adjusting learning rate of group 0 to 2.3956e-06.
outputs_pos.loss : 1.295892596244812
outputs_pos.loss : 0.916776716709137
outputs_pos.loss : 0.9942497611045837
outputs_pos.loss : 1.0856684446334839
outputs_pos.loss : 1.2635804414749146
outputs_pos.loss : 1.2069783210754395
outputs_pos.loss : 1.6554569005966187
outputs_pos.loss : 1.2090495824813843
Epoch 01311: adjusting learning rate of group 0 to 2.3955e-06.
outputs_pos.loss : 0.5994901657104492
outputs_pos.loss : 1.0950847864151
outputs_pos.loss : 1.1223405599594116
outputs_pos.loss : 1.359948754310608
outputs_pos.loss : 1.114678978919983
outputs_pos.loss : 0.8769263029098511
outputs_pos.loss : 0.9019793272018433
outputs_pos.loss : 1.0186939239501953
Epoch 01312: adjusting learning rate of group 0 to 2.3953e-06.
outputs_pos.loss : 1.0802661180496216
outputs_pos.loss : 1.399125099182129
outputs_pos.loss : 1.6722464561462402
outputs_pos.loss : 1.137553095817566
outputs_pos.loss : 0.985805869102478
outputs_pos.loss : 1.3604052066802979
outputs_pos.loss : 0.9983205795288086
outputs_pos.loss : 0.893298327922821
Epoch 01313: adjusting learning rate of group 0 to 2.3952e-06.
outputs_pos.loss : 1.0617802143096924
outputs_pos.loss : 1.3502161502838135
outputs_pos.loss : 0.705302357673645
outputs_pos.loss : 1.2074202299118042
outputs_pos.loss : 0.7368786931037903
outputs_pos.loss : 1.9618213176727295
outputs_pos.loss : 1.1901650428771973
outputs_pos.loss : 0.8270930647850037
Epoch 01314: adjusting learning rate of group 0 to 2.3950e-06.
outputs_pos.loss : 1.5226322412490845
outputs_pos.loss : 1.1764237880706787
outputs_pos.loss : 1.0574945211410522
outputs_pos.loss : 0.9887776374816895
outputs_pos.loss : 0.7956852912902832
outputs_pos.loss : 1.4890433549880981
outputs_pos.loss : 1.0101194381713867
outputs_pos.loss : 1.1161261796951294
Epoch 01315: adjusting learning rate of group 0 to 2.3948e-06.
outputs_pos.loss : 1.0605262517929077
outputs_pos.loss : 1.3620415925979614
outputs_pos.loss : 1.3591694831848145
outputs_pos.loss : 1.3551901578903198
outputs_pos.loss : 1.0416496992111206
outputs_pos.loss : 1.552986741065979
outputs_pos.loss : 1.2050477266311646
outputs_pos.loss : 1.1416852474212646
Epoch 01316: adjusting learning rate of group 0 to 2.3947e-06.
outputs_pos.loss : 0.7838562726974487
outputs_pos.loss : 1.0939812660217285
outputs_pos.loss : 1.6533232927322388
outputs_pos.loss : 0.8789995312690735
outputs_pos.loss : 1.6673153638839722
outputs_pos.loss : 0.698947548866272
outputs_pos.loss : 1.3436402082443237
outputs_pos.loss : 1.2068794965744019
Epoch 01317: adjusting learning rate of group 0 to 2.3945e-06.
outputs_pos.loss : 1.0926806926727295
outputs_pos.loss : 0.9608970880508423
outputs_pos.loss : 1.1935230493545532
outputs_pos.loss : 0.9271022081375122
outputs_pos.loss : 1.427079677581787
outputs_pos.loss : 1.1555570363998413
outputs_pos.loss : 1.4616190195083618
outputs_pos.loss : 0.9278927445411682
Epoch 01318: adjusting learning rate of group 0 to 2.3944e-06.
outputs_pos.loss : 1.6584240198135376
outputs_pos.loss : 0.8357448577880859
outputs_pos.loss : 1.0047279596328735
outputs_pos.loss : 0.9897692799568176
outputs_pos.loss : 1.264884352684021
outputs_pos.loss : 1.4206159114837646
outputs_pos.loss : 1.2252888679504395
outputs_pos.loss : 1.0644139051437378
Epoch 01319: adjusting learning rate of group 0 to 2.3942e-06.
outputs_pos.loss : 1.297072172164917
outputs_pos.loss : 1.1066449880599976
outputs_pos.loss : 1.038295865058899
outputs_pos.loss : 0.6414304375648499
outputs_pos.loss : 1.1181737184524536
outputs_pos.loss : 0.9219702482223511
outputs_pos.loss : 0.9806367754936218
outputs_pos.loss : 1.3534753322601318
Epoch 01320: adjusting learning rate of group 0 to 2.3941e-06.
outputs_pos.loss : 1.2138129472732544
outputs_pos.loss : 1.3437000513076782
outputs_pos.loss : 0.8471139669418335
outputs_pos.loss : 1.2240256071090698
outputs_pos.loss : 1.0412086248397827
outputs_pos.loss : 1.4591667652130127
outputs_pos.loss : 0.8112001419067383
outputs_pos.loss : 1.3706388473510742
Epoch 01321: adjusting learning rate of group 0 to 2.3939e-06.
outputs_pos.loss : 1.4830141067504883
outputs_pos.loss : 1.1146507263183594
outputs_pos.loss : 0.5388904213905334
outputs_pos.loss : 0.8788822889328003
outputs_pos.loss : 1.3809508085250854
outputs_pos.loss : 0.9664823412895203
outputs_pos.loss : 1.3753464221954346
outputs_pos.loss : 1.1262857913970947
Epoch 01322: adjusting learning rate of group 0 to 2.3937e-06.
outputs_pos.loss : 1.142236590385437
outputs_pos.loss : 1.1600489616394043
outputs_pos.loss : 1.6249144077301025
outputs_pos.loss : 1.05133855342865
outputs_pos.loss : 1.1167880296707153
outputs_pos.loss : 0.7429529428482056
outputs_pos.loss : 1.9715440273284912
outputs_pos.loss : 1.1775400638580322
Epoch 01323: adjusting learning rate of group 0 to 2.3936e-06.
outputs_pos.loss : 1.7649831771850586
outputs_pos.loss : 1.0397381782531738
outputs_pos.loss : 0.8225042223930359
outputs_pos.loss : 1.2957135438919067
outputs_pos.loss : 1.2881311178207397
outputs_pos.loss : 1.1506872177124023
outputs_pos.loss : 1.0985565185546875
outputs_pos.loss : 0.9860444068908691
Epoch 01324: adjusting learning rate of group 0 to 2.3934e-06.
outputs_pos.loss : 1.0171682834625244
outputs_pos.loss : 1.2857284545898438
outputs_pos.loss : 1.5849250555038452
outputs_pos.loss : 1.3957723379135132
outputs_pos.loss : 1.8861393928527832
outputs_pos.loss : 0.8226258754730225
outputs_pos.loss : 1.1269450187683105
outputs_pos.loss : 1.028024673461914
Epoch 01325: adjusting learning rate of group 0 to 2.3933e-06.
outputs_pos.loss : 1.7145838737487793
outputs_pos.loss : 0.9421737194061279
outputs_pos.loss : 1.155135154724121
outputs_pos.loss : 1.1592789888381958
outputs_pos.loss : 0.8040336966514587
outputs_pos.loss : 1.39541494846344
outputs_pos.loss : 1.5483416318893433
outputs_pos.loss : 1.675196647644043
Epoch 01326: adjusting learning rate of group 0 to 2.3931e-06.
outputs_pos.loss : 1.1462461948394775
outputs_pos.loss : 1.0757205486297607
outputs_pos.loss : 1.1571353673934937
outputs_pos.loss : 1.1462692022323608
outputs_pos.loss : 1.0548866987228394
outputs_pos.loss : 1.5557845830917358
outputs_pos.loss : 0.9571449160575867
outputs_pos.loss : 1.209276795387268
Epoch 01327: adjusting learning rate of group 0 to 2.3929e-06.
outputs_pos.loss : 1.464023470878601
outputs_pos.loss : 0.8581740260124207
outputs_pos.loss : 0.8201329112052917
outputs_pos.loss : 1.3060085773468018
outputs_pos.loss : 0.8752769231796265
outputs_pos.loss : 0.8991435170173645
outputs_pos.loss : 1.0089747905731201
outputs_pos.loss : 1.3210699558258057
Epoch 01328: adjusting learning rate of group 0 to 2.3928e-06.
outputs_pos.loss : 1.926893711090088
outputs_pos.loss : 1.190435767173767
outputs_pos.loss : 0.9275031089782715
outputs_pos.loss : 1.3353952169418335
outputs_pos.loss : 1.5584030151367188
outputs_pos.loss : 1.3068870306015015
outputs_pos.loss : 1.1616911888122559
outputs_pos.loss : 2.0594208240509033
Epoch 01329: adjusting learning rate of group 0 to 2.3926e-06.
outputs_pos.loss : 1.3793652057647705
outputs_pos.loss : 0.6076931953430176
outputs_pos.loss : 0.7710520625114441
outputs_pos.loss : 1.0148190259933472
outputs_pos.loss : 0.9832375049591064
outputs_pos.loss : 1.0065656900405884
outputs_pos.loss : 1.292498230934143
outputs_pos.loss : 0.9614323973655701
Epoch 01330: adjusting learning rate of group 0 to 2.3925e-06.
outputs_pos.loss : 1.3465832471847534
outputs_pos.loss : 0.9087902307510376
outputs_pos.loss : 1.0783312320709229
outputs_pos.loss : 0.7363030910491943
outputs_pos.loss : 1.0485767126083374
outputs_pos.loss : 0.9569538235664368
outputs_pos.loss : 1.1564271450042725
outputs_pos.loss : 0.97152179479599
Epoch 01331: adjusting learning rate of group 0 to 2.3923e-06.
outputs_pos.loss : 1.2564911842346191
outputs_pos.loss : 0.968919038772583
outputs_pos.loss : 1.4346195459365845
outputs_pos.loss : 1.3206701278686523
outputs_pos.loss : 0.9286924004554749
outputs_pos.loss : 1.3630691766738892
outputs_pos.loss : 1.065667986869812
outputs_pos.loss : 1.0129284858703613
Epoch 01332: adjusting learning rate of group 0 to 2.3921e-06.
outputs_pos.loss : 0.8357611894607544
outputs_pos.loss : 1.3153979778289795
outputs_pos.loss : 1.0318089723587036
outputs_pos.loss : 0.8489784598350525
outputs_pos.loss : 1.097679615020752
outputs_pos.loss : 0.9709908366203308
outputs_pos.loss : 1.3086742162704468
outputs_pos.loss : 1.25338876247406
Epoch 01333: adjusting learning rate of group 0 to 2.3920e-06.
outputs_pos.loss : 1.3401480913162231
outputs_pos.loss : 1.1126892566680908
outputs_pos.loss : 1.2797492742538452
outputs_pos.loss : 1.6630871295928955
outputs_pos.loss : 1.5766857862472534
outputs_pos.loss : 0.7328521609306335
outputs_pos.loss : 1.0020747184753418
outputs_pos.loss : 0.9688013792037964
Epoch 01334: adjusting learning rate of group 0 to 2.3918e-06.
outputs_pos.loss : 0.8442319631576538
outputs_pos.loss : 1.4556314945220947
outputs_pos.loss : 1.4900912046432495
outputs_pos.loss : 1.1533136367797852
outputs_pos.loss : 0.8489127159118652
outputs_pos.loss : 0.9335787296295166
outputs_pos.loss : 0.7657086253166199
outputs_pos.loss : 1.0510796308517456
Epoch 01335: adjusting learning rate of group 0 to 2.3917e-06.
outputs_pos.loss : 1.7862378358840942
outputs_pos.loss : 1.0729964971542358
outputs_pos.loss : 1.824720859527588
outputs_pos.loss : 1.0644299983978271
outputs_pos.loss : 1.0830923318862915
outputs_pos.loss : 0.9804138541221619
outputs_pos.loss : 1.1047989130020142
outputs_pos.loss : 1.0082467794418335
Epoch 01336: adjusting learning rate of group 0 to 2.3915e-06.
outputs_pos.loss : 0.9502732753753662
outputs_pos.loss : 0.7805548310279846
outputs_pos.loss : 1.796844482421875
outputs_pos.loss : 0.8730403184890747
outputs_pos.loss : 0.9680398106575012
outputs_pos.loss : 1.3609391450881958
outputs_pos.loss : 1.2399983406066895
outputs_pos.loss : 0.823993444442749
Epoch 01337: adjusting learning rate of group 0 to 2.3913e-06.
outputs_pos.loss : 1.451145052909851
outputs_pos.loss : 1.0794283151626587
outputs_pos.loss : 1.217956304550171
outputs_pos.loss : 1.3669633865356445
outputs_pos.loss : 1.2853832244873047
outputs_pos.loss : 1.0208690166473389
outputs_pos.loss : 1.0846196413040161
outputs_pos.loss : 0.9664955139160156
Epoch 01338: adjusting learning rate of group 0 to 2.3912e-06.
outputs_pos.loss : 1.0931693315505981
outputs_pos.loss : 1.1819417476654053
outputs_pos.loss : 0.9597829580307007
outputs_pos.loss : 1.141600251197815
outputs_pos.loss : 0.827707052230835
outputs_pos.loss : 0.8549705743789673
outputs_pos.loss : 0.6009163856506348
outputs_pos.loss : 1.0854157209396362
Epoch 01339: adjusting learning rate of group 0 to 2.3910e-06.
outputs_pos.loss : 0.8361090421676636
outputs_pos.loss : 1.4348875284194946
outputs_pos.loss : 0.7331885099411011
outputs_pos.loss : 1.286937952041626
outputs_pos.loss : 1.3606534004211426
outputs_pos.loss : 1.388116717338562
outputs_pos.loss : 1.296804428100586
outputs_pos.loss : 1.2356935739517212
Epoch 01340: adjusting learning rate of group 0 to 2.3909e-06.
outputs_pos.loss : 1.3040794134140015
outputs_pos.loss : 1.3406145572662354
outputs_pos.loss : 1.3418219089508057
outputs_pos.loss : 1.2781060934066772
outputs_pos.loss : 1.236251711845398
outputs_pos.loss : 1.0461221933364868
outputs_pos.loss : 1.0681589841842651
outputs_pos.loss : 1.0955427885055542
Epoch 01341: adjusting learning rate of group 0 to 2.3907e-06.
outputs_pos.loss : 1.2880523204803467
outputs_pos.loss : 1.1808695793151855
outputs_pos.loss : 0.7393066883087158
outputs_pos.loss : 1.0280874967575073
outputs_pos.loss : 1.6356621980667114
outputs_pos.loss : 1.420250654220581
outputs_pos.loss : 1.5626280307769775
outputs_pos.loss : 1.0641379356384277
Epoch 01342: adjusting learning rate of group 0 to 2.3905e-06.
outputs_pos.loss : 0.9523766040802002
outputs_pos.loss : 0.784439742565155
outputs_pos.loss : 0.7866412997245789
outputs_pos.loss : 1.3183046579360962
outputs_pos.loss : 1.1022783517837524
outputs_pos.loss : 1.2762119770050049
outputs_pos.loss : 1.0380842685699463
outputs_pos.loss : 1.1304651498794556
Epoch 01343: adjusting learning rate of group 0 to 2.3904e-06.
outputs_pos.loss : 0.958008348941803
outputs_pos.loss : 1.6502158641815186
outputs_pos.loss : 0.8793227672576904
outputs_pos.loss : 1.1177126169204712
outputs_pos.loss : 1.298374056816101
outputs_pos.loss : 1.4883699417114258
outputs_pos.loss : 0.8540690541267395
outputs_pos.loss : 0.9838325381278992
Epoch 01344: adjusting learning rate of group 0 to 2.3902e-06.
outputs_pos.loss : 1.1489723920822144
outputs_pos.loss : 0.8438217639923096
outputs_pos.loss : 1.5597797632217407
outputs_pos.loss : 0.9932218194007874
outputs_pos.loss : 1.1388216018676758
outputs_pos.loss : 1.0612841844558716
outputs_pos.loss : 0.8177626132965088
outputs_pos.loss : 1.1246100664138794
Epoch 01345: adjusting learning rate of group 0 to 2.3901e-06.
outputs_pos.loss : 1.4087707996368408
outputs_pos.loss : 0.9172093868255615
outputs_pos.loss : 0.6004613637924194
outputs_pos.loss : 1.2303870916366577
outputs_pos.loss : 1.1340981721878052
outputs_pos.loss : 1.219154953956604
outputs_pos.loss : 1.7361177206039429
outputs_pos.loss : 1.2504976987838745
Epoch 01346: adjusting learning rate of group 0 to 2.3899e-06.
outputs_pos.loss : 1.175483226776123
outputs_pos.loss : 1.085328221321106
outputs_pos.loss : 1.7051228284835815
outputs_pos.loss : 0.9048798084259033
outputs_pos.loss : 1.0029261112213135
outputs_pos.loss : 1.0423970222473145
outputs_pos.loss : 0.8981521129608154
outputs_pos.loss : 1.3253754377365112
Epoch 01347: adjusting learning rate of group 0 to 2.3897e-06.
outputs_pos.loss : 1.4668155908584595
outputs_pos.loss : 1.6246899366378784
outputs_pos.loss : 0.8410370945930481
outputs_pos.loss : 1.125324010848999
outputs_pos.loss : 0.8104097247123718
outputs_pos.loss : 1.43895423412323
outputs_pos.loss : 1.5823171138763428
outputs_pos.loss : 0.8379740118980408
Epoch 01348: adjusting learning rate of group 0 to 2.3896e-06.
outputs_pos.loss : 1.689810872077942
outputs_pos.loss : 1.9318135976791382
outputs_pos.loss : 1.7471935749053955
outputs_pos.loss : 1.7582364082336426
outputs_pos.loss : 1.4335824251174927
outputs_pos.loss : 1.1623698472976685
outputs_pos.loss : 0.8708381652832031
outputs_pos.loss : 1.168494462966919
Epoch 01349: adjusting learning rate of group 0 to 2.3894e-06.
outputs_pos.loss : 1.4682416915893555
outputs_pos.loss : 1.4071894884109497
outputs_pos.loss : 1.1914721727371216
outputs_pos.loss : 1.0875717401504517
outputs_pos.loss : 1.336342215538025
outputs_pos.loss : 1.1261640787124634
outputs_pos.loss : 1.2064543962478638
outputs_pos.loss : 1.6877251863479614
Epoch 01350: adjusting learning rate of group 0 to 2.3893e-06.
outputs_pos.loss : 0.8472241163253784
outputs_pos.loss : 1.3815175294876099
outputs_pos.loss : 0.8577836751937866
outputs_pos.loss : 0.9402124285697937
outputs_pos.loss : 1.3233052492141724
outputs_pos.loss : 1.1056636571884155
outputs_pos.loss : 1.2444541454315186
outputs_pos.loss : 0.8874720931053162
Epoch 01351: adjusting learning rate of group 0 to 2.3891e-06.
outputs_pos.loss : 0.9069497585296631
outputs_pos.loss : 1.5293906927108765
outputs_pos.loss : 1.3330851793289185
outputs_pos.loss : 0.8415965437889099
outputs_pos.loss : 1.096925973892212
outputs_pos.loss : 1.131883978843689
outputs_pos.loss : 0.683753490447998
outputs_pos.loss : 0.9536525011062622
Epoch 01352: adjusting learning rate of group 0 to 2.3889e-06.
outputs_pos.loss : 1.164631962776184
outputs_pos.loss : 1.1272937059402466
outputs_pos.loss : 0.7831632494926453
outputs_pos.loss : 1.5656052827835083
outputs_pos.loss : 0.953521192073822
outputs_pos.loss : 1.4170926809310913
outputs_pos.loss : 0.9490394592285156
outputs_pos.loss : 0.6643226146697998
Epoch 01353: adjusting learning rate of group 0 to 2.3888e-06.
outputs_pos.loss : 1.25054931640625
outputs_pos.loss : 1.3157776594161987
outputs_pos.loss : 1.117919921875
outputs_pos.loss : 0.8142989873886108
outputs_pos.loss : 1.2125511169433594
outputs_pos.loss : 1.3176084756851196
outputs_pos.loss : 1.0119317770004272
outputs_pos.loss : 1.1938399076461792
Epoch 01354: adjusting learning rate of group 0 to 2.3886e-06.
outputs_pos.loss : 0.9994738101959229
outputs_pos.loss : 1.073501706123352
outputs_pos.loss : 1.2581878900527954
outputs_pos.loss : 1.0407873392105103
outputs_pos.loss : 1.260165810585022
outputs_pos.loss : 1.6582573652267456
outputs_pos.loss : 1.2161251306533813
outputs_pos.loss : 1.7799766063690186
Epoch 01355: adjusting learning rate of group 0 to 2.3884e-06.
outputs_pos.loss : 1.0335355997085571
outputs_pos.loss : 1.133872628211975
outputs_pos.loss : 1.02921724319458
outputs_pos.loss : 1.1036796569824219
outputs_pos.loss : 1.3476150035858154
outputs_pos.loss : 0.9985968470573425
outputs_pos.loss : 1.1710478067398071
outputs_pos.loss : 0.9883991479873657
Epoch 01356: adjusting learning rate of group 0 to 2.3883e-06.
outputs_pos.loss : 0.8688730597496033
outputs_pos.loss : 1.2113090753555298
outputs_pos.loss : 0.722555935382843
outputs_pos.loss : 1.032161831855774
outputs_pos.loss : 1.555816411972046
outputs_pos.loss : 1.084594488143921
outputs_pos.loss : 1.046392798423767
outputs_pos.loss : 1.0147486925125122
Epoch 01357: adjusting learning rate of group 0 to 2.3881e-06.
outputs_pos.loss : 1.064686894416809
outputs_pos.loss : 1.029866099357605
outputs_pos.loss : 0.9927121996879578
outputs_pos.loss : 1.423712968826294
outputs_pos.loss : 1.3106080293655396
outputs_pos.loss : 1.1239651441574097
outputs_pos.loss : 0.9872236847877502
outputs_pos.loss : 1.4954466819763184
Epoch 01358: adjusting learning rate of group 0 to 2.3880e-06.
outputs_pos.loss : 1.1090701818466187
outputs_pos.loss : 0.9546636343002319
outputs_pos.loss : 0.8124204874038696
outputs_pos.loss : 0.972985029220581
outputs_pos.loss : 1.2414870262145996
outputs_pos.loss : 1.1144033670425415
outputs_pos.loss : 0.8030221462249756
outputs_pos.loss : 1.2671188116073608
Epoch 01359: adjusting learning rate of group 0 to 2.3878e-06.
outputs_pos.loss : 1.1503221988677979
outputs_pos.loss : 1.209637999534607
outputs_pos.loss : 1.069503664970398
outputs_pos.loss : 1.3216345310211182
outputs_pos.loss : 2.039954900741577
outputs_pos.loss : 0.8802741765975952
outputs_pos.loss : 1.9681953191757202
outputs_pos.loss : 1.2809967994689941
Epoch 01360: adjusting learning rate of group 0 to 2.3876e-06.
outputs_pos.loss : 1.377893328666687
outputs_pos.loss : 1.1826348304748535
outputs_pos.loss : 0.63913494348526
outputs_pos.loss : 2.500797986984253
outputs_pos.loss : 1.4660683870315552
outputs_pos.loss : 0.7989130020141602
outputs_pos.loss : 1.049353003501892
outputs_pos.loss : 1.4637423753738403
Epoch 01361: adjusting learning rate of group 0 to 2.3875e-06.
outputs_pos.loss : 0.8113322257995605
outputs_pos.loss : 0.8879567384719849
outputs_pos.loss : 1.9132722616195679
outputs_pos.loss : 1.1713294982910156
outputs_pos.loss : 1.5252258777618408
outputs_pos.loss : 1.2179323434829712
outputs_pos.loss : 1.9894018173217773
outputs_pos.loss : 1.1383129358291626
Epoch 01362: adjusting learning rate of group 0 to 2.3873e-06.
outputs_pos.loss : 1.1603419780731201
outputs_pos.loss : 1.18119215965271
outputs_pos.loss : 1.543828010559082
outputs_pos.loss : 0.9773273468017578
outputs_pos.loss : 1.086390733718872
outputs_pos.loss : 1.251365303993225
outputs_pos.loss : 0.8463853001594543
outputs_pos.loss : 1.0336899757385254
Epoch 01363: adjusting learning rate of group 0 to 2.3871e-06.
outputs_pos.loss : 0.7869229912757874
outputs_pos.loss : 1.1159083843231201
outputs_pos.loss : 1.4721015691757202
outputs_pos.loss : 1.1645712852478027
outputs_pos.loss : 1.2375437021255493
outputs_pos.loss : 1.4013192653656006
outputs_pos.loss : 1.361464262008667
outputs_pos.loss : 1.6445040702819824
Epoch 01364: adjusting learning rate of group 0 to 2.3870e-06.
outputs_pos.loss : 1.1632815599441528
outputs_pos.loss : 1.153785228729248
outputs_pos.loss : 0.7101114988327026
outputs_pos.loss : 1.3689526319503784
outputs_pos.loss : 1.1067843437194824
outputs_pos.loss : 0.8179726600646973
outputs_pos.loss : 1.0199801921844482
outputs_pos.loss : 1.0336167812347412
Epoch 01365: adjusting learning rate of group 0 to 2.3868e-06.
outputs_pos.loss : 1.034346103668213
outputs_pos.loss : 1.2934887409210205
outputs_pos.loss : 1.125159740447998
outputs_pos.loss : 1.334639310836792
outputs_pos.loss : 1.3322116136550903
outputs_pos.loss : 0.9207772612571716
outputs_pos.loss : 0.8955375552177429
outputs_pos.loss : 0.8546706438064575
Epoch 01366: adjusting learning rate of group 0 to 2.3867e-06.
outputs_pos.loss : 0.8990787863731384
outputs_pos.loss : 1.1970226764678955
outputs_pos.loss : 0.6949491500854492
outputs_pos.loss : 1.2396843433380127
outputs_pos.loss : 1.0663518905639648
outputs_pos.loss : 1.0241491794586182
outputs_pos.loss : 1.0312602519989014
outputs_pos.loss : 2.0345375537872314
Epoch 01367: adjusting learning rate of group 0 to 2.3865e-06.
outputs_pos.loss : 1.438217043876648
outputs_pos.loss : 1.439427137374878
outputs_pos.loss : 1.1825591325759888
outputs_pos.loss : 1.095556378364563
outputs_pos.loss : 1.338802456855774
outputs_pos.loss : 1.4734253883361816
outputs_pos.loss : 0.8909915089607239
outputs_pos.loss : 0.9930289387702942
Epoch 01368: adjusting learning rate of group 0 to 2.3863e-06.
outputs_pos.loss : 0.8841263651847839
outputs_pos.loss : 1.203397274017334
outputs_pos.loss : 1.3234895467758179
outputs_pos.loss : 1.147087574005127
outputs_pos.loss : 1.0715309381484985
outputs_pos.loss : 1.1639925241470337
outputs_pos.loss : 0.8962424993515015
outputs_pos.loss : 1.1337984800338745
Epoch 01369: adjusting learning rate of group 0 to 2.3862e-06.
outputs_pos.loss : 1.4489879608154297
outputs_pos.loss : 1.2147088050842285
outputs_pos.loss : 1.313077449798584
outputs_pos.loss : 0.9474893808364868
outputs_pos.loss : 1.2602720260620117
outputs_pos.loss : 0.9353591799736023
outputs_pos.loss : 1.2268725633621216
outputs_pos.loss : 0.9909877181053162
Epoch 01370: adjusting learning rate of group 0 to 2.3860e-06.
outputs_pos.loss : 0.9988561868667603
outputs_pos.loss : 1.5621376037597656
outputs_pos.loss : 1.4152686595916748
outputs_pos.loss : 1.3368606567382812
outputs_pos.loss : 1.3740538358688354
outputs_pos.loss : 0.8838562965393066
outputs_pos.loss : 1.3244812488555908
outputs_pos.loss : 1.0203319787979126
Epoch 01371: adjusting learning rate of group 0 to 2.3858e-06.
outputs_pos.loss : 1.5231719017028809
outputs_pos.loss : 0.9921169877052307
outputs_pos.loss : 0.9604124426841736
outputs_pos.loss : 0.9560954570770264
outputs_pos.loss : 1.0785645246505737
outputs_pos.loss : 1.3345766067504883
outputs_pos.loss : 1.736344814300537
outputs_pos.loss : 0.780384361743927
Epoch 01372: adjusting learning rate of group 0 to 2.3857e-06.
outputs_pos.loss : 1.0152572393417358
outputs_pos.loss : 1.0710539817810059
outputs_pos.loss : 0.9508163332939148
outputs_pos.loss : 1.4272209405899048
outputs_pos.loss : 0.8811250925064087
outputs_pos.loss : 1.5046989917755127
outputs_pos.loss : 1.4839235544204712
outputs_pos.loss : 1.2391464710235596
Epoch 01373: adjusting learning rate of group 0 to 2.3855e-06.
outputs_pos.loss : 1.072404384613037
outputs_pos.loss : 1.2351700067520142
outputs_pos.loss : 0.8960428237915039
outputs_pos.loss : 1.7699875831604004
outputs_pos.loss : 1.7667407989501953
outputs_pos.loss : 0.9330834150314331
outputs_pos.loss : 1.3297710418701172
outputs_pos.loss : 1.2176671028137207
Epoch 01374: adjusting learning rate of group 0 to 2.3853e-06.
outputs_pos.loss : 1.144721508026123
outputs_pos.loss : 0.8274266123771667
outputs_pos.loss : 0.9441106915473938
outputs_pos.loss : 1.120153784751892
outputs_pos.loss : 0.8880757689476013
outputs_pos.loss : 1.4403640031814575
outputs_pos.loss : 0.8776239156723022
outputs_pos.loss : 1.1364353895187378
Epoch 01375: adjusting learning rate of group 0 to 2.3852e-06.
outputs_pos.loss : 1.1188514232635498
outputs_pos.loss : 1.1841989755630493
outputs_pos.loss : 1.8617924451828003
outputs_pos.loss : 0.989328145980835
outputs_pos.loss : 0.8985132575035095
outputs_pos.loss : 0.894436240196228
outputs_pos.loss : 0.7683682441711426
outputs_pos.loss : 0.8240509629249573
Epoch 01376: adjusting learning rate of group 0 to 2.3850e-06.
outputs_pos.loss : 1.3907279968261719
outputs_pos.loss : 1.410111665725708
outputs_pos.loss : 2.074516773223877
outputs_pos.loss : 1.2390133142471313
outputs_pos.loss : 1.4430323839187622
outputs_pos.loss : 1.292769193649292
outputs_pos.loss : 0.9902482032775879
outputs_pos.loss : 0.7130534648895264
Epoch 01377: adjusting learning rate of group 0 to 2.3848e-06.
outputs_pos.loss : 0.9989321231842041
outputs_pos.loss : 0.9682118892669678
outputs_pos.loss : 1.0661238431930542
outputs_pos.loss : 1.0690253973007202
outputs_pos.loss : 1.215024471282959
outputs_pos.loss : 1.1407241821289062
outputs_pos.loss : 1.1770457029342651
outputs_pos.loss : 1.223172903060913
Epoch 01378: adjusting learning rate of group 0 to 2.3847e-06.
outputs_pos.loss : 1.0057635307312012
outputs_pos.loss : 1.3755990266799927
outputs_pos.loss : 0.9222040772438049
outputs_pos.loss : 1.191117763519287
outputs_pos.loss : 0.7718533873558044
outputs_pos.loss : 1.4368072748184204
outputs_pos.loss : 1.1245355606079102
outputs_pos.loss : 1.129716396331787
Epoch 01379: adjusting learning rate of group 0 to 2.3845e-06.
outputs_pos.loss : 1.134132981300354
outputs_pos.loss : 0.9547298550605774
outputs_pos.loss : 1.1474180221557617
outputs_pos.loss : 1.0892562866210938
outputs_pos.loss : 1.40350341796875
outputs_pos.loss : 0.8811087608337402
outputs_pos.loss : 0.9430142641067505
outputs_pos.loss : 0.8277994990348816
Epoch 01380: adjusting learning rate of group 0 to 2.3844e-06.
outputs_pos.loss : 1.600159764289856
outputs_pos.loss : 0.8872120380401611
outputs_pos.loss : 1.3272291421890259
outputs_pos.loss : 0.8680652379989624
outputs_pos.loss : 1.0055207014083862
outputs_pos.loss : 1.4222854375839233
outputs_pos.loss : 0.9693045020103455
outputs_pos.loss : 0.8875766396522522
Epoch 01381: adjusting learning rate of group 0 to 2.3842e-06.
outputs_pos.loss : 1.4257638454437256
outputs_pos.loss : 1.3059533834457397
outputs_pos.loss : 1.2942689657211304
outputs_pos.loss : 0.9333537817001343
outputs_pos.loss : 0.9546164274215698
outputs_pos.loss : 1.1299511194229126
outputs_pos.loss : 0.6067445278167725
outputs_pos.loss : 1.040876865386963
Epoch 01382: adjusting learning rate of group 0 to 2.3840e-06.
outputs_pos.loss : 1.0929138660430908
outputs_pos.loss : 0.8337289094924927
outputs_pos.loss : 1.3368552923202515
outputs_pos.loss : 1.6890807151794434
outputs_pos.loss : 1.3071030378341675
outputs_pos.loss : 1.514434576034546
outputs_pos.loss : 0.9833196997642517
outputs_pos.loss : 1.242630958557129
Epoch 01383: adjusting learning rate of group 0 to 2.3839e-06.
outputs_pos.loss : 1.0240187644958496
outputs_pos.loss : 1.5670747756958008
outputs_pos.loss : 1.2342151403427124
outputs_pos.loss : 1.0601774454116821
outputs_pos.loss : 0.9457836747169495
outputs_pos.loss : 1.335737705230713
outputs_pos.loss : 1.4141769409179688
outputs_pos.loss : 0.908447802066803
Epoch 01384: adjusting learning rate of group 0 to 2.3837e-06.
outputs_pos.loss : 0.7646782994270325
outputs_pos.loss : 1.7052513360977173
outputs_pos.loss : 1.0694221258163452
outputs_pos.loss : 1.3029046058654785
outputs_pos.loss : 0.9155146479606628
outputs_pos.loss : 0.8645220994949341
outputs_pos.loss : 0.6538624167442322
outputs_pos.loss : 1.189083456993103
Epoch 01385: adjusting learning rate of group 0 to 2.3835e-06.
outputs_pos.loss : 1.3902469873428345
outputs_pos.loss : 0.8077075481414795
outputs_pos.loss : 1.1362286806106567
outputs_pos.loss : 0.9699994325637817
outputs_pos.loss : 0.9429457783699036
outputs_pos.loss : 1.120439887046814
outputs_pos.loss : 1.0011337995529175
outputs_pos.loss : 0.8469130396842957
Epoch 01386: adjusting learning rate of group 0 to 2.3834e-06.
outputs_pos.loss : 1.0985362529754639
outputs_pos.loss : 1.3814547061920166
outputs_pos.loss : 1.378657579421997
outputs_pos.loss : 0.7445834875106812
outputs_pos.loss : 1.0962580442428589
outputs_pos.loss : 0.9818627238273621
outputs_pos.loss : 0.9238414168357849
outputs_pos.loss : 1.0761303901672363
Epoch 01387: adjusting learning rate of group 0 to 2.3832e-06.
outputs_pos.loss : 1.2855631113052368
outputs_pos.loss : 1.5849709510803223
outputs_pos.loss : 1.556473970413208
outputs_pos.loss : 1.1085827350616455
outputs_pos.loss : 1.3951979875564575
outputs_pos.loss : 1.2167096138000488
outputs_pos.loss : 0.8923211693763733
outputs_pos.loss : 1.065632700920105
Epoch 01388: adjusting learning rate of group 0 to 2.3830e-06.
outputs_pos.loss : 1.1516122817993164
outputs_pos.loss : 1.0016096830368042
outputs_pos.loss : 1.1290618181228638
outputs_pos.loss : 1.0864235162734985
outputs_pos.loss : 1.0498576164245605
outputs_pos.loss : 0.9884437322616577
outputs_pos.loss : 1.3410717248916626
outputs_pos.loss : 1.9312764406204224
Epoch 01389: adjusting learning rate of group 0 to 2.3829e-06.
outputs_pos.loss : 0.8394492864608765
outputs_pos.loss : 1.5076351165771484
outputs_pos.loss : 1.049194097518921
outputs_pos.loss : 1.0685787200927734
outputs_pos.loss : 1.1263563632965088
outputs_pos.loss : 1.3650813102722168
outputs_pos.loss : 0.9871914982795715
outputs_pos.loss : 0.8817796111106873
Epoch 01390: adjusting learning rate of group 0 to 2.3827e-06.
outputs_pos.loss : 0.9422670006752014
outputs_pos.loss : 1.0791831016540527
outputs_pos.loss : 0.9453493356704712
outputs_pos.loss : 1.200958490371704
outputs_pos.loss : 0.826872706413269
outputs_pos.loss : 1.4107718467712402
outputs_pos.loss : 1.440537929534912
outputs_pos.loss : 1.3793011903762817
Epoch 01391: adjusting learning rate of group 0 to 2.3825e-06.
outputs_pos.loss : 0.8858884572982788
outputs_pos.loss : 1.275787591934204
outputs_pos.loss : 0.829785168170929
outputs_pos.loss : 1.0576183795928955
outputs_pos.loss : 1.1723560094833374
outputs_pos.loss : 0.9713518619537354
outputs_pos.loss : 0.9915795922279358
outputs_pos.loss : 1.1087334156036377
Epoch 01392: adjusting learning rate of group 0 to 2.3824e-06.
outputs_pos.loss : 1.0445070266723633
outputs_pos.loss : 1.335946798324585
outputs_pos.loss : 1.7209364175796509
outputs_pos.loss : 1.2140558958053589
outputs_pos.loss : 1.024162769317627
outputs_pos.loss : 1.2343850135803223
outputs_pos.loss : 1.2616044282913208
outputs_pos.loss : 0.772268533706665
Epoch 01393: adjusting learning rate of group 0 to 2.3822e-06.
outputs_pos.loss : 1.2197712659835815
outputs_pos.loss : 1.0975837707519531
outputs_pos.loss : 1.0735374689102173
outputs_pos.loss : 0.9940993189811707
outputs_pos.loss : 1.2538576126098633
outputs_pos.loss : 1.0784426927566528
outputs_pos.loss : 1.4710052013397217
outputs_pos.loss : 0.985112190246582
Epoch 01394: adjusting learning rate of group 0 to 2.3820e-06.
outputs_pos.loss : 2.1744577884674072
outputs_pos.loss : 1.4385920763015747
outputs_pos.loss : 1.245531439781189
outputs_pos.loss : 0.998221218585968
outputs_pos.loss : 1.8290202617645264
outputs_pos.loss : 1.428171157836914
outputs_pos.loss : 1.1730772256851196
outputs_pos.loss : 1.111362338066101
Epoch 01395: adjusting learning rate of group 0 to 2.3819e-06.
outputs_pos.loss : 1.6419934034347534
outputs_pos.loss : 1.371565818786621
outputs_pos.loss : 0.9974188208580017
outputs_pos.loss : 1.1913694143295288
outputs_pos.loss : 1.8234087228775024
outputs_pos.loss : 1.958954930305481
outputs_pos.loss : 1.2749587297439575
outputs_pos.loss : 0.7975581884384155
Epoch 01396: adjusting learning rate of group 0 to 2.3817e-06.
outputs_pos.loss : 1.096459150314331
outputs_pos.loss : 1.157867431640625
outputs_pos.loss : 1.1849900484085083
outputs_pos.loss : 1.3424310684204102
outputs_pos.loss : 1.0939546823501587
outputs_pos.loss : 1.0195839405059814
outputs_pos.loss : 1.099456787109375
outputs_pos.loss : 1.2056316137313843
Epoch 01397: adjusting learning rate of group 0 to 2.3815e-06.
outputs_pos.loss : 0.8050288558006287
outputs_pos.loss : 1.048709511756897
outputs_pos.loss : 1.0000144243240356
outputs_pos.loss : 1.5539171695709229
outputs_pos.loss : 1.2794902324676514
outputs_pos.loss : 0.9288762807846069
outputs_pos.loss : 1.6956508159637451
outputs_pos.loss : 1.2973048686981201
Epoch 01398: adjusting learning rate of group 0 to 2.3814e-06.
outputs_pos.loss : 0.9571075439453125
outputs_pos.loss : 0.8797260522842407
outputs_pos.loss : 1.1102688312530518
outputs_pos.loss : 1.1119674444198608
outputs_pos.loss : 1.8246432542800903
outputs_pos.loss : 1.3600610494613647
outputs_pos.loss : 1.200820803642273
outputs_pos.loss : 1.0818289518356323
Epoch 01399: adjusting learning rate of group 0 to 2.3812e-06.
outputs_pos.loss : 1.562180995941162
outputs_pos.loss : 0.9988530278205872
outputs_pos.loss : 1.398146629333496
outputs_pos.loss : 1.0844770669937134
outputs_pos.loss : 1.0152839422225952
outputs_pos.loss : 1.0654388666152954
outputs_pos.loss : 1.4065946340560913
outputs_pos.loss : 0.892403781414032
Epoch 01400: adjusting learning rate of group 0 to 2.3810e-06.
outputs_pos.loss : 0.8843070268630981
outputs_pos.loss : 1.3482097387313843
outputs_pos.loss : 0.758989155292511
outputs_pos.loss : 2.2339553833007812
outputs_pos.loss : 1.504651665687561
outputs_pos.loss : 0.9758954644203186
outputs_pos.loss : 1.375462293624878
outputs_pos.loss : 1.099397897720337
Epoch 01401: adjusting learning rate of group 0 to 2.3809e-06.
outputs_pos.loss : 1.0618314743041992
outputs_pos.loss : 1.1991100311279297
outputs_pos.loss : 0.9361212849617004
outputs_pos.loss : 1.1810153722763062
outputs_pos.loss : 1.0290004014968872
outputs_pos.loss : 0.8051348328590393
outputs_pos.loss : 1.6160249710083008
outputs_pos.loss : 1.289728045463562
Epoch 01402: adjusting learning rate of group 0 to 2.3807e-06.
outputs_pos.loss : 1.0092400312423706
outputs_pos.loss : 0.7280529737472534
outputs_pos.loss : 1.010870337486267
outputs_pos.loss : 1.1635528802871704
outputs_pos.loss : 1.1429754495620728
outputs_pos.loss : 1.1456265449523926
outputs_pos.loss : 1.7761563062667847
outputs_pos.loss : 0.9325916767120361
Epoch 01403: adjusting learning rate of group 0 to 2.3805e-06.
outputs_pos.loss : 0.8674472570419312
outputs_pos.loss : 1.2963836193084717
outputs_pos.loss : 1.2517586946487427
outputs_pos.loss : 0.976134181022644
outputs_pos.loss : 0.9300137758255005
outputs_pos.loss : 0.8280779123306274
outputs_pos.loss : 1.0792170763015747
outputs_pos.loss : 1.0007121562957764
Epoch 01404: adjusting learning rate of group 0 to 2.3804e-06.
outputs_pos.loss : 0.9228254556655884
outputs_pos.loss : 1.158380389213562
outputs_pos.loss : 0.9042654037475586
outputs_pos.loss : 0.8702904582023621
outputs_pos.loss : 1.0624226331710815
outputs_pos.loss : 1.2433196306228638
outputs_pos.loss : 1.0159246921539307
outputs_pos.loss : 1.0705524682998657
Epoch 01405: adjusting learning rate of group 0 to 2.3802e-06.
outputs_pos.loss : 1.401121735572815
outputs_pos.loss : 1.0446182489395142
outputs_pos.loss : 1.002915382385254
outputs_pos.loss : 1.8240025043487549
outputs_pos.loss : 1.001715898513794
outputs_pos.loss : 1.3325462341308594
outputs_pos.loss : 1.2289763689041138
outputs_pos.loss : 1.1205966472625732
Epoch 01406: adjusting learning rate of group 0 to 2.3800e-06.
outputs_pos.loss : 1.4398969411849976
outputs_pos.loss : 1.0316755771636963
outputs_pos.loss : 0.5734829306602478
outputs_pos.loss : 1.1178351640701294
outputs_pos.loss : 1.0329992771148682
outputs_pos.loss : 1.0665689706802368
outputs_pos.loss : 1.0014010667800903
outputs_pos.loss : 1.0324368476867676
Epoch 01407: adjusting learning rate of group 0 to 2.3799e-06.
outputs_pos.loss : 1.2059589624404907
outputs_pos.loss : 0.8181292414665222
outputs_pos.loss : 1.0237147808074951
outputs_pos.loss : 0.9245224595069885
outputs_pos.loss : 1.3055357933044434
outputs_pos.loss : 1.1953462362289429
outputs_pos.loss : 0.8234635591506958
outputs_pos.loss : 0.9692066311836243
Epoch 01408: adjusting learning rate of group 0 to 2.3797e-06.
outputs_pos.loss : 1.4649242162704468
outputs_pos.loss : 0.8090248703956604
outputs_pos.loss : 0.9860024452209473
outputs_pos.loss : 1.1304084062576294
outputs_pos.loss : 1.4379782676696777
outputs_pos.loss : 1.3982945680618286
outputs_pos.loss : 1.4783378839492798
outputs_pos.loss : 1.2268965244293213
Epoch 01409: adjusting learning rate of group 0 to 2.3795e-06.
outputs_pos.loss : 1.3406248092651367
outputs_pos.loss : 0.9819018840789795
outputs_pos.loss : 0.7981795072555542
outputs_pos.loss : 1.109591007232666
outputs_pos.loss : 0.966946542263031
outputs_pos.loss : 1.2643595933914185
outputs_pos.loss : 1.086694359779358
outputs_pos.loss : 1.272973656654358
Epoch 01410: adjusting learning rate of group 0 to 2.3794e-06.
outputs_pos.loss : 1.083723783493042
outputs_pos.loss : 1.8763612508773804
outputs_pos.loss : 1.2808183431625366
outputs_pos.loss : 1.1680468320846558
outputs_pos.loss : 0.9064147472381592
outputs_pos.loss : 1.4820879697799683
outputs_pos.loss : 1.1884291172027588
outputs_pos.loss : 1.1378189325332642
Epoch 01411: adjusting learning rate of group 0 to 2.3792e-06.
outputs_pos.loss : 1.095716953277588
outputs_pos.loss : 0.8076092004776001
outputs_pos.loss : 1.3240476846694946
outputs_pos.loss : 1.417714238166809
outputs_pos.loss : 0.9985561966896057
outputs_pos.loss : 0.9937706589698792
outputs_pos.loss : 0.9749205112457275
outputs_pos.loss : 0.8833780288696289
Epoch 01412: adjusting learning rate of group 0 to 2.3790e-06.
outputs_pos.loss : 0.8114579916000366
outputs_pos.loss : 0.7766261100769043
outputs_pos.loss : 1.08632493019104
outputs_pos.loss : 1.6178736686706543
outputs_pos.loss : 1.1737143993377686
outputs_pos.loss : 1.058833360671997
outputs_pos.loss : 1.6898581981658936
outputs_pos.loss : 1.3076467514038086
Epoch 01413: adjusting learning rate of group 0 to 2.3789e-06.
outputs_pos.loss : 1.1989872455596924
outputs_pos.loss : 1.683387279510498
outputs_pos.loss : 1.0241425037384033
outputs_pos.loss : 0.6732779741287231
outputs_pos.loss : 0.9632254838943481
outputs_pos.loss : 0.9527297616004944
outputs_pos.loss : 1.8961151838302612
outputs_pos.loss : 1.3347162008285522
Epoch 01414: adjusting learning rate of group 0 to 2.3787e-06.
outputs_pos.loss : 2.045135974884033
outputs_pos.loss : 1.5455442667007446
outputs_pos.loss : 1.3589166402816772
outputs_pos.loss : 1.1468414068222046
outputs_pos.loss : 1.3571970462799072
outputs_pos.loss : 1.064454436302185
outputs_pos.loss : 0.8354555368423462
outputs_pos.loss : 1.2313735485076904
Epoch 01415: adjusting learning rate of group 0 to 2.3785e-06.
outputs_pos.loss : 0.750145673751831
outputs_pos.loss : 1.4884518384933472
outputs_pos.loss : 1.542964220046997
outputs_pos.loss : 0.7489204406738281
outputs_pos.loss : 1.5862419605255127
outputs_pos.loss : 1.2087278366088867
outputs_pos.loss : 1.2396215200424194
outputs_pos.loss : 1.479561448097229
Epoch 01416: adjusting learning rate of group 0 to 2.3783e-06.
outputs_pos.loss : 1.211344599723816
outputs_pos.loss : 1.4350486993789673
outputs_pos.loss : 1.006600022315979
outputs_pos.loss : 0.7340801954269409
outputs_pos.loss : 1.0713715553283691
outputs_pos.loss : 0.9542236924171448
outputs_pos.loss : 1.2827889919281006
outputs_pos.loss : 1.044912576675415
Epoch 01417: adjusting learning rate of group 0 to 2.3782e-06.
outputs_pos.loss : 1.321740746498108
outputs_pos.loss : 0.5559000372886658
outputs_pos.loss : 1.526437759399414
outputs_pos.loss : 1.670425295829773
outputs_pos.loss : 1.7220537662506104
outputs_pos.loss : 1.1703318357467651
outputs_pos.loss : 1.328218698501587
outputs_pos.loss : 1.3148378133773804
Epoch 01418: adjusting learning rate of group 0 to 2.3780e-06.
outputs_pos.loss : 1.2310506105422974
outputs_pos.loss : 1.114734172821045
outputs_pos.loss : 1.56136953830719
outputs_pos.loss : 0.8021044731140137
outputs_pos.loss : 1.5087168216705322
outputs_pos.loss : 1.3304568529129028
outputs_pos.loss : 1.6290348768234253
outputs_pos.loss : 1.0487664937973022
Epoch 01419: adjusting learning rate of group 0 to 2.3778e-06.
outputs_pos.loss : 0.722612738609314
outputs_pos.loss : 1.2758475542068481
outputs_pos.loss : 1.1880594491958618
outputs_pos.loss : 0.9932895302772522
outputs_pos.loss : 1.0777825117111206
outputs_pos.loss : 1.3171401023864746
outputs_pos.loss : 1.4270060062408447
outputs_pos.loss : 1.5415692329406738
Epoch 01420: adjusting learning rate of group 0 to 2.3777e-06.
outputs_pos.loss : 2.13008189201355
outputs_pos.loss : 1.5738556385040283
outputs_pos.loss : 1.2720890045166016
outputs_pos.loss : 0.9183829426765442
outputs_pos.loss : 1.4622018337249756
outputs_pos.loss : 1.0776187181472778
outputs_pos.loss : 1.0650088787078857
outputs_pos.loss : 1.610297679901123
Epoch 01421: adjusting learning rate of group 0 to 2.3775e-06.
outputs_pos.loss : 1.288657546043396
outputs_pos.loss : 0.8754466772079468
outputs_pos.loss : 0.9499509930610657
outputs_pos.loss : 0.950147807598114
outputs_pos.loss : 1.1611381769180298
outputs_pos.loss : 1.3946824073791504
outputs_pos.loss : 1.3449312448501587
outputs_pos.loss : 1.0183595418930054
Epoch 01422: adjusting learning rate of group 0 to 2.3773e-06.
outputs_pos.loss : 0.9575564861297607
outputs_pos.loss : 0.9334808588027954
outputs_pos.loss : 1.8306390047073364
outputs_pos.loss : 1.4160538911819458
outputs_pos.loss : 0.9573732614517212
outputs_pos.loss : 1.1880640983581543
outputs_pos.loss : 0.8716424703598022
outputs_pos.loss : 1.3249520063400269
Epoch 01423: adjusting learning rate of group 0 to 2.3772e-06.
outputs_pos.loss : 1.413716197013855
outputs_pos.loss : 0.9256595969200134
outputs_pos.loss : 1.0185929536819458
outputs_pos.loss : 1.5200724601745605
outputs_pos.loss : 0.8470433354377747
outputs_pos.loss : 1.205967903137207
outputs_pos.loss : 0.7550858855247498
outputs_pos.loss : 1.0359798669815063
Epoch 01424: adjusting learning rate of group 0 to 2.3770e-06.
outputs_pos.loss : 1.0084635019302368
outputs_pos.loss : 1.3892329931259155
outputs_pos.loss : 1.653761386871338
outputs_pos.loss : 1.1020034551620483
outputs_pos.loss : 1.11665940284729
outputs_pos.loss : 1.5514283180236816
outputs_pos.loss : 0.765673041343689
outputs_pos.loss : 1.0192443132400513
Epoch 01425: adjusting learning rate of group 0 to 2.3768e-06.
outputs_pos.loss : 1.2508546113967896
outputs_pos.loss : 1.2952735424041748
outputs_pos.loss : 1.1704258918762207
outputs_pos.loss : 1.476047158241272
outputs_pos.loss : 1.1514092683792114
outputs_pos.loss : 1.0553557872772217
outputs_pos.loss : 1.1273733377456665
outputs_pos.loss : 1.2357099056243896
Epoch 01426: adjusting learning rate of group 0 to 2.3766e-06.
outputs_pos.loss : 1.2387014627456665
outputs_pos.loss : 1.237178921699524
outputs_pos.loss : 0.804710328578949
outputs_pos.loss : 0.9542468786239624
outputs_pos.loss : 0.8674638271331787
outputs_pos.loss : 1.196040153503418
outputs_pos.loss : 0.8819563984870911
outputs_pos.loss : 1.1572602987289429
Epoch 01427: adjusting learning rate of group 0 to 2.3765e-06.
outputs_pos.loss : 1.145786166191101
outputs_pos.loss : 0.800921618938446
outputs_pos.loss : 1.4198832511901855
outputs_pos.loss : 1.2029985189437866
outputs_pos.loss : 1.3946025371551514
outputs_pos.loss : 1.1963578462600708
outputs_pos.loss : 1.3794910907745361
outputs_pos.loss : 1.2329769134521484
Epoch 01428: adjusting learning rate of group 0 to 2.3763e-06.
outputs_pos.loss : 0.6669744849205017
outputs_pos.loss : 1.087445616722107
outputs_pos.loss : 1.2951663732528687
outputs_pos.loss : 0.957203209400177
outputs_pos.loss : 1.1346746683120728
outputs_pos.loss : 1.1010916233062744
outputs_pos.loss : 1.0609291791915894
outputs_pos.loss : 1.2911404371261597
Epoch 01429: adjusting learning rate of group 0 to 2.3761e-06.
outputs_pos.loss : 2.0618743896484375
outputs_pos.loss : 1.4732826948165894
outputs_pos.loss : 1.2218414545059204
outputs_pos.loss : 1.1679145097732544
outputs_pos.loss : 1.2257170677185059
outputs_pos.loss : 1.359710931777954
outputs_pos.loss : 0.8387539386749268
outputs_pos.loss : 0.994318962097168
Epoch 01430: adjusting learning rate of group 0 to 2.3760e-06.
outputs_pos.loss : 1.1203131675720215
outputs_pos.loss : 0.9353495240211487
outputs_pos.loss : 0.7198529839515686
outputs_pos.loss : 0.9646272659301758
outputs_pos.loss : 0.8988069295883179
outputs_pos.loss : 0.9708693027496338
outputs_pos.loss : 0.9228578209877014
outputs_pos.loss : 1.4938052892684937
Epoch 01431: adjusting learning rate of group 0 to 2.3758e-06.
outputs_pos.loss : 1.2010383605957031
outputs_pos.loss : 1.240268588066101
outputs_pos.loss : 0.8808765411376953
outputs_pos.loss : 0.8938097953796387
outputs_pos.loss : 1.258428692817688
outputs_pos.loss : 0.9216554760932922
outputs_pos.loss : 1.1467223167419434
outputs_pos.loss : 1.2317157983779907
Epoch 01432: adjusting learning rate of group 0 to 2.3756e-06.
outputs_pos.loss : 0.8785496354103088
outputs_pos.loss : 0.9876556396484375
outputs_pos.loss : 0.7868086695671082
outputs_pos.loss : 1.1385749578475952
outputs_pos.loss : 1.0561274290084839
outputs_pos.loss : 1.1705090999603271
outputs_pos.loss : 0.5686124563217163
outputs_pos.loss : 1.0524892807006836
Epoch 01433: adjusting learning rate of group 0 to 2.3755e-06.
outputs_pos.loss : 0.811154842376709
outputs_pos.loss : 1.2969385385513306
outputs_pos.loss : 1.0272376537322998
outputs_pos.loss : 1.077505350112915
outputs_pos.loss : 1.3351818323135376
outputs_pos.loss : 0.9598134756088257
outputs_pos.loss : 2.09134840965271
outputs_pos.loss : 1.0094361305236816
Epoch 01434: adjusting learning rate of group 0 to 2.3753e-06.
outputs_pos.loss : 1.2502413988113403
outputs_pos.loss : 0.8904427289962769
outputs_pos.loss : 1.3426381349563599
outputs_pos.loss : 1.2766759395599365
outputs_pos.loss : 1.4608677625656128
outputs_pos.loss : 1.1392430067062378
outputs_pos.loss : 1.0068608522415161
outputs_pos.loss : 1.181597113609314
Epoch 01435: adjusting learning rate of group 0 to 2.3751e-06.
outputs_pos.loss : 1.1787388324737549
outputs_pos.loss : 1.0176763534545898
outputs_pos.loss : 1.0822899341583252
outputs_pos.loss : 0.8515455722808838
outputs_pos.loss : 1.3621052503585815
outputs_pos.loss : 1.0699191093444824
outputs_pos.loss : 1.13861083984375
outputs_pos.loss : 0.9981933832168579
Epoch 01436: adjusting learning rate of group 0 to 2.3749e-06.
outputs_pos.loss : 0.9623396992683411
outputs_pos.loss : 0.7502397298812866
outputs_pos.loss : 1.018828272819519
outputs_pos.loss : 1.05685555934906
outputs_pos.loss : 1.2309703826904297
outputs_pos.loss : 1.2015215158462524
outputs_pos.loss : 1.4318697452545166
outputs_pos.loss : 1.3898335695266724
Epoch 01437: adjusting learning rate of group 0 to 2.3748e-06.
outputs_pos.loss : 1.057928204536438
outputs_pos.loss : 1.6434404850006104
outputs_pos.loss : 1.175167202949524
outputs_pos.loss : 1.0807796716690063
outputs_pos.loss : 1.2491495609283447
outputs_pos.loss : 1.3451118469238281
outputs_pos.loss : 0.9831428527832031
outputs_pos.loss : 1.0891072750091553
Epoch 01438: adjusting learning rate of group 0 to 2.3746e-06.
outputs_pos.loss : 1.367016315460205
outputs_pos.loss : 0.9728952050209045
outputs_pos.loss : 0.9186696410179138
outputs_pos.loss : 1.1536389589309692
outputs_pos.loss : 1.1006793975830078
outputs_pos.loss : 0.9040189385414124
outputs_pos.loss : 1.0547674894332886
outputs_pos.loss : 0.9875513911247253
Epoch 01439: adjusting learning rate of group 0 to 2.3744e-06.
outputs_pos.loss : 1.5866026878356934
outputs_pos.loss : 1.3767324686050415
outputs_pos.loss : 1.3925584554672241
outputs_pos.loss : 1.124537706375122
outputs_pos.loss : 0.6791772842407227
outputs_pos.loss : 1.3058561086654663
outputs_pos.loss : 1.1339613199234009
outputs_pos.loss : 1.0191186666488647
Epoch 01440: adjusting learning rate of group 0 to 2.3743e-06.
outputs_pos.loss : 1.0866740942001343
outputs_pos.loss : 1.2910988330841064
outputs_pos.loss : 1.4954369068145752
outputs_pos.loss : 0.8254945874214172
outputs_pos.loss : 0.8669096827507019
outputs_pos.loss : 1.7019855976104736
outputs_pos.loss : 1.271032691001892
outputs_pos.loss : 0.8674847483634949
Epoch 01441: adjusting learning rate of group 0 to 2.3741e-06.
outputs_pos.loss : 1.0206066370010376
outputs_pos.loss : 1.3135062456130981
outputs_pos.loss : 1.1071945428848267
outputs_pos.loss : 1.1663906574249268
outputs_pos.loss : 1.0308914184570312
outputs_pos.loss : 0.9506711959838867
outputs_pos.loss : 1.2856534719467163
outputs_pos.loss : 1.4301966428756714
Epoch 01442: adjusting learning rate of group 0 to 2.3739e-06.
outputs_pos.loss : 1.3012529611587524
outputs_pos.loss : 0.8633142113685608
outputs_pos.loss : 1.130008578300476
outputs_pos.loss : 0.9128761291503906
outputs_pos.loss : 1.1207026243209839
outputs_pos.loss : 1.1294481754302979
outputs_pos.loss : 1.2062078714370728
outputs_pos.loss : 1.3552912473678589
Epoch 01443: adjusting learning rate of group 0 to 2.3737e-06.
outputs_pos.loss : 0.9375261068344116
outputs_pos.loss : 0.7932202816009521
outputs_pos.loss : 1.192893147468567
outputs_pos.loss : 1.2634345293045044
outputs_pos.loss : 1.1295042037963867
outputs_pos.loss : 1.0871572494506836
outputs_pos.loss : 1.0599032640457153
outputs_pos.loss : 0.8834271430969238
Epoch 01444: adjusting learning rate of group 0 to 2.3736e-06.
outputs_pos.loss : 1.485845685005188
outputs_pos.loss : 1.4423329830169678
outputs_pos.loss : 1.4220443964004517
outputs_pos.loss : 1.102892518043518
outputs_pos.loss : 1.0023224353790283
outputs_pos.loss : 1.3542414903640747
outputs_pos.loss : 1.344070553779602
outputs_pos.loss : 0.5621618628501892
Epoch 01445: adjusting learning rate of group 0 to 2.3734e-06.
outputs_pos.loss : 1.0089354515075684
outputs_pos.loss : 0.9395818710327148
outputs_pos.loss : 1.788283109664917
outputs_pos.loss : 0.9936078190803528
outputs_pos.loss : 1.3549473285675049
outputs_pos.loss : 1.1825345754623413
outputs_pos.loss : 1.1238172054290771
outputs_pos.loss : 1.111033320426941
Epoch 01446: adjusting learning rate of group 0 to 2.3732e-06.
outputs_pos.loss : 0.9326490759849548
outputs_pos.loss : 1.7930238246917725
outputs_pos.loss : 1.1346943378448486
outputs_pos.loss : 1.4057917594909668
outputs_pos.loss : 1.0496761798858643
outputs_pos.loss : 0.7228540778160095
outputs_pos.loss : 1.191773533821106
outputs_pos.loss : 0.6879971027374268
Epoch 01447: adjusting learning rate of group 0 to 2.3731e-06.
outputs_pos.loss : 0.922850489616394
outputs_pos.loss : 1.6758280992507935
outputs_pos.loss : 1.1116820573806763
outputs_pos.loss : 0.8766574859619141
outputs_pos.loss : 1.4556833505630493
outputs_pos.loss : 1.2040176391601562
outputs_pos.loss : 1.1177990436553955
outputs_pos.loss : 1.0303092002868652
Epoch 01448: adjusting learning rate of group 0 to 2.3729e-06.
outputs_pos.loss : 1.533215045928955
outputs_pos.loss : 0.7561917304992676
outputs_pos.loss : 1.021999716758728
outputs_pos.loss : 1.1912286281585693
outputs_pos.loss : 1.762437343597412
outputs_pos.loss : 1.3728923797607422
outputs_pos.loss : 1.3400369882583618
outputs_pos.loss : 1.2317413091659546
Epoch 01449: adjusting learning rate of group 0 to 2.3727e-06.
outputs_pos.loss : 0.9697306156158447
outputs_pos.loss : 1.2950502634048462
outputs_pos.loss : 1.2536983489990234
outputs_pos.loss : 0.8449925184249878
outputs_pos.loss : 2.104799270629883
outputs_pos.loss : 1.182179570198059
outputs_pos.loss : 1.3320105075836182
outputs_pos.loss : 1.0452343225479126
Epoch 01450: adjusting learning rate of group 0 to 2.3725e-06.
outputs_pos.loss : 1.3132375478744507
outputs_pos.loss : 1.594580054283142
outputs_pos.loss : 1.4932916164398193
outputs_pos.loss : 0.9728272557258606
outputs_pos.loss : 1.5476802587509155
outputs_pos.loss : 1.3311439752578735
outputs_pos.loss : 1.036950707435608
outputs_pos.loss : 0.7824321985244751
Epoch 01451: adjusting learning rate of group 0 to 2.3724e-06.
outputs_pos.loss : 1.054965853691101
outputs_pos.loss : 1.684248447418213
outputs_pos.loss : 0.8249587416648865
outputs_pos.loss : 1.576759934425354
outputs_pos.loss : 1.0192866325378418
outputs_pos.loss : 1.3314305543899536
outputs_pos.loss : 0.8287697434425354
outputs_pos.loss : 0.8944957256317139
Epoch 01452: adjusting learning rate of group 0 to 2.3722e-06.
outputs_pos.loss : 0.8457441329956055
outputs_pos.loss : 1.448442816734314
outputs_pos.loss : 1.1060729026794434
outputs_pos.loss : 1.2614421844482422
outputs_pos.loss : 1.864113211631775
outputs_pos.loss : 1.1536636352539062
outputs_pos.loss : 0.9971438050270081
outputs_pos.loss : 1.0958316326141357
Epoch 01453: adjusting learning rate of group 0 to 2.3720e-06.
outputs_pos.loss : 1.0885627269744873
outputs_pos.loss : 1.0209015607833862
outputs_pos.loss : 0.9792444109916687
outputs_pos.loss : 1.4688799381256104
outputs_pos.loss : 0.9277677536010742
outputs_pos.loss : 1.0257012844085693
outputs_pos.loss : 1.211127758026123
outputs_pos.loss : 1.0721900463104248
Epoch 01454: adjusting learning rate of group 0 to 2.3718e-06.
outputs_pos.loss : 0.8509151935577393
outputs_pos.loss : 1.2571409940719604
outputs_pos.loss : 0.9903677701950073
outputs_pos.loss : 0.8720092177391052
outputs_pos.loss : 1.2322405576705933
outputs_pos.loss : 1.3526266813278198
outputs_pos.loss : 0.6762693524360657
outputs_pos.loss : 0.7468042373657227
Epoch 01455: adjusting learning rate of group 0 to 2.3717e-06.
outputs_pos.loss : 0.8565462231636047
outputs_pos.loss : 0.7004594206809998
outputs_pos.loss : 0.9681441783905029
outputs_pos.loss : 0.7670927047729492
outputs_pos.loss : 1.4543641805648804
outputs_pos.loss : 0.9513850212097168
outputs_pos.loss : 1.0056772232055664
outputs_pos.loss : 1.0656960010528564
Epoch 01456: adjusting learning rate of group 0 to 2.3715e-06.
outputs_pos.loss : 0.9877393841743469
outputs_pos.loss : 1.089255690574646
outputs_pos.loss : 1.3974955081939697
outputs_pos.loss : 1.6457037925720215
outputs_pos.loss : 1.0415855646133423
outputs_pos.loss : 0.915056586265564
outputs_pos.loss : 1.0906140804290771
outputs_pos.loss : 0.9364568591117859
Epoch 01457: adjusting learning rate of group 0 to 2.3713e-06.
outputs_pos.loss : 0.9347473978996277
outputs_pos.loss : 0.7948850989341736
outputs_pos.loss : 0.8722054362297058
outputs_pos.loss : 1.175163745880127
outputs_pos.loss : 0.9057841897010803
outputs_pos.loss : 1.3019357919692993
outputs_pos.loss : 0.8282164335250854
outputs_pos.loss : 1.0759282112121582
Epoch 01458: adjusting learning rate of group 0 to 2.3711e-06.
outputs_pos.loss : 1.4994494915008545
outputs_pos.loss : 1.4929219484329224
outputs_pos.loss : 1.2824134826660156
outputs_pos.loss : 1.1047755479812622
outputs_pos.loss : 1.2691099643707275
outputs_pos.loss : 1.0908470153808594
outputs_pos.loss : 0.9293007254600525
outputs_pos.loss : 1.2379173040390015
Epoch 01459: adjusting learning rate of group 0 to 2.3710e-06.
outputs_pos.loss : 1.2453385591506958
outputs_pos.loss : 1.2183083295822144
outputs_pos.loss : 1.4391947984695435
outputs_pos.loss : 0.8477153182029724
outputs_pos.loss : 1.4793850183486938
outputs_pos.loss : 1.4179551601409912
outputs_pos.loss : 1.5197008848190308
outputs_pos.loss : 1.0501412153244019
Epoch 01460: adjusting learning rate of group 0 to 2.3708e-06.
outputs_pos.loss : 1.0226043462753296
outputs_pos.loss : 1.3479517698287964
outputs_pos.loss : 1.297467589378357
outputs_pos.loss : 1.793539047241211
outputs_pos.loss : 0.8743388056755066
outputs_pos.loss : 0.8067224621772766
outputs_pos.loss : 1.4057844877243042
outputs_pos.loss : 1.027402639389038
Epoch 01461: adjusting learning rate of group 0 to 2.3706e-06.
outputs_pos.loss : 0.9901505708694458
outputs_pos.loss : 1.3106945753097534
outputs_pos.loss : 0.9292381405830383
outputs_pos.loss : 1.7306385040283203
outputs_pos.loss : 0.9999719262123108
outputs_pos.loss : 1.1771409511566162
outputs_pos.loss : 0.951391339302063
outputs_pos.loss : 1.3736482858657837
Epoch 01462: adjusting learning rate of group 0 to 2.3705e-06.
outputs_pos.loss : 1.1091487407684326
outputs_pos.loss : 1.0831592082977295
outputs_pos.loss : 0.9836511611938477
outputs_pos.loss : 0.9556543231010437
outputs_pos.loss : 0.8220067620277405
outputs_pos.loss : 1.5524799823760986
outputs_pos.loss : 0.9651975035667419
outputs_pos.loss : 1.6281346082687378
Epoch 01463: adjusting learning rate of group 0 to 2.3703e-06.
outputs_pos.loss : 0.9680674076080322
outputs_pos.loss : 1.0494985580444336
outputs_pos.loss : 1.2991466522216797
outputs_pos.loss : 0.8693541288375854
outputs_pos.loss : 0.9289035797119141
outputs_pos.loss : 0.8002222180366516
outputs_pos.loss : 0.8754344582557678
outputs_pos.loss : 1.0424903631210327
Epoch 01464: adjusting learning rate of group 0 to 2.3701e-06.
outputs_pos.loss : 1.1795654296875
outputs_pos.loss : 1.4636343717575073
outputs_pos.loss : 1.0925378799438477
outputs_pos.loss : 1.15144681930542
outputs_pos.loss : 1.0885250568389893
outputs_pos.loss : 1.1169465780258179
outputs_pos.loss : 1.55740225315094
outputs_pos.loss : 0.7624572515487671
Epoch 01465: adjusting learning rate of group 0 to 2.3699e-06.
outputs_pos.loss : 1.134590744972229
outputs_pos.loss : 1.177547574043274
outputs_pos.loss : 0.8885879516601562
outputs_pos.loss : 1.1231237649917603
outputs_pos.loss : 1.3465007543563843
outputs_pos.loss : 0.966399073600769
outputs_pos.loss : 1.250881314277649
outputs_pos.loss : 0.9754047989845276
Epoch 01466: adjusting learning rate of group 0 to 2.3698e-06.
outputs_pos.loss : 1.437328577041626
outputs_pos.loss : 1.0438659191131592
outputs_pos.loss : 1.2664670944213867
outputs_pos.loss : 1.3178071975708008
outputs_pos.loss : 0.997875452041626
outputs_pos.loss : 0.9376522898674011
outputs_pos.loss : 0.9962836503982544
outputs_pos.loss : 0.9800205230712891
Epoch 01467: adjusting learning rate of group 0 to 2.3696e-06.
outputs_pos.loss : 0.9057548642158508
outputs_pos.loss : 1.1882063150405884
outputs_pos.loss : 0.8221712708473206
outputs_pos.loss : 1.3372819423675537
outputs_pos.loss : 1.2699697017669678
outputs_pos.loss : 1.8145232200622559
outputs_pos.loss : 1.046980857849121
outputs_pos.loss : 1.2167952060699463
Epoch 01468: adjusting learning rate of group 0 to 2.3694e-06.
outputs_pos.loss : 1.5220576524734497
outputs_pos.loss : 1.4916917085647583
outputs_pos.loss : 1.2230079174041748
outputs_pos.loss : 1.300620198249817
outputs_pos.loss : 0.8931468725204468
outputs_pos.loss : 0.820197343826294
outputs_pos.loss : 1.0832887887954712
outputs_pos.loss : 1.0412815809249878
Epoch 01469: adjusting learning rate of group 0 to 2.3692e-06.
outputs_pos.loss : 1.166343092918396
outputs_pos.loss : 1.5556199550628662
outputs_pos.loss : 0.9793471097946167
outputs_pos.loss : 1.0157766342163086
outputs_pos.loss : 1.4854321479797363
outputs_pos.loss : 1.9692745208740234
outputs_pos.loss : 1.346591830253601
outputs_pos.loss : 1.2043803930282593
Epoch 01470: adjusting learning rate of group 0 to 2.3691e-06.
outputs_pos.loss : 1.2592767477035522
outputs_pos.loss : 1.3846522569656372
outputs_pos.loss : 1.1044728755950928
outputs_pos.loss : 1.100773811340332
outputs_pos.loss : 1.1415172815322876
outputs_pos.loss : 1.663094401359558
outputs_pos.loss : 0.8783053159713745
outputs_pos.loss : 1.1151583194732666
Epoch 01471: adjusting learning rate of group 0 to 2.3689e-06.
outputs_pos.loss : 1.3860255479812622
outputs_pos.loss : 1.3684271574020386
outputs_pos.loss : 1.0169113874435425
outputs_pos.loss : 1.3027546405792236
outputs_pos.loss : 1.5549592971801758
outputs_pos.loss : 1.183544397354126
outputs_pos.loss : 1.3947293758392334
outputs_pos.loss : 1.3769922256469727
Epoch 01472: adjusting learning rate of group 0 to 2.3687e-06.
outputs_pos.loss : 1.1362221240997314
outputs_pos.loss : 1.1719361543655396
outputs_pos.loss : 1.2256097793579102
outputs_pos.loss : 1.0192866325378418
outputs_pos.loss : 1.5077126026153564
outputs_pos.loss : 1.1392185688018799
outputs_pos.loss : 0.872090756893158
outputs_pos.loss : 1.2897562980651855
Epoch 01473: adjusting learning rate of group 0 to 2.3685e-06.
outputs_pos.loss : 1.6335399150848389
outputs_pos.loss : 1.1654397249221802
outputs_pos.loss : 1.1476281881332397
outputs_pos.loss : 0.9807563424110413
outputs_pos.loss : 1.1058247089385986
outputs_pos.loss : 1.1173418760299683
outputs_pos.loss : 1.0865085124969482
outputs_pos.loss : 1.174630045890808
Epoch 01474: adjusting learning rate of group 0 to 2.3684e-06.
outputs_pos.loss : 1.431955337524414
outputs_pos.loss : 1.2029505968093872
outputs_pos.loss : 0.9486864805221558
outputs_pos.loss : 1.0829582214355469
outputs_pos.loss : 0.9162313342094421
outputs_pos.loss : 1.0962026119232178
outputs_pos.loss : 1.1801284551620483
outputs_pos.loss : 1.0642883777618408
Epoch 01475: adjusting learning rate of group 0 to 2.3682e-06.
outputs_pos.loss : 1.59969961643219
outputs_pos.loss : 0.9576427936553955
outputs_pos.loss : 1.2366522550582886
outputs_pos.loss : 1.126345157623291
outputs_pos.loss : 1.3604764938354492
outputs_pos.loss : 0.8348879218101501
outputs_pos.loss : 1.246268391609192
outputs_pos.loss : 1.1855978965759277
Epoch 01476: adjusting learning rate of group 0 to 2.3680e-06.
outputs_pos.loss : 1.4056189060211182
outputs_pos.loss : 0.844264566898346
outputs_pos.loss : 1.253980278968811
outputs_pos.loss : 1.2264585494995117
outputs_pos.loss : 0.9307245016098022
outputs_pos.loss : 1.0671457052230835
outputs_pos.loss : 1.2050890922546387
outputs_pos.loss : 1.0493091344833374
Epoch 01477: adjusting learning rate of group 0 to 2.3678e-06.
outputs_pos.loss : 1.1274529695510864
outputs_pos.loss : 0.9687144756317139
outputs_pos.loss : 1.1692981719970703
outputs_pos.loss : 0.6000863313674927
outputs_pos.loss : 0.9618309736251831
outputs_pos.loss : 1.2675119638442993
outputs_pos.loss : 1.3607124090194702
outputs_pos.loss : 0.9915564060211182
Epoch 01478: adjusting learning rate of group 0 to 2.3677e-06.
outputs_pos.loss : 1.365143060684204
outputs_pos.loss : 0.7092195153236389
outputs_pos.loss : 1.1013805866241455
outputs_pos.loss : 1.1612474918365479
outputs_pos.loss : 1.2340562343597412
outputs_pos.loss : 0.758750855922699
outputs_pos.loss : 1.2337440252304077
outputs_pos.loss : 1.0976223945617676
Epoch 01479: adjusting learning rate of group 0 to 2.3675e-06.
outputs_pos.loss : 1.0465357303619385
outputs_pos.loss : 0.6039416193962097
outputs_pos.loss : 1.2356994152069092
outputs_pos.loss : 1.5872610807418823
outputs_pos.loss : 0.9910552501678467
outputs_pos.loss : 0.9318459033966064
outputs_pos.loss : 1.095254898071289
outputs_pos.loss : 1.231749415397644
Epoch 01480: adjusting learning rate of group 0 to 2.3673e-06.
outputs_pos.loss : 1.0722575187683105
outputs_pos.loss : 1.0157703161239624
outputs_pos.loss : 1.085997223854065
outputs_pos.loss : 1.375481367111206
outputs_pos.loss : 1.1622546911239624
outputs_pos.loss : 1.282546043395996
outputs_pos.loss : 1.4116514921188354
outputs_pos.loss : 1.6755526065826416
Epoch 01481: adjusting learning rate of group 0 to 2.3671e-06.
outputs_pos.loss : 1.0382028818130493
outputs_pos.loss : 0.8930045366287231
outputs_pos.loss : 1.468291163444519
outputs_pos.loss : 1.3394323587417603
outputs_pos.loss : 1.1266207695007324
outputs_pos.loss : 1.3282798528671265
outputs_pos.loss : 1.8906121253967285
outputs_pos.loss : 1.3998717069625854
Epoch 01482: adjusting learning rate of group 0 to 2.3669e-06.
outputs_pos.loss : 1.1437052488327026
outputs_pos.loss : 1.070907711982727
outputs_pos.loss : 1.188014268875122
outputs_pos.loss : 1.1157153844833374
outputs_pos.loss : 1.0659390687942505
outputs_pos.loss : 1.2407366037368774
outputs_pos.loss : 1.1085690259933472
outputs_pos.loss : 0.9769845604896545
Epoch 01483: adjusting learning rate of group 0 to 2.3668e-06.
outputs_pos.loss : 1.5908869504928589
outputs_pos.loss : 0.8682228922843933
outputs_pos.loss : 1.4368689060211182
outputs_pos.loss : 1.0421693325042725
outputs_pos.loss : 1.0893795490264893
outputs_pos.loss : 1.77759850025177
outputs_pos.loss : 0.7357072830200195
outputs_pos.loss : 1.2250727415084839
Epoch 01484: adjusting learning rate of group 0 to 2.3666e-06.
outputs_pos.loss : 0.8899796605110168
outputs_pos.loss : 1.7297054529190063
outputs_pos.loss : 1.1118485927581787
outputs_pos.loss : 0.8835697174072266
outputs_pos.loss : 0.9181580543518066
outputs_pos.loss : 1.246720314025879
outputs_pos.loss : 1.3549903631210327
outputs_pos.loss : 0.8443301320075989
Epoch 01485: adjusting learning rate of group 0 to 2.3664e-06.
outputs_pos.loss : 1.0536463260650635
outputs_pos.loss : 1.1571298837661743
outputs_pos.loss : 1.3351696729660034
outputs_pos.loss : 0.7352258563041687
outputs_pos.loss : 1.4630483388900757
outputs_pos.loss : 1.0858911275863647
outputs_pos.loss : 0.6409593224525452
outputs_pos.loss : 1.1027284860610962
Epoch 01486: adjusting learning rate of group 0 to 2.3662e-06.
outputs_pos.loss : 1.2017790079116821
outputs_pos.loss : 0.8088834285736084
outputs_pos.loss : 1.7702078819274902
outputs_pos.loss : 0.6934449076652527
outputs_pos.loss : 1.032737374305725
outputs_pos.loss : 1.4116590023040771
outputs_pos.loss : 1.4986145496368408
outputs_pos.loss : 0.9304097890853882
Epoch 01487: adjusting learning rate of group 0 to 2.3661e-06.
outputs_pos.loss : 0.9638822078704834
outputs_pos.loss : 1.140283226966858
outputs_pos.loss : 0.6208572387695312
outputs_pos.loss : 1.259602427482605
outputs_pos.loss : 1.4038641452789307
outputs_pos.loss : 0.8322141766548157
outputs_pos.loss : 1.3491519689559937
outputs_pos.loss : 0.940963864326477
Epoch 01488: adjusting learning rate of group 0 to 2.3659e-06.
outputs_pos.loss : 0.9173241853713989
outputs_pos.loss : 1.461983323097229
outputs_pos.loss : 1.5098646879196167
outputs_pos.loss : 1.4065598249435425
outputs_pos.loss : 1.2716147899627686
outputs_pos.loss : 0.8950315713882446
outputs_pos.loss : 1.1125664710998535
outputs_pos.loss : 1.5063905715942383
Epoch 01489: adjusting learning rate of group 0 to 2.3657e-06.
outputs_pos.loss : 1.0819438695907593
outputs_pos.loss : 1.2262446880340576
outputs_pos.loss : 0.9144125580787659
outputs_pos.loss : 0.9326741695404053
outputs_pos.loss : 1.3432995080947876
outputs_pos.loss : 1.2799209356307983
outputs_pos.loss : 1.439283013343811
outputs_pos.loss : 0.9405705332756042
Epoch 01490: adjusting learning rate of group 0 to 2.3655e-06.
outputs_pos.loss : 1.0845636129379272
outputs_pos.loss : 0.8204272985458374
outputs_pos.loss : 1.267218828201294
outputs_pos.loss : 1.267499327659607
outputs_pos.loss : 0.9307210445404053
outputs_pos.loss : 1.3204957246780396
outputs_pos.loss : 1.1798896789550781
outputs_pos.loss : 1.239894151687622
Epoch 01491: adjusting learning rate of group 0 to 2.3654e-06.
outputs_pos.loss : 0.8135989308357239
outputs_pos.loss : 0.7608175277709961
outputs_pos.loss : 1.6070771217346191
outputs_pos.loss : 1.3247853517532349
outputs_pos.loss : 1.0015112161636353
outputs_pos.loss : 0.9324032664299011
outputs_pos.loss : 1.2718427181243896
outputs_pos.loss : 1.475163459777832
Epoch 01492: adjusting learning rate of group 0 to 2.3652e-06.
outputs_pos.loss : 1.087257981300354
outputs_pos.loss : 1.2367171049118042
outputs_pos.loss : 1.2586487531661987
outputs_pos.loss : 1.5779715776443481
outputs_pos.loss : 1.1288230419158936
outputs_pos.loss : 1.0389072895050049
outputs_pos.loss : 0.7435015439987183
outputs_pos.loss : 1.3027535676956177
Epoch 01493: adjusting learning rate of group 0 to 2.3650e-06.
outputs_pos.loss : 0.9401434659957886
outputs_pos.loss : 1.168420672416687
outputs_pos.loss : 1.0699762105941772
outputs_pos.loss : 1.374085545539856
outputs_pos.loss : 1.1182799339294434
outputs_pos.loss : 1.4516963958740234
outputs_pos.loss : 1.1445835828781128
outputs_pos.loss : 1.3832470178604126
Epoch 01494: adjusting learning rate of group 0 to 2.3648e-06.
outputs_pos.loss : 0.8069827556610107
outputs_pos.loss : 1.0121665000915527
outputs_pos.loss : 1.0084948539733887
outputs_pos.loss : 1.133985161781311
outputs_pos.loss : 1.1311901807785034
outputs_pos.loss : 1.2955541610717773
outputs_pos.loss : 1.3444732427597046
outputs_pos.loss : 1.0812851190567017
Epoch 01495: adjusting learning rate of group 0 to 2.3646e-06.
outputs_pos.loss : 1.1216379404067993
outputs_pos.loss : 0.710139811038971
outputs_pos.loss : 0.9310464262962341
outputs_pos.loss : 0.9619764685630798
outputs_pos.loss : 1.5518848896026611
outputs_pos.loss : 1.0704607963562012
outputs_pos.loss : 1.0891515016555786
outputs_pos.loss : 0.9388890862464905
Epoch 01496: adjusting learning rate of group 0 to 2.3645e-06.
outputs_pos.loss : 0.9218066334724426
outputs_pos.loss : 1.0840142965316772
outputs_pos.loss : 1.1463075876235962
outputs_pos.loss : 1.242805004119873
outputs_pos.loss : 1.125160813331604
outputs_pos.loss : 1.277632236480713
outputs_pos.loss : 1.4558539390563965
outputs_pos.loss : 1.1769864559173584
Epoch 01497: adjusting learning rate of group 0 to 2.3643e-06.
outputs_pos.loss : 0.9650388360023499
outputs_pos.loss : 1.26958429813385
outputs_pos.loss : 1.2502859830856323
outputs_pos.loss : 1.5864036083221436
outputs_pos.loss : 1.557826042175293
outputs_pos.loss : 0.902935802936554
outputs_pos.loss : 2.0111000537872314
outputs_pos.loss : 1.5175771713256836
Epoch 01498: adjusting learning rate of group 0 to 2.3641e-06.
outputs_pos.loss : 0.8164965510368347
outputs_pos.loss : 0.9067873954772949
outputs_pos.loss : 1.7303518056869507
outputs_pos.loss : 1.1705595254898071
outputs_pos.loss : 1.021888256072998
outputs_pos.loss : 1.0159070491790771
outputs_pos.loss : 1.3602238893508911
outputs_pos.loss : 0.8829519748687744
Epoch 01499: adjusting learning rate of group 0 to 2.3639e-06.
outputs_pos.loss : 1.9045735597610474
outputs_pos.loss : 1.3272457122802734
outputs_pos.loss : 1.2024496793746948
outputs_pos.loss : 1.0534603595733643
outputs_pos.loss : 1.4011791944503784
outputs_pos.loss : 1.157056450843811
outputs_pos.loss : 0.774712085723877
outputs_pos.loss : 1.6081560850143433
Epoch 01500: adjusting learning rate of group 0 to 2.3638e-06.
outputs_pos.loss : 1.2659343481063843
outputs_pos.loss : 1.4750148057937622
outputs_pos.loss : 1.1816387176513672
outputs_pos.loss : 0.8096831440925598
outputs_pos.loss : 1.623979091644287
outputs_pos.loss : 1.5707929134368896
outputs_pos.loss : 1.2566888332366943
outputs_pos.loss : 1.1753644943237305
Epoch 01501: adjusting learning rate of group 0 to 2.3636e-06.
outputs_pos.loss : 1.1947256326675415
outputs_pos.loss : 2.2684555053710938
outputs_pos.loss : 1.0914573669433594
outputs_pos.loss : 1.7962321043014526
outputs_pos.loss : 1.4461477994918823
outputs_pos.loss : 1.4582668542861938
outputs_pos.loss : 1.033052682876587
outputs_pos.loss : 0.8472376465797424
Epoch 01502: adjusting learning rate of group 0 to 2.3634e-06.
outputs_pos.loss : 1.8247287273406982
outputs_pos.loss : 1.109547734260559
outputs_pos.loss : 1.1305553913116455
outputs_pos.loss : 1.1338937282562256
outputs_pos.loss : 1.053221583366394
outputs_pos.loss : 2.196122407913208
outputs_pos.loss : 1.3773177862167358
outputs_pos.loss : 0.9527281522750854
Epoch 01503: adjusting learning rate of group 0 to 2.3632e-06.
outputs_pos.loss : 1.476800799369812
outputs_pos.loss : 0.8322502970695496
outputs_pos.loss : 0.971295177936554
outputs_pos.loss : 1.1636619567871094
outputs_pos.loss : 1.2842340469360352
outputs_pos.loss : 0.9763493537902832
outputs_pos.loss : 1.087485432624817
outputs_pos.loss : 0.8587228655815125
Epoch 01504: adjusting learning rate of group 0 to 2.3630e-06.
outputs_pos.loss : 0.9525195956230164
outputs_pos.loss : 0.9839187264442444
outputs_pos.loss : 1.492814540863037
outputs_pos.loss : 0.5620814561843872
outputs_pos.loss : 0.8679143786430359
outputs_pos.loss : 1.2624560594558716
outputs_pos.loss : 0.8939758539199829
outputs_pos.loss : 1.2194294929504395
Epoch 01505: adjusting learning rate of group 0 to 2.3629e-06.
outputs_pos.loss : 1.6757053136825562
outputs_pos.loss : 1.003761887550354
outputs_pos.loss : 0.9504535794258118
outputs_pos.loss : 1.2038122415542603
outputs_pos.loss : 0.9340205788612366
outputs_pos.loss : 1.2410035133361816
outputs_pos.loss : 1.320196509361267
outputs_pos.loss : 1.1641788482666016
Epoch 01506: adjusting learning rate of group 0 to 2.3627e-06.
outputs_pos.loss : 0.721367597579956
outputs_pos.loss : 1.1429795026779175
outputs_pos.loss : 1.4370949268341064
outputs_pos.loss : 1.327315092086792
outputs_pos.loss : 0.9351912140846252
outputs_pos.loss : 1.9604036808013916
outputs_pos.loss : 1.4620633125305176
outputs_pos.loss : 1.3692800998687744
Epoch 01507: adjusting learning rate of group 0 to 2.3625e-06.
outputs_pos.loss : 1.061885118484497
outputs_pos.loss : 0.6401112079620361
outputs_pos.loss : 1.1917550563812256
outputs_pos.loss : 0.8397629261016846
outputs_pos.loss : 1.5103569030761719
outputs_pos.loss : 1.3494588136672974
outputs_pos.loss : 1.052728295326233
outputs_pos.loss : 0.809476912021637
Epoch 01508: adjusting learning rate of group 0 to 2.3623e-06.
outputs_pos.loss : 1.1004281044006348
outputs_pos.loss : 1.225042462348938
outputs_pos.loss : 1.8024027347564697
outputs_pos.loss : 1.5003461837768555
outputs_pos.loss : 1.2892305850982666
outputs_pos.loss : 0.9425899982452393
outputs_pos.loss : 0.9255769848823547
outputs_pos.loss : 0.9824241399765015
Epoch 01509: adjusting learning rate of group 0 to 2.3621e-06.
outputs_pos.loss : 0.8387346863746643
outputs_pos.loss : 1.196326494216919
outputs_pos.loss : 1.0869847536087036
outputs_pos.loss : 1.307768702507019
outputs_pos.loss : 1.0018166303634644
outputs_pos.loss : 1.677716612815857
outputs_pos.loss : 1.0487807989120483
outputs_pos.loss : 1.0882014036178589
Epoch 01510: adjusting learning rate of group 0 to 2.3620e-06.
outputs_pos.loss : 1.376651406288147
outputs_pos.loss : 1.401178002357483
outputs_pos.loss : 1.2513542175292969
outputs_pos.loss : 1.236674427986145
outputs_pos.loss : 1.122243046760559
outputs_pos.loss : 1.2085121870040894
outputs_pos.loss : 1.0014406442642212
outputs_pos.loss : 1.548690915107727
Epoch 01511: adjusting learning rate of group 0 to 2.3618e-06.
outputs_pos.loss : 0.7556430697441101
outputs_pos.loss : 1.492687702178955
outputs_pos.loss : 1.3887969255447388
outputs_pos.loss : 0.9325296878814697
outputs_pos.loss : 1.4128113985061646
outputs_pos.loss : 0.785611629486084
outputs_pos.loss : 1.8956588506698608
outputs_pos.loss : 0.9281228184700012
Epoch 01512: adjusting learning rate of group 0 to 2.3616e-06.
outputs_pos.loss : 1.1855816841125488
outputs_pos.loss : 1.3978257179260254
outputs_pos.loss : 0.8306695222854614
outputs_pos.loss : 1.5612387657165527
outputs_pos.loss : 1.5003143548965454
outputs_pos.loss : 0.929022490978241
outputs_pos.loss : 1.2817624807357788
outputs_pos.loss : 1.1237618923187256
Epoch 01513: adjusting learning rate of group 0 to 2.3614e-06.
outputs_pos.loss : 1.2633014917373657
outputs_pos.loss : 1.3891983032226562
outputs_pos.loss : 1.382483959197998
outputs_pos.loss : 1.0205931663513184
outputs_pos.loss : 1.3751792907714844
outputs_pos.loss : 1.1620417833328247
outputs_pos.loss : 1.3840652704238892
outputs_pos.loss : 0.9316133260726929
Epoch 01514: adjusting learning rate of group 0 to 2.3613e-06.
outputs_pos.loss : 1.8870267868041992
outputs_pos.loss : 1.2587345838546753
outputs_pos.loss : 1.0370903015136719
outputs_pos.loss : 0.9436718821525574
outputs_pos.loss : 1.4216368198394775
outputs_pos.loss : 1.5505152940750122
outputs_pos.loss : 1.392318844795227
outputs_pos.loss : 0.9031362533569336
Epoch 01515: adjusting learning rate of group 0 to 2.3611e-06.
outputs_pos.loss : 1.2352381944656372
outputs_pos.loss : 1.1827186346054077
outputs_pos.loss : 1.379037618637085
outputs_pos.loss : 1.299595832824707
outputs_pos.loss : 1.2177733182907104
outputs_pos.loss : 1.2216291427612305
outputs_pos.loss : 1.5024232864379883
outputs_pos.loss : 1.0768649578094482
Epoch 01516: adjusting learning rate of group 0 to 2.3609e-06.
outputs_pos.loss : 0.8342901468276978
outputs_pos.loss : 1.4735870361328125
outputs_pos.loss : 1.1649893522262573
outputs_pos.loss : 0.9804519414901733
outputs_pos.loss : 0.9865193963050842
outputs_pos.loss : 1.2512315511703491
outputs_pos.loss : 0.9148293733596802
outputs_pos.loss : 1.2620797157287598
Epoch 01517: adjusting learning rate of group 0 to 2.3607e-06.
outputs_pos.loss : 1.1851720809936523
outputs_pos.loss : 1.6733174324035645
outputs_pos.loss : 0.982254683971405
outputs_pos.loss : 1.3270976543426514
outputs_pos.loss : 0.9512459635734558
outputs_pos.loss : 0.9082842469215393
outputs_pos.loss : 1.150260329246521
outputs_pos.loss : 1.0729533433914185
Epoch 01518: adjusting learning rate of group 0 to 2.3605e-06.
outputs_pos.loss : 1.165789008140564
outputs_pos.loss : 1.3368698358535767
outputs_pos.loss : 0.9071657061576843
outputs_pos.loss : 0.8801841735839844
outputs_pos.loss : 1.1122838258743286
outputs_pos.loss : 1.3538919687271118
outputs_pos.loss : 1.2534111738204956
outputs_pos.loss : 1.6681023836135864
Epoch 01519: adjusting learning rate of group 0 to 2.3604e-06.
outputs_pos.loss : 1.5113779306411743
outputs_pos.loss : 1.26210355758667
outputs_pos.loss : 1.1426266431808472
outputs_pos.loss : 1.3509840965270996
outputs_pos.loss : 1.1016665697097778
outputs_pos.loss : 1.0101416110992432
outputs_pos.loss : 1.3364607095718384
outputs_pos.loss : 1.1681263446807861
Epoch 01520: adjusting learning rate of group 0 to 2.3602e-06.
outputs_pos.loss : 1.4027189016342163
outputs_pos.loss : 1.0667189359664917
outputs_pos.loss : 1.1090092658996582
outputs_pos.loss : 1.1244066953659058
outputs_pos.loss : 1.19247305393219
outputs_pos.loss : 1.2662856578826904
outputs_pos.loss : 1.029031753540039
outputs_pos.loss : 1.0715054273605347
Epoch 01521: adjusting learning rate of group 0 to 2.3600e-06.
outputs_pos.loss : 1.3616044521331787
outputs_pos.loss : 0.8879485130310059
outputs_pos.loss : 1.2642604112625122
outputs_pos.loss : 1.1368205547332764
outputs_pos.loss : 1.209362506866455
outputs_pos.loss : 1.30267333984375
outputs_pos.loss : 0.9386199712753296
outputs_pos.loss : 1.1118348836898804
Epoch 01522: adjusting learning rate of group 0 to 2.3598e-06.
outputs_pos.loss : 1.2820870876312256
outputs_pos.loss : 0.7874481081962585
outputs_pos.loss : 1.025592565536499
outputs_pos.loss : 0.9970503449440002
outputs_pos.loss : 1.0438847541809082
outputs_pos.loss : 1.1770533323287964
outputs_pos.loss : 1.1895663738250732
outputs_pos.loss : 0.9752873182296753
Epoch 01523: adjusting learning rate of group 0 to 2.3596e-06.
outputs_pos.loss : 1.2553316354751587
outputs_pos.loss : 0.7707037925720215
outputs_pos.loss : 1.4207379817962646
outputs_pos.loss : 1.0468782186508179
outputs_pos.loss : 1.2864168882369995
outputs_pos.loss : 1.2988067865371704
outputs_pos.loss : 0.8670319318771362
outputs_pos.loss : 1.1879912614822388
Epoch 01524: adjusting learning rate of group 0 to 2.3594e-06.
outputs_pos.loss : 1.2567490339279175
outputs_pos.loss : 1.3251371383666992
outputs_pos.loss : 1.210392713546753
outputs_pos.loss : 1.133735179901123
outputs_pos.loss : 1.0174444913864136
outputs_pos.loss : 1.3926750421524048
outputs_pos.loss : 1.1662449836730957
outputs_pos.loss : 1.3607486486434937
Epoch 01525: adjusting learning rate of group 0 to 2.3593e-06.
outputs_pos.loss : 1.4221357107162476
outputs_pos.loss : 1.1505615711212158
outputs_pos.loss : 1.1221307516098022
outputs_pos.loss : 1.283075213432312
outputs_pos.loss : 1.445482611656189
outputs_pos.loss : 1.5635274648666382
outputs_pos.loss : 1.4046218395233154
outputs_pos.loss : 1.0346685647964478
Epoch 01526: adjusting learning rate of group 0 to 2.3591e-06.
outputs_pos.loss : 1.3476659059524536
outputs_pos.loss : 1.4056614637374878
outputs_pos.loss : 1.2529056072235107
outputs_pos.loss : 1.4498636722564697
outputs_pos.loss : 0.9275830388069153
outputs_pos.loss : 1.1958891153335571
outputs_pos.loss : 1.5369534492492676
outputs_pos.loss : 1.04323410987854
Epoch 01527: adjusting learning rate of group 0 to 2.3589e-06.
outputs_pos.loss : 1.1027449369430542
outputs_pos.loss : 1.4834613800048828
outputs_pos.loss : 1.0462536811828613
outputs_pos.loss : 1.039909839630127
outputs_pos.loss : 1.257209300994873
outputs_pos.loss : 0.7827786207199097
outputs_pos.loss : 1.2180095911026
outputs_pos.loss : 1.0235366821289062
Epoch 01528: adjusting learning rate of group 0 to 2.3587e-06.
outputs_pos.loss : 0.9846833944320679
outputs_pos.loss : 1.0091391801834106
outputs_pos.loss : 1.2740386724472046
outputs_pos.loss : 1.0014230012893677
outputs_pos.loss : 0.9440287351608276
outputs_pos.loss : 1.234561562538147
outputs_pos.loss : 0.9040306806564331
outputs_pos.loss : 1.001281499862671
Epoch 01529: adjusting learning rate of group 0 to 2.3585e-06.
outputs_pos.loss : 1.2032248973846436
outputs_pos.loss : 0.886925995349884
outputs_pos.loss : 1.1317965984344482
outputs_pos.loss : 0.7481858730316162
outputs_pos.loss : 1.6833035945892334
outputs_pos.loss : 1.7899872064590454
outputs_pos.loss : 0.9750762581825256
outputs_pos.loss : 1.062737226486206
Epoch 01530: adjusting learning rate of group 0 to 2.3584e-06.
outputs_pos.loss : 1.2507272958755493
outputs_pos.loss : 0.9171196818351746
outputs_pos.loss : 1.292351484298706
outputs_pos.loss : 1.3152027130126953
outputs_pos.loss : 1.2867660522460938
outputs_pos.loss : 1.4262670278549194
outputs_pos.loss : 1.029792070388794
outputs_pos.loss : 0.9653180837631226
Epoch 01531: adjusting learning rate of group 0 to 2.3582e-06.
outputs_pos.loss : 1.4624080657958984
outputs_pos.loss : 0.6690283417701721
outputs_pos.loss : 1.1286070346832275
outputs_pos.loss : 0.77545565366745
outputs_pos.loss : 1.3869214057922363
outputs_pos.loss : 0.8931853771209717
outputs_pos.loss : 1.4204998016357422
outputs_pos.loss : 1.2460706233978271
Epoch 01532: adjusting learning rate of group 0 to 2.3580e-06.
outputs_pos.loss : 1.3736292123794556
outputs_pos.loss : 1.2345433235168457
outputs_pos.loss : 1.127149224281311
outputs_pos.loss : 1.0381231307983398
outputs_pos.loss : 1.0688527822494507
outputs_pos.loss : 0.8192306160926819
outputs_pos.loss : 0.8661690354347229
outputs_pos.loss : 1.0894883871078491
Epoch 01533: adjusting learning rate of group 0 to 2.3578e-06.
outputs_pos.loss : 1.3349690437316895
outputs_pos.loss : 1.2318207025527954
outputs_pos.loss : 0.8506327867507935
outputs_pos.loss : 0.631045937538147
outputs_pos.loss : 0.9418221712112427
outputs_pos.loss : 0.9493222236633301
outputs_pos.loss : 1.333541750907898
outputs_pos.loss : 1.1216074228286743
Epoch 01534: adjusting learning rate of group 0 to 2.3576e-06.
outputs_pos.loss : 1.2972344160079956
outputs_pos.loss : 0.8910004496574402
outputs_pos.loss : 1.0215002298355103
outputs_pos.loss : 0.7686493396759033
outputs_pos.loss : 0.8971273303031921
outputs_pos.loss : 0.9762808084487915
outputs_pos.loss : 0.8698248267173767
outputs_pos.loss : 1.0759762525558472
Epoch 01535: adjusting learning rate of group 0 to 2.3575e-06.
outputs_pos.loss : 1.342413306236267
outputs_pos.loss : 0.7538445591926575
outputs_pos.loss : 0.8128253221511841
outputs_pos.loss : 1.018751621246338
outputs_pos.loss : 1.0140002965927124
outputs_pos.loss : 1.2281006574630737
outputs_pos.loss : 1.1326830387115479
outputs_pos.loss : 1.1271677017211914
Epoch 01536: adjusting learning rate of group 0 to 2.3573e-06.
outputs_pos.loss : 0.7684590220451355
outputs_pos.loss : 1.1373018026351929
outputs_pos.loss : 1.0150715112686157
outputs_pos.loss : 0.9746057391166687
outputs_pos.loss : 1.212079644203186
outputs_pos.loss : 1.0053763389587402
outputs_pos.loss : 0.6471436023712158
outputs_pos.loss : 1.7549481391906738
Epoch 01537: adjusting learning rate of group 0 to 2.3571e-06.
outputs_pos.loss : 1.1169087886810303
outputs_pos.loss : 1.7713507413864136
outputs_pos.loss : 0.5973933935165405
outputs_pos.loss : 0.9577263593673706
outputs_pos.loss : 0.9309280514717102
outputs_pos.loss : 1.302239179611206
outputs_pos.loss : 1.7848962545394897
outputs_pos.loss : 1.2379730939865112
Epoch 01538: adjusting learning rate of group 0 to 2.3569e-06.
outputs_pos.loss : 1.597369909286499
outputs_pos.loss : 1.1799684762954712
outputs_pos.loss : 1.06343412399292
outputs_pos.loss : 1.0283491611480713
outputs_pos.loss : 0.9095852971076965
outputs_pos.loss : 0.804726779460907
outputs_pos.loss : 1.3863956928253174
outputs_pos.loss : 1.1259034872055054
Epoch 01539: adjusting learning rate of group 0 to 2.3567e-06.
outputs_pos.loss : 0.8409792184829712
outputs_pos.loss : 1.3601911067962646
outputs_pos.loss : 1.3152344226837158
outputs_pos.loss : 0.7395886182785034
outputs_pos.loss : 0.9118623733520508
outputs_pos.loss : 0.7619672417640686
outputs_pos.loss : 1.085073709487915
outputs_pos.loss : 1.1483157873153687
Epoch 01540: adjusting learning rate of group 0 to 2.3565e-06.
outputs_pos.loss : 1.1504030227661133
outputs_pos.loss : 1.1034095287322998
outputs_pos.loss : 1.8875352144241333
outputs_pos.loss : 1.0318235158920288
outputs_pos.loss : 0.860937774181366
outputs_pos.loss : 1.1811290979385376
outputs_pos.loss : 1.2422069311141968
outputs_pos.loss : 1.043776512145996
Epoch 01541: adjusting learning rate of group 0 to 2.3564e-06.
outputs_pos.loss : 1.475411057472229
outputs_pos.loss : 0.8850900530815125
outputs_pos.loss : 0.835455060005188
outputs_pos.loss : 0.8697404265403748
outputs_pos.loss : 0.9145113825798035
outputs_pos.loss : 1.447892427444458
outputs_pos.loss : 1.0189110040664673
outputs_pos.loss : 0.9119080305099487
Epoch 01542: adjusting learning rate of group 0 to 2.3562e-06.
outputs_pos.loss : 0.7923825979232788
outputs_pos.loss : 1.2828720808029175
outputs_pos.loss : 1.1680090427398682
outputs_pos.loss : 0.8920638561248779
outputs_pos.loss : 1.0357457399368286
outputs_pos.loss : 1.2786695957183838
outputs_pos.loss : 1.025547742843628
outputs_pos.loss : 1.565456509590149
Epoch 01543: adjusting learning rate of group 0 to 2.3560e-06.
outputs_pos.loss : 0.9367163777351379
outputs_pos.loss : 1.0459283590316772
outputs_pos.loss : 1.4139635562896729
outputs_pos.loss : 0.7924864292144775
outputs_pos.loss : 0.8720955848693848
outputs_pos.loss : 1.2763217687606812
outputs_pos.loss : 1.14878511428833
outputs_pos.loss : 1.1423115730285645
Epoch 01544: adjusting learning rate of group 0 to 2.3558e-06.
outputs_pos.loss : 1.0141984224319458
outputs_pos.loss : 1.5511499643325806
outputs_pos.loss : 0.8519420027732849
outputs_pos.loss : 1.2742395401000977
outputs_pos.loss : 1.2579938173294067
outputs_pos.loss : 1.1106491088867188
outputs_pos.loss : 0.7610119581222534
outputs_pos.loss : 1.0362260341644287
Epoch 01545: adjusting learning rate of group 0 to 2.3556e-06.
outputs_pos.loss : 1.0941882133483887
outputs_pos.loss : 1.0823997259140015
outputs_pos.loss : 1.0670655965805054
outputs_pos.loss : 1.127259612083435
outputs_pos.loss : 0.8939520716667175
outputs_pos.loss : 1.0565717220306396
outputs_pos.loss : 1.3255198001861572
outputs_pos.loss : 0.8827922940254211
Epoch 01546: adjusting learning rate of group 0 to 2.3554e-06.
outputs_pos.loss : 1.2440537214279175
outputs_pos.loss : 1.2163429260253906
outputs_pos.loss : 1.073962688446045
outputs_pos.loss : 0.8606351613998413
outputs_pos.loss : 1.1873339414596558
outputs_pos.loss : 1.4344314336776733
outputs_pos.loss : 1.15811288356781
outputs_pos.loss : 1.48391854763031
Epoch 01547: adjusting learning rate of group 0 to 2.3553e-06.
outputs_pos.loss : 1.318098783493042
outputs_pos.loss : 1.1636431217193604
outputs_pos.loss : 1.4428867101669312
outputs_pos.loss : 1.0008126497268677
outputs_pos.loss : 0.8800535202026367
outputs_pos.loss : 1.1685103178024292
outputs_pos.loss : 0.910250723361969
outputs_pos.loss : 0.9978004693984985
Epoch 01548: adjusting learning rate of group 0 to 2.3551e-06.
outputs_pos.loss : 0.8812341690063477
outputs_pos.loss : 1.274675965309143
outputs_pos.loss : 1.143542766571045
outputs_pos.loss : 0.8270654678344727
outputs_pos.loss : 0.9669956564903259
outputs_pos.loss : 0.932812511920929
outputs_pos.loss : 1.3282042741775513
outputs_pos.loss : 1.0678892135620117
Epoch 01549: adjusting learning rate of group 0 to 2.3549e-06.
outputs_pos.loss : 1.0393283367156982
outputs_pos.loss : 1.0230435132980347
outputs_pos.loss : 1.511365532875061
outputs_pos.loss : 1.2772127389907837
outputs_pos.loss : 0.8714783787727356
outputs_pos.loss : 0.7064786553382874
outputs_pos.loss : 1.0301655530929565
outputs_pos.loss : 1.2308253049850464
Epoch 01550: adjusting learning rate of group 0 to 2.3547e-06.
outputs_pos.loss : 1.1342030763626099
outputs_pos.loss : 1.1432464122772217
outputs_pos.loss : 1.0740382671356201
outputs_pos.loss : 1.296702265739441
outputs_pos.loss : 1.3460595607757568
outputs_pos.loss : 1.641463041305542
outputs_pos.loss : 1.251944661140442
outputs_pos.loss : 1.2149736881256104
Epoch 01551: adjusting learning rate of group 0 to 2.3545e-06.
outputs_pos.loss : 0.9712852835655212
outputs_pos.loss : 1.3794894218444824
outputs_pos.loss : 1.2274190187454224
outputs_pos.loss : 0.6238749027252197
outputs_pos.loss : 1.5478895902633667
outputs_pos.loss : 0.9136398434638977
outputs_pos.loss : 1.7023650407791138
outputs_pos.loss : 0.918628454208374
Epoch 01552: adjusting learning rate of group 0 to 2.3543e-06.
outputs_pos.loss : 1.0660810470581055
outputs_pos.loss : 0.9379107356071472
outputs_pos.loss : 1.5521278381347656
outputs_pos.loss : 0.8479686379432678
outputs_pos.loss : 1.3440062999725342
outputs_pos.loss : 1.1082690954208374
outputs_pos.loss : 1.0517383813858032
outputs_pos.loss : 1.2067137956619263
Epoch 01553: adjusting learning rate of group 0 to 2.3542e-06.
outputs_pos.loss : 1.5557913780212402
outputs_pos.loss : 1.024769902229309
outputs_pos.loss : 1.2402317523956299
outputs_pos.loss : 1.6159793138504028
outputs_pos.loss : 1.5596011877059937
outputs_pos.loss : 1.1195029020309448
outputs_pos.loss : 1.0855324268341064
outputs_pos.loss : 0.9980523586273193
Epoch 01554: adjusting learning rate of group 0 to 2.3540e-06.
outputs_pos.loss : 0.8292618989944458
outputs_pos.loss : 0.9248237013816833
outputs_pos.loss : 1.588544249534607
outputs_pos.loss : 0.8767773509025574
outputs_pos.loss : 1.2440283298492432
outputs_pos.loss : 1.4220805168151855
outputs_pos.loss : 0.9885169267654419
outputs_pos.loss : 0.8047928214073181
Epoch 01555: adjusting learning rate of group 0 to 2.3538e-06.
outputs_pos.loss : 1.5471035242080688
outputs_pos.loss : 1.1302460432052612
outputs_pos.loss : 1.4318134784698486
outputs_pos.loss : 1.2354092597961426
outputs_pos.loss : 0.6494234204292297
outputs_pos.loss : 0.8357551097869873
outputs_pos.loss : 1.4655705690383911
outputs_pos.loss : 1.0260429382324219
Epoch 01556: adjusting learning rate of group 0 to 2.3536e-06.
outputs_pos.loss : 1.0375348329544067
outputs_pos.loss : 1.418306589126587
outputs_pos.loss : 0.8636175990104675
outputs_pos.loss : 0.7924105525016785
outputs_pos.loss : 1.0387942790985107
outputs_pos.loss : 2.146219491958618
outputs_pos.loss : 0.912546694278717
outputs_pos.loss : 1.4590078592300415
Epoch 01557: adjusting learning rate of group 0 to 2.3534e-06.
outputs_pos.loss : 1.0143450498580933
outputs_pos.loss : 0.6392301917076111
outputs_pos.loss : 1.685083270072937
outputs_pos.loss : 1.1401634216308594
outputs_pos.loss : 1.1698946952819824
outputs_pos.loss : 1.282433271408081
outputs_pos.loss : 0.8619015216827393
outputs_pos.loss : 1.2800341844558716
Epoch 01558: adjusting learning rate of group 0 to 2.3532e-06.
outputs_pos.loss : 1.3630026578903198
outputs_pos.loss : 1.3983700275421143
outputs_pos.loss : 1.5747802257537842
outputs_pos.loss : 1.0162867307662964
outputs_pos.loss : 1.3126189708709717
outputs_pos.loss : 0.7545911073684692
outputs_pos.loss : 1.1053322553634644
outputs_pos.loss : 1.44127357006073
Epoch 01559: adjusting learning rate of group 0 to 2.3530e-06.
outputs_pos.loss : 1.0886136293411255
outputs_pos.loss : 1.176561713218689
outputs_pos.loss : 0.9864169359207153
outputs_pos.loss : 0.905920147895813
outputs_pos.loss : 1.149701714515686
outputs_pos.loss : 1.1879788637161255
outputs_pos.loss : 1.0699373483657837
outputs_pos.loss : 0.8714653253555298
Epoch 01560: adjusting learning rate of group 0 to 2.3529e-06.
outputs_pos.loss : 1.318240761756897
outputs_pos.loss : 0.8972620964050293
outputs_pos.loss : 1.4312728643417358
outputs_pos.loss : 1.1417354345321655
outputs_pos.loss : 0.8812164664268494
outputs_pos.loss : 1.153159737586975
outputs_pos.loss : 1.2086610794067383
outputs_pos.loss : 1.1574327945709229
Epoch 01561: adjusting learning rate of group 0 to 2.3527e-06.
outputs_pos.loss : 1.0942606925964355
outputs_pos.loss : 1.0785019397735596
outputs_pos.loss : 0.8569452166557312
outputs_pos.loss : 1.0430750846862793
outputs_pos.loss : 1.2050772905349731
outputs_pos.loss : 1.1073137521743774
outputs_pos.loss : 1.7040784358978271
outputs_pos.loss : 1.2518666982650757
Epoch 01562: adjusting learning rate of group 0 to 2.3525e-06.
outputs_pos.loss : 1.4105567932128906
outputs_pos.loss : 0.8644911050796509
outputs_pos.loss : 1.225245714187622
outputs_pos.loss : 1.0987067222595215
outputs_pos.loss : 1.1203839778900146
outputs_pos.loss : 1.202155590057373
outputs_pos.loss : 1.0086933374404907
outputs_pos.loss : 1.0043275356292725
Epoch 01563: adjusting learning rate of group 0 to 2.3523e-06.
outputs_pos.loss : 1.2097433805465698
outputs_pos.loss : 0.7222899198532104
outputs_pos.loss : 1.2547427415847778
outputs_pos.loss : 1.282243013381958
outputs_pos.loss : 1.0591179132461548
outputs_pos.loss : 1.4111838340759277
outputs_pos.loss : 1.9756815433502197
outputs_pos.loss : 1.2324624061584473
Epoch 01564: adjusting learning rate of group 0 to 2.3521e-06.
outputs_pos.loss : 1.727797269821167
outputs_pos.loss : 0.9687392711639404
outputs_pos.loss : 1.1292378902435303
outputs_pos.loss : 1.0896443128585815
outputs_pos.loss : 0.7868745923042297
outputs_pos.loss : 1.18977952003479
outputs_pos.loss : 1.165080189704895
outputs_pos.loss : 1.2267597913742065
Epoch 01565: adjusting learning rate of group 0 to 2.3519e-06.
outputs_pos.loss : 1.789031744003296
outputs_pos.loss : 1.306796669960022
outputs_pos.loss : 1.3102999925613403
outputs_pos.loss : 0.8052374720573425
outputs_pos.loss : 1.4863253831863403
outputs_pos.loss : 0.8499071002006531
outputs_pos.loss : 0.9188222289085388
outputs_pos.loss : 1.0249933004379272
Epoch 01566: adjusting learning rate of group 0 to 2.3518e-06.
outputs_pos.loss : 1.368687629699707
outputs_pos.loss : 1.0347483158111572
outputs_pos.loss : 1.1480867862701416
outputs_pos.loss : 1.1445754766464233
outputs_pos.loss : 0.6838818788528442
outputs_pos.loss : 1.051077127456665
outputs_pos.loss : 1.0162441730499268
outputs_pos.loss : 1.422676920890808
Epoch 01567: adjusting learning rate of group 0 to 2.3516e-06.
outputs_pos.loss : 1.0406651496887207
outputs_pos.loss : 1.4613864421844482
outputs_pos.loss : 1.463472604751587
outputs_pos.loss : 1.054221510887146
outputs_pos.loss : 1.1119513511657715
outputs_pos.loss : 0.44648709893226624
outputs_pos.loss : 1.0765891075134277
outputs_pos.loss : 1.0519585609436035
Epoch 01568: adjusting learning rate of group 0 to 2.3514e-06.
outputs_pos.loss : 1.4593976736068726
outputs_pos.loss : 1.214365839958191
outputs_pos.loss : 1.7159457206726074
outputs_pos.loss : 1.4649381637573242
outputs_pos.loss : 0.7625141739845276
outputs_pos.loss : 1.2898893356323242
outputs_pos.loss : 0.7793428301811218
outputs_pos.loss : 1.1819796562194824
Epoch 01569: adjusting learning rate of group 0 to 2.3512e-06.
outputs_pos.loss : 0.8783990144729614
outputs_pos.loss : 1.0357619524002075
outputs_pos.loss : 1.4734371900558472
outputs_pos.loss : 0.9900693297386169
outputs_pos.loss : 1.1632764339447021
outputs_pos.loss : 1.2932615280151367
outputs_pos.loss : 1.0103578567504883
outputs_pos.loss : 1.0884329080581665
Epoch 01570: adjusting learning rate of group 0 to 2.3510e-06.
outputs_pos.loss : 1.4641886949539185
outputs_pos.loss : 1.0032672882080078
outputs_pos.loss : 1.3179905414581299
outputs_pos.loss : 0.7765486240386963
outputs_pos.loss : 1.1549335718154907
outputs_pos.loss : 1.523221492767334
outputs_pos.loss : 1.13935124874115
outputs_pos.loss : 1.054509162902832
Epoch 01571: adjusting learning rate of group 0 to 2.3508e-06.
outputs_pos.loss : 2.1479249000549316
outputs_pos.loss : 0.9731242656707764
outputs_pos.loss : 1.6548513174057007
outputs_pos.loss : 1.1031638383865356
outputs_pos.loss : 1.1848104000091553
outputs_pos.loss : 1.2440370321273804
outputs_pos.loss : 0.9726517200469971
outputs_pos.loss : 1.5229591131210327
Epoch 01572: adjusting learning rate of group 0 to 2.3506e-06.
outputs_pos.loss : 1.0796072483062744
outputs_pos.loss : 1.3112003803253174
outputs_pos.loss : 0.788675844669342
outputs_pos.loss : 1.0820608139038086
outputs_pos.loss : 1.3474175930023193
outputs_pos.loss : 1.2811012268066406
outputs_pos.loss : 0.7111462950706482
outputs_pos.loss : 1.217172622680664
Epoch 01573: adjusting learning rate of group 0 to 2.3505e-06.
outputs_pos.loss : 0.8482629060745239
outputs_pos.loss : 2.265465021133423
outputs_pos.loss : 0.9731913805007935
outputs_pos.loss : 1.442447543144226
outputs_pos.loss : 1.9003721475601196
outputs_pos.loss : 0.8625034093856812
outputs_pos.loss : 1.0848392248153687
outputs_pos.loss : 1.9176702499389648
Epoch 01574: adjusting learning rate of group 0 to 2.3503e-06.
outputs_pos.loss : 1.0927942991256714
outputs_pos.loss : 1.1425247192382812
outputs_pos.loss : 1.0442204475402832
outputs_pos.loss : 0.8549638390541077
outputs_pos.loss : 1.0772732496261597
outputs_pos.loss : 1.4063197374343872
outputs_pos.loss : 1.0549941062927246
outputs_pos.loss : 0.9553931355476379
Epoch 01575: adjusting learning rate of group 0 to 2.3501e-06.
outputs_pos.loss : 0.7301811575889587
outputs_pos.loss : 1.10479736328125
outputs_pos.loss : 1.0184354782104492
outputs_pos.loss : 1.320693850517273
outputs_pos.loss : 0.5297548174858093
outputs_pos.loss : 0.8829176425933838
outputs_pos.loss : 1.7756720781326294
outputs_pos.loss : 1.2987529039382935
Epoch 01576: adjusting learning rate of group 0 to 2.3499e-06.
outputs_pos.loss : 1.5785179138183594
outputs_pos.loss : 1.0148427486419678
outputs_pos.loss : 1.0522874593734741
outputs_pos.loss : 1.5341354608535767
outputs_pos.loss : 1.3670929670333862
outputs_pos.loss : 1.080152988433838
outputs_pos.loss : 1.2547229528427124
outputs_pos.loss : 1.0452839136123657
Epoch 01577: adjusting learning rate of group 0 to 2.3497e-06.
outputs_pos.loss : 1.1162009239196777
outputs_pos.loss : 0.8509773015975952
outputs_pos.loss : 1.0103225708007812
outputs_pos.loss : 1.4843451976776123
outputs_pos.loss : 1.295121669769287
outputs_pos.loss : 1.35941743850708
outputs_pos.loss : 1.1751385927200317
outputs_pos.loss : 0.9664928317070007
Epoch 01578: adjusting learning rate of group 0 to 2.3495e-06.
outputs_pos.loss : 0.575813889503479
outputs_pos.loss : 0.784622311592102
outputs_pos.loss : 0.9859728217124939
outputs_pos.loss : 1.4086079597473145
outputs_pos.loss : 0.8771442770957947
outputs_pos.loss : 0.650885820388794
outputs_pos.loss : 1.2797738313674927
outputs_pos.loss : 1.8237011432647705
Epoch 01579: adjusting learning rate of group 0 to 2.3493e-06.
outputs_pos.loss : 1.0526487827301025
outputs_pos.loss : 0.8318100571632385
outputs_pos.loss : 1.1965899467468262
outputs_pos.loss : 1.512992024421692
outputs_pos.loss : 0.9145371913909912
outputs_pos.loss : 1.3390346765518188
outputs_pos.loss : 1.1883097887039185
outputs_pos.loss : 1.0252546072006226
Epoch 01580: adjusting learning rate of group 0 to 2.3491e-06.
outputs_pos.loss : 1.2569018602371216
outputs_pos.loss : 0.8447932004928589
outputs_pos.loss : 1.0068103075027466
outputs_pos.loss : 0.7037795186042786
outputs_pos.loss : 1.433056116104126
outputs_pos.loss : 0.8476847410202026
outputs_pos.loss : 1.1271107196807861
outputs_pos.loss : 1.3681808710098267
Epoch 01581: adjusting learning rate of group 0 to 2.3490e-06.
outputs_pos.loss : 1.7395561933517456
outputs_pos.loss : 1.427083134651184
outputs_pos.loss : 0.9800575971603394
outputs_pos.loss : 1.0713269710540771
outputs_pos.loss : 1.284138560295105
outputs_pos.loss : 1.320857286453247
outputs_pos.loss : 1.100876808166504
outputs_pos.loss : 0.8267098665237427
Epoch 01582: adjusting learning rate of group 0 to 2.3488e-06.
outputs_pos.loss : 1.6224029064178467
outputs_pos.loss : 1.2966699600219727
outputs_pos.loss : 1.05450439453125
outputs_pos.loss : 0.7704665660858154
outputs_pos.loss : 1.257793664932251
outputs_pos.loss : 1.0775383710861206
outputs_pos.loss : 1.2892500162124634
outputs_pos.loss : 0.8677485585212708
Epoch 01583: adjusting learning rate of group 0 to 2.3486e-06.
outputs_pos.loss : 1.0441428422927856
outputs_pos.loss : 1.219591498374939
outputs_pos.loss : 1.3108233213424683
outputs_pos.loss : 0.9342873096466064
outputs_pos.loss : 1.3629941940307617
outputs_pos.loss : 0.9316515922546387
outputs_pos.loss : 1.2869606018066406
outputs_pos.loss : 1.143502116203308
Epoch 01584: adjusting learning rate of group 0 to 2.3484e-06.
outputs_pos.loss : 1.6924490928649902
outputs_pos.loss : 1.2646892070770264
outputs_pos.loss : 1.1277507543563843
outputs_pos.loss : 1.2890979051589966
outputs_pos.loss : 2.109544038772583
outputs_pos.loss : 1.364090085029602
outputs_pos.loss : 1.0855371952056885
outputs_pos.loss : 1.2557777166366577
Epoch 01585: adjusting learning rate of group 0 to 2.3482e-06.
outputs_pos.loss : 0.8823883533477783
outputs_pos.loss : 1.130339503288269
outputs_pos.loss : 1.7835171222686768
outputs_pos.loss : 0.7144198417663574
outputs_pos.loss : 1.0371227264404297
outputs_pos.loss : 1.0715036392211914
outputs_pos.loss : 1.1465826034545898
outputs_pos.loss : 1.3874928951263428
Epoch 01586: adjusting learning rate of group 0 to 2.3480e-06.
outputs_pos.loss : 1.1123765707015991
outputs_pos.loss : 1.2193130254745483
outputs_pos.loss : 1.4980500936508179
outputs_pos.loss : 1.2335158586502075
outputs_pos.loss : 1.663008213043213
outputs_pos.loss : 0.9678397178649902
outputs_pos.loss : 1.091735601425171
outputs_pos.loss : 1.3327466249465942
Epoch 01587: adjusting learning rate of group 0 to 2.3478e-06.
outputs_pos.loss : 1.3382216691970825
outputs_pos.loss : 1.3009353876113892
outputs_pos.loss : 0.959324300289154
outputs_pos.loss : 1.5573878288269043
outputs_pos.loss : 1.352379560470581
outputs_pos.loss : 0.9267842173576355
outputs_pos.loss : 0.9759702086448669
outputs_pos.loss : 1.6308554410934448
Epoch 01588: adjusting learning rate of group 0 to 2.3476e-06.
outputs_pos.loss : 1.0221925973892212
outputs_pos.loss : 0.9801599979400635
outputs_pos.loss : 1.0937318801879883
outputs_pos.loss : 1.3001736402511597
outputs_pos.loss : 1.382707118988037
outputs_pos.loss : 0.9315081238746643
outputs_pos.loss : 1.0797725915908813
outputs_pos.loss : 1.2701759338378906
Epoch 01589: adjusting learning rate of group 0 to 2.3475e-06.
outputs_pos.loss : 1.4105173349380493
outputs_pos.loss : 0.7887090444564819
outputs_pos.loss : 1.1365065574645996
outputs_pos.loss : 0.6767371892929077
outputs_pos.loss : 1.0467007160186768
outputs_pos.loss : 0.9284022450447083
outputs_pos.loss : 1.0328785181045532
outputs_pos.loss : 1.6359540224075317
Epoch 01590: adjusting learning rate of group 0 to 2.3473e-06.
outputs_pos.loss : 0.9806246757507324
outputs_pos.loss : 1.130691409111023
outputs_pos.loss : 1.1276698112487793
outputs_pos.loss : 1.0866841077804565
outputs_pos.loss : 0.8974446058273315
outputs_pos.loss : 0.7302563786506653
outputs_pos.loss : 1.2327314615249634
outputs_pos.loss : 1.1644384860992432
Epoch 01591: adjusting learning rate of group 0 to 2.3471e-06.
outputs_pos.loss : 1.3411998748779297
outputs_pos.loss : 1.5984400510787964
outputs_pos.loss : 1.2340608835220337
outputs_pos.loss : 1.286911964416504
outputs_pos.loss : 1.4290257692337036
outputs_pos.loss : 1.1260664463043213
outputs_pos.loss : 1.2455317974090576
outputs_pos.loss : 1.2245244979858398
Epoch 01592: adjusting learning rate of group 0 to 2.3469e-06.
outputs_pos.loss : 1.2392683029174805
outputs_pos.loss : 1.1063897609710693
outputs_pos.loss : 0.9304826855659485
outputs_pos.loss : 0.7858796715736389
outputs_pos.loss : 1.0456396341323853
outputs_pos.loss : 1.384746789932251
outputs_pos.loss : 1.6782416105270386
outputs_pos.loss : 1.3214266300201416
Epoch 01593: adjusting learning rate of group 0 to 2.3467e-06.
outputs_pos.loss : 1.2461744546890259
outputs_pos.loss : 1.248338222503662
outputs_pos.loss : 1.082271695137024
outputs_pos.loss : 0.9324830174446106
outputs_pos.loss : 1.073330044746399
outputs_pos.loss : 1.0351985692977905
outputs_pos.loss : 1.7813067436218262
outputs_pos.loss : 1.2174859046936035
Epoch 01594: adjusting learning rate of group 0 to 2.3465e-06.
outputs_pos.loss : 1.5601983070373535
outputs_pos.loss : 1.207324743270874
outputs_pos.loss : 1.2877676486968994
outputs_pos.loss : 0.8708621263504028
outputs_pos.loss : 0.7307661771774292
outputs_pos.loss : 1.1806657314300537
outputs_pos.loss : 1.070064663887024
outputs_pos.loss : 1.128362774848938
Epoch 01595: adjusting learning rate of group 0 to 2.3463e-06.
outputs_pos.loss : 1.1561726331710815
outputs_pos.loss : 1.2263413667678833
outputs_pos.loss : 0.9842117428779602
outputs_pos.loss : 1.2223721742630005
outputs_pos.loss : 1.5585399866104126
outputs_pos.loss : 1.9604263305664062
outputs_pos.loss : 1.148236870765686
outputs_pos.loss : 1.268446922302246
Epoch 01596: adjusting learning rate of group 0 to 2.3461e-06.
outputs_pos.loss : 1.6103942394256592
outputs_pos.loss : 1.3389179706573486
outputs_pos.loss : 1.2480638027191162
outputs_pos.loss : 0.9345411658287048
outputs_pos.loss : 1.0871318578720093
outputs_pos.loss : 1.1933553218841553
outputs_pos.loss : 1.3594416379928589
outputs_pos.loss : 0.8050607442855835
Epoch 01597: adjusting learning rate of group 0 to 2.3460e-06.
outputs_pos.loss : 0.7825613617897034
outputs_pos.loss : 1.1454989910125732
outputs_pos.loss : 1.047145962715149
outputs_pos.loss : 1.224510669708252
outputs_pos.loss : 1.1700279712677002
outputs_pos.loss : 0.7302314043045044
outputs_pos.loss : 1.1951268911361694
outputs_pos.loss : 0.9740142822265625
Epoch 01598: adjusting learning rate of group 0 to 2.3458e-06.
outputs_pos.loss : 1.1943461894989014
outputs_pos.loss : 0.8591976761817932
outputs_pos.loss : 1.0033842325210571
outputs_pos.loss : 1.403839111328125
outputs_pos.loss : 1.567787766456604
outputs_pos.loss : 0.968812882900238
outputs_pos.loss : 0.8478080630302429
outputs_pos.loss : 0.7732012271881104
Epoch 01599: adjusting learning rate of group 0 to 2.3456e-06.
outputs_pos.loss : 1.4753352403640747
outputs_pos.loss : 0.823250949382782
outputs_pos.loss : 1.163010835647583
outputs_pos.loss : 0.9122709035873413
outputs_pos.loss : 0.7517344355583191
outputs_pos.loss : 1.0882867574691772
outputs_pos.loss : 1.137534260749817
outputs_pos.loss : 1.363451600074768
Epoch 01600: adjusting learning rate of group 0 to 2.3454e-06.
outputs_pos.loss : 1.2300020456314087
outputs_pos.loss : 1.2798833847045898
outputs_pos.loss : 1.5756586790084839
outputs_pos.loss : 1.214558720588684
outputs_pos.loss : 1.1757190227508545
outputs_pos.loss : 1.310939908027649
outputs_pos.loss : 1.2663218975067139
outputs_pos.loss : 1.0743283033370972
Epoch 01601: adjusting learning rate of group 0 to 2.3452e-06.
outputs_pos.loss : 1.2263981103897095
outputs_pos.loss : 1.3438081741333008
outputs_pos.loss : 1.1452938318252563
outputs_pos.loss : 1.582205057144165
outputs_pos.loss : 0.8365046977996826
outputs_pos.loss : 1.4190303087234497
outputs_pos.loss : 0.7946296334266663
outputs_pos.loss : 1.1233041286468506
Epoch 01602: adjusting learning rate of group 0 to 2.3450e-06.
outputs_pos.loss : 1.099481225013733
outputs_pos.loss : 1.2700272798538208
outputs_pos.loss : 0.9857004284858704
outputs_pos.loss : 1.0210403203964233
outputs_pos.loss : 0.9990500211715698
outputs_pos.loss : 0.9971901178359985
outputs_pos.loss : 1.2272019386291504
outputs_pos.loss : 0.9454193115234375
Epoch 01603: adjusting learning rate of group 0 to 2.3448e-06.
outputs_pos.loss : 1.280895709991455
outputs_pos.loss : 0.9334747791290283
outputs_pos.loss : 1.8457804918289185
outputs_pos.loss : 1.2064695358276367
outputs_pos.loss : 1.2873973846435547
outputs_pos.loss : 0.93960040807724
outputs_pos.loss : 1.4271870851516724
outputs_pos.loss : 1.2528959512710571
Epoch 01604: adjusting learning rate of group 0 to 2.3446e-06.
outputs_pos.loss : 0.8323900103569031
outputs_pos.loss : 1.3087831735610962
outputs_pos.loss : 1.4118967056274414
outputs_pos.loss : 1.3017394542694092
outputs_pos.loss : 0.4726566970348358
outputs_pos.loss : 1.3745253086090088
outputs_pos.loss : 1.308054804801941
outputs_pos.loss : 1.2144426107406616
Epoch 01605: adjusting learning rate of group 0 to 2.3444e-06.
outputs_pos.loss : 1.2311034202575684
outputs_pos.loss : 1.2880644798278809
outputs_pos.loss : 1.181961178779602
outputs_pos.loss : 1.4888477325439453
outputs_pos.loss : 1.4445075988769531
outputs_pos.loss : 0.6009755730628967
outputs_pos.loss : 1.5821253061294556
outputs_pos.loss : 0.9229031801223755
Epoch 01606: adjusting learning rate of group 0 to 2.3442e-06.
outputs_pos.loss : 1.1622949838638306
outputs_pos.loss : 1.6230827569961548
outputs_pos.loss : 1.0027416944503784
outputs_pos.loss : 1.5006115436553955
outputs_pos.loss : 1.4652303457260132
outputs_pos.loss : 1.19581937789917
outputs_pos.loss : 0.9772776365280151
outputs_pos.loss : 1.0135692358016968
Epoch 01607: adjusting learning rate of group 0 to 2.3441e-06.
outputs_pos.loss : 1.424896240234375
outputs_pos.loss : 0.967551052570343
outputs_pos.loss : 1.4879649877548218
outputs_pos.loss : 1.174365758895874
outputs_pos.loss : 1.58940589427948
outputs_pos.loss : 0.8402358889579773
outputs_pos.loss : 1.2554110288619995
outputs_pos.loss : 1.1882379055023193
Epoch 01608: adjusting learning rate of group 0 to 2.3439e-06.
outputs_pos.loss : 0.9923022389411926
outputs_pos.loss : 0.9794279336929321
outputs_pos.loss : 1.1534991264343262
outputs_pos.loss : 1.0502800941467285
outputs_pos.loss : 1.2064660787582397
outputs_pos.loss : 1.1651374101638794
outputs_pos.loss : 0.9779152870178223
outputs_pos.loss : 2.259592294692993
Epoch 01609: adjusting learning rate of group 0 to 2.3437e-06.
outputs_pos.loss : 1.4189177751541138
outputs_pos.loss : 0.8292245864868164
outputs_pos.loss : 0.7802427411079407
outputs_pos.loss : 1.4963680505752563
outputs_pos.loss : 1.088769555091858
outputs_pos.loss : 1.3641469478607178
outputs_pos.loss : 1.2999401092529297
outputs_pos.loss : 1.2128454446792603
Epoch 01610: adjusting learning rate of group 0 to 2.3435e-06.
outputs_pos.loss : 1.2278140783309937
outputs_pos.loss : 1.2019872665405273
outputs_pos.loss : 1.3332635164260864
outputs_pos.loss : 1.2168798446655273
outputs_pos.loss : 1.0878558158874512
outputs_pos.loss : 1.0441513061523438
outputs_pos.loss : 0.7901231050491333
outputs_pos.loss : 1.1632471084594727
Epoch 01611: adjusting learning rate of group 0 to 2.3433e-06.
outputs_pos.loss : 0.8576461672782898
outputs_pos.loss : 1.0942058563232422
outputs_pos.loss : 1.8538241386413574
outputs_pos.loss : 1.3615977764129639
outputs_pos.loss : 0.7482684254646301
outputs_pos.loss : 1.085669994354248
outputs_pos.loss : 1.1928822994232178
outputs_pos.loss : 1.0227762460708618
Epoch 01612: adjusting learning rate of group 0 to 2.3431e-06.
outputs_pos.loss : 1.1557698249816895
outputs_pos.loss : 1.6629841327667236
outputs_pos.loss : 1.1129764318466187
outputs_pos.loss : 1.0673801898956299
outputs_pos.loss : 1.6039094924926758
outputs_pos.loss : 1.092748999595642
outputs_pos.loss : 1.099656581878662
outputs_pos.loss : 1.1791499853134155
Epoch 01613: adjusting learning rate of group 0 to 2.3429e-06.
outputs_pos.loss : 1.3263815641403198
outputs_pos.loss : 0.9366273880004883
outputs_pos.loss : 2.3263912200927734
outputs_pos.loss : 1.063973307609558
outputs_pos.loss : 1.289768934249878
outputs_pos.loss : 1.2260799407958984
outputs_pos.loss : 1.5438697338104248
outputs_pos.loss : 0.8977910280227661
Epoch 01614: adjusting learning rate of group 0 to 2.3427e-06.
outputs_pos.loss : 0.8460226058959961
outputs_pos.loss : 0.9771889448165894
outputs_pos.loss : 1.016961932182312
outputs_pos.loss : 0.8735077977180481
outputs_pos.loss : 0.7832517027854919
outputs_pos.loss : 1.440679669380188
outputs_pos.loss : 0.9401007294654846
outputs_pos.loss : 1.4286961555480957
Epoch 01615: adjusting learning rate of group 0 to 2.3425e-06.
outputs_pos.loss : 1.1691042184829712
outputs_pos.loss : 1.305786371231079
outputs_pos.loss : 1.0020055770874023
outputs_pos.loss : 1.7093682289123535
outputs_pos.loss : 1.1014636754989624
outputs_pos.loss : 1.040710210800171
outputs_pos.loss : 1.4502111673355103
outputs_pos.loss : 1.5436973571777344
Epoch 01616: adjusting learning rate of group 0 to 2.3423e-06.
outputs_pos.loss : 1.0037519931793213
outputs_pos.loss : 1.238808512687683
outputs_pos.loss : 1.2859588861465454
outputs_pos.loss : 1.325373649597168
outputs_pos.loss : 1.4816395044326782
outputs_pos.loss : 1.3662256002426147
outputs_pos.loss : 1.0742745399475098
outputs_pos.loss : 0.8694208860397339
Epoch 01617: adjusting learning rate of group 0 to 2.3422e-06.
outputs_pos.loss : 1.30265212059021
outputs_pos.loss : 1.081940770149231
outputs_pos.loss : 0.8564048409461975
outputs_pos.loss : 1.6868363618850708
outputs_pos.loss : 1.5024352073669434
outputs_pos.loss : 1.6678392887115479
outputs_pos.loss : 1.1242531538009644
outputs_pos.loss : 1.1066157817840576
Epoch 01618: adjusting learning rate of group 0 to 2.3420e-06.
outputs_pos.loss : 1.2167316675186157
outputs_pos.loss : 1.5340732336044312
outputs_pos.loss : 1.1093767881393433
outputs_pos.loss : 1.1160457134246826
outputs_pos.loss : 1.2855712175369263
outputs_pos.loss : 1.0866516828536987
outputs_pos.loss : 1.1668602228164673
outputs_pos.loss : 1.5004264116287231
Epoch 01619: adjusting learning rate of group 0 to 2.3418e-06.
outputs_pos.loss : 1.5829943418502808
outputs_pos.loss : 0.8040492534637451
outputs_pos.loss : 1.0615756511688232
outputs_pos.loss : 1.655860424041748
outputs_pos.loss : 1.2007555961608887
outputs_pos.loss : 1.4826750755310059
outputs_pos.loss : 1.026584267616272
outputs_pos.loss : 1.4155609607696533
Epoch 01620: adjusting learning rate of group 0 to 2.3416e-06.
outputs_pos.loss : 1.5826975107192993
outputs_pos.loss : 1.1868306398391724
outputs_pos.loss : 1.0448967218399048
outputs_pos.loss : 1.2038270235061646
outputs_pos.loss : 0.7511377930641174
outputs_pos.loss : 1.3504929542541504
outputs_pos.loss : 1.3191769123077393
outputs_pos.loss : 1.4335846900939941
Epoch 01621: adjusting learning rate of group 0 to 2.3414e-06.
outputs_pos.loss : 0.4364445209503174
outputs_pos.loss : 1.1866670846939087
outputs_pos.loss : 1.284335732460022
outputs_pos.loss : 1.0944989919662476
outputs_pos.loss : 1.140389084815979
outputs_pos.loss : 1.0681222677230835
outputs_pos.loss : 1.0737135410308838
outputs_pos.loss : 1.6651532649993896
Epoch 01622: adjusting learning rate of group 0 to 2.3412e-06.
outputs_pos.loss : 1.2953307628631592
outputs_pos.loss : 1.395982265472412
outputs_pos.loss : 1.073090672492981
outputs_pos.loss : 0.8710947632789612
outputs_pos.loss : 0.7439711093902588
outputs_pos.loss : 0.7189350724220276
outputs_pos.loss : 0.8048019409179688
outputs_pos.loss : 1.5502370595932007
Epoch 01623: adjusting learning rate of group 0 to 2.3410e-06.
outputs_pos.loss : 1.4936866760253906
outputs_pos.loss : 1.2768388986587524
outputs_pos.loss : 0.9994410872459412
outputs_pos.loss : 1.2934775352478027
outputs_pos.loss : 1.0228081941604614
outputs_pos.loss : 1.5331103801727295
outputs_pos.loss : 0.9482419490814209
outputs_pos.loss : 0.748421311378479
Epoch 01624: adjusting learning rate of group 0 to 2.3408e-06.
outputs_pos.loss : 1.8910130262374878
outputs_pos.loss : 1.5708763599395752
outputs_pos.loss : 1.1497904062271118
outputs_pos.loss : 1.5361589193344116
outputs_pos.loss : 0.6918227672576904
outputs_pos.loss : 1.2949045896530151
outputs_pos.loss : 1.681427001953125
outputs_pos.loss : 1.2580260038375854
Epoch 01625: adjusting learning rate of group 0 to 2.3406e-06.
outputs_pos.loss : 1.601543664932251
outputs_pos.loss : 1.3312690258026123
outputs_pos.loss : 1.6184881925582886
outputs_pos.loss : 1.185477375984192
outputs_pos.loss : 0.9530826807022095
outputs_pos.loss : 1.140175700187683
outputs_pos.loss : 1.2548439502716064
outputs_pos.loss : 0.9395915269851685
Epoch 01626: adjusting learning rate of group 0 to 2.3404e-06.
outputs_pos.loss : 1.3820240497589111
outputs_pos.loss : 1.0213892459869385
outputs_pos.loss : 1.2556300163269043
outputs_pos.loss : 0.9116544127464294
outputs_pos.loss : 0.8000227808952332
outputs_pos.loss : 1.052106261253357
outputs_pos.loss : 0.8599576950073242
outputs_pos.loss : 0.8630043268203735
Epoch 01627: adjusting learning rate of group 0 to 2.3402e-06.
outputs_pos.loss : 1.1553455591201782
outputs_pos.loss : 1.1091617345809937
outputs_pos.loss : 1.1580199003219604
outputs_pos.loss : 0.8913475275039673
outputs_pos.loss : 0.8487632870674133
outputs_pos.loss : 0.9024313688278198
outputs_pos.loss : 0.8634834289550781
outputs_pos.loss : 0.8244607448577881
Epoch 01628: adjusting learning rate of group 0 to 2.3400e-06.
outputs_pos.loss : 0.5781175494194031
outputs_pos.loss : 1.0568318367004395
outputs_pos.loss : 1.2339824438095093
outputs_pos.loss : 0.9580831527709961
outputs_pos.loss : 1.1471110582351685
outputs_pos.loss : 1.2284011840820312
outputs_pos.loss : 1.4853695631027222
outputs_pos.loss : 1.2546645402908325
Epoch 01629: adjusting learning rate of group 0 to 2.3399e-06.
outputs_pos.loss : 1.8204240798950195
outputs_pos.loss : 1.2526987791061401
outputs_pos.loss : 0.9038968682289124
outputs_pos.loss : 0.8937608003616333
outputs_pos.loss : 0.8811935186386108
outputs_pos.loss : 0.8216617107391357
outputs_pos.loss : 1.554661750793457
outputs_pos.loss : 1.1091874837875366
Epoch 01630: adjusting learning rate of group 0 to 2.3397e-06.
outputs_pos.loss : 0.8498266935348511
outputs_pos.loss : 0.9851564764976501
outputs_pos.loss : 0.8232700824737549
outputs_pos.loss : 1.3710999488830566
outputs_pos.loss : 1.0591977834701538
outputs_pos.loss : 0.8061854243278503
outputs_pos.loss : 0.9876638054847717
outputs_pos.loss : 1.361908197402954
Epoch 01631: adjusting learning rate of group 0 to 2.3395e-06.
outputs_pos.loss : 1.1492395401000977
outputs_pos.loss : 1.4868639707565308
outputs_pos.loss : 1.3528879880905151
outputs_pos.loss : 1.0843160152435303
outputs_pos.loss : 0.8523929715156555
outputs_pos.loss : 1.3865728378295898
outputs_pos.loss : 1.0697541236877441
outputs_pos.loss : 1.1029969453811646
Epoch 01632: adjusting learning rate of group 0 to 2.3393e-06.
outputs_pos.loss : 0.9419602155685425
outputs_pos.loss : 1.2774029970169067
outputs_pos.loss : 1.148691177368164
outputs_pos.loss : 1.7289458513259888
outputs_pos.loss : 0.9246734976768494
outputs_pos.loss : 1.1130791902542114
outputs_pos.loss : 1.169549584388733
outputs_pos.loss : 1.0911052227020264
Epoch 01633: adjusting learning rate of group 0 to 2.3391e-06.
outputs_pos.loss : 0.9431254267692566
outputs_pos.loss : 0.8973711133003235
outputs_pos.loss : 1.276371955871582
outputs_pos.loss : 0.9261664748191833
outputs_pos.loss : 1.31939697265625
outputs_pos.loss : 1.175480604171753
outputs_pos.loss : 0.996336042881012
outputs_pos.loss : 1.1198313236236572
Epoch 01634: adjusting learning rate of group 0 to 2.3389e-06.
outputs_pos.loss : 0.833655834197998
outputs_pos.loss : 1.1614489555358887
outputs_pos.loss : 1.3072983026504517
outputs_pos.loss : 1.099064588546753
outputs_pos.loss : 1.2155741453170776
outputs_pos.loss : 1.0960828065872192
outputs_pos.loss : 1.2244880199432373
outputs_pos.loss : 0.9451753497123718
Epoch 01635: adjusting learning rate of group 0 to 2.3387e-06.
outputs_pos.loss : 0.8321826457977295
outputs_pos.loss : 1.5085114240646362
outputs_pos.loss : 1.1457719802856445
outputs_pos.loss : 0.9332227110862732
outputs_pos.loss : 1.201522707939148
outputs_pos.loss : 0.9711697697639465
outputs_pos.loss : 1.125910758972168
outputs_pos.loss : 1.5278716087341309
Epoch 01636: adjusting learning rate of group 0 to 2.3385e-06.
outputs_pos.loss : 1.721341848373413
outputs_pos.loss : 1.1157547235488892
outputs_pos.loss : 1.5069248676300049
outputs_pos.loss : 0.7936606407165527
outputs_pos.loss : 1.1444050073623657
outputs_pos.loss : 1.1438589096069336
outputs_pos.loss : 1.3008379936218262
outputs_pos.loss : 2.0215904712677
Epoch 01637: adjusting learning rate of group 0 to 2.3383e-06.
outputs_pos.loss : 1.1130675077438354
outputs_pos.loss : 1.2079335451126099
outputs_pos.loss : 0.9216287136077881
outputs_pos.loss : 0.9185647368431091
outputs_pos.loss : 1.056917428970337
outputs_pos.loss : 1.1837146282196045
outputs_pos.loss : 1.2592586278915405
outputs_pos.loss : 1.0971893072128296
Epoch 01638: adjusting learning rate of group 0 to 2.3381e-06.
outputs_pos.loss : 0.8548519015312195
outputs_pos.loss : 0.9381762146949768
outputs_pos.loss : 1.1317139863967896
outputs_pos.loss : 1.631392240524292
outputs_pos.loss : 1.0929137468338013
outputs_pos.loss : 1.2888883352279663
outputs_pos.loss : 1.4672640562057495
outputs_pos.loss : 1.2173452377319336
Epoch 01639: adjusting learning rate of group 0 to 2.3379e-06.
outputs_pos.loss : 1.3086744546890259
outputs_pos.loss : 1.4426963329315186
outputs_pos.loss : 1.095159888267517
outputs_pos.loss : 0.8947221636772156
outputs_pos.loss : 0.8978592157363892
outputs_pos.loss : 1.2633538246154785
outputs_pos.loss : 1.086936354637146
outputs_pos.loss : 1.2750738859176636
Epoch 01640: adjusting learning rate of group 0 to 2.3377e-06.
outputs_pos.loss : 1.1508550643920898
outputs_pos.loss : 1.1980834007263184
outputs_pos.loss : 1.1113522052764893
outputs_pos.loss : 1.3859206438064575
outputs_pos.loss : 1.1404125690460205
outputs_pos.loss : 0.9097203016281128
outputs_pos.loss : 1.142811894416809
outputs_pos.loss : 1.681738257408142
Epoch 01641: adjusting learning rate of group 0 to 2.3375e-06.
outputs_pos.loss : 1.091366171836853
outputs_pos.loss : 1.25224769115448
outputs_pos.loss : 1.1818264722824097
outputs_pos.loss : 1.2555124759674072
outputs_pos.loss : 0.9892019629478455
outputs_pos.loss : 1.0149235725402832
outputs_pos.loss : 0.9794144630432129
outputs_pos.loss : 1.139836072921753
Epoch 01642: adjusting learning rate of group 0 to 2.3373e-06.
outputs_pos.loss : 0.952090322971344
outputs_pos.loss : 1.8809794187545776
outputs_pos.loss : 1.375856876373291
outputs_pos.loss : 1.3298559188842773
outputs_pos.loss : 1.0119154453277588
outputs_pos.loss : 1.1834625005722046
outputs_pos.loss : 1.153566598892212
outputs_pos.loss : 0.8853642344474792
Epoch 01643: adjusting learning rate of group 0 to 2.3371e-06.
outputs_pos.loss : 1.211401343345642
outputs_pos.loss : 0.8524255752563477
outputs_pos.loss : 1.203507900238037
outputs_pos.loss : 1.1186058521270752
outputs_pos.loss : 1.1300386190414429
outputs_pos.loss : 1.0320864915847778
outputs_pos.loss : 1.5758899450302124
outputs_pos.loss : 1.2184771299362183
Epoch 01644: adjusting learning rate of group 0 to 2.3370e-06.
outputs_pos.loss : 1.3600120544433594
outputs_pos.loss : 1.6641649007797241
outputs_pos.loss : 1.3209148645401
outputs_pos.loss : 1.1212382316589355
outputs_pos.loss : 1.4547011852264404
outputs_pos.loss : 1.3202553987503052
outputs_pos.loss : 0.9562829732894897
outputs_pos.loss : 1.5191611051559448
Epoch 01645: adjusting learning rate of group 0 to 2.3368e-06.
outputs_pos.loss : 1.0324493646621704
outputs_pos.loss : 1.1334271430969238
outputs_pos.loss : 1.1488286256790161
outputs_pos.loss : 1.7246770858764648
outputs_pos.loss : 1.3259670734405518
outputs_pos.loss : 0.9288047552108765
outputs_pos.loss : 1.1908782720565796
outputs_pos.loss : 1.1228150129318237
Epoch 01646: adjusting learning rate of group 0 to 2.3366e-06.
outputs_pos.loss : 1.3201675415039062
outputs_pos.loss : 1.444119930267334
outputs_pos.loss : 1.1043347120285034
outputs_pos.loss : 1.051228404045105
outputs_pos.loss : 1.1401258707046509
outputs_pos.loss : 0.9042131900787354
outputs_pos.loss : 1.376396656036377
outputs_pos.loss : 0.9338471293449402
Epoch 01647: adjusting learning rate of group 0 to 2.3364e-06.
outputs_pos.loss : 1.1904929876327515
outputs_pos.loss : 1.1662542819976807
outputs_pos.loss : 1.374274492263794
outputs_pos.loss : 0.9497387409210205
outputs_pos.loss : 1.2172473669052124
outputs_pos.loss : 1.215004324913025
outputs_pos.loss : 1.103874921798706
outputs_pos.loss : 0.9613031148910522
Epoch 01648: adjusting learning rate of group 0 to 2.3362e-06.
outputs_pos.loss : 1.5572320222854614
outputs_pos.loss : 1.114493727684021
outputs_pos.loss : 1.1180057525634766
outputs_pos.loss : 1.3498440980911255
outputs_pos.loss : 0.9737739562988281
outputs_pos.loss : 1.5066142082214355
outputs_pos.loss : 0.8337141871452332
outputs_pos.loss : 1.1570407152175903
Epoch 01649: adjusting learning rate of group 0 to 2.3360e-06.
outputs_pos.loss : 2.050410270690918
outputs_pos.loss : 0.906756579875946
outputs_pos.loss : 1.255234718322754
outputs_pos.loss : 1.3177342414855957
outputs_pos.loss : 1.2334730625152588
outputs_pos.loss : 1.0935332775115967
outputs_pos.loss : 0.920504629611969
outputs_pos.loss : 1.2251125574111938
Epoch 01650: adjusting learning rate of group 0 to 2.3358e-06.
outputs_pos.loss : 1.5644910335540771
outputs_pos.loss : 1.0415807962417603
outputs_pos.loss : 1.1745340824127197
outputs_pos.loss : 1.2457870244979858
outputs_pos.loss : 0.5519683361053467
outputs_pos.loss : 1.242247223854065
outputs_pos.loss : 1.1437839269638062
outputs_pos.loss : 1.2600098848342896
Epoch 01651: adjusting learning rate of group 0 to 2.3356e-06.
outputs_pos.loss : 0.972582221031189
outputs_pos.loss : 1.5844311714172363
outputs_pos.loss : 1.3494610786437988
outputs_pos.loss : 1.008910059928894
outputs_pos.loss : 0.7528263330459595
outputs_pos.loss : 0.9127349853515625
outputs_pos.loss : 1.1893919706344604
outputs_pos.loss : 1.427184820175171
Epoch 01652: adjusting learning rate of group 0 to 2.3354e-06.
outputs_pos.loss : 1.2036575078964233
outputs_pos.loss : 1.0002903938293457
outputs_pos.loss : 1.168506145477295
outputs_pos.loss : 1.08757746219635
outputs_pos.loss : 1.4737813472747803
outputs_pos.loss : 1.069440245628357
outputs_pos.loss : 1.0163275003433228
outputs_pos.loss : 1.0808172225952148
Epoch 01653: adjusting learning rate of group 0 to 2.3352e-06.
outputs_pos.loss : 1.5663715600967407
outputs_pos.loss : 1.1489722728729248
outputs_pos.loss : 1.2503162622451782
outputs_pos.loss : 1.7962039709091187
outputs_pos.loss : 0.830388605594635
outputs_pos.loss : 1.2674083709716797
outputs_pos.loss : 1.499545693397522
outputs_pos.loss : 1.0070786476135254
Epoch 01654: adjusting learning rate of group 0 to 2.3350e-06.
outputs_pos.loss : 0.8136860132217407
outputs_pos.loss : 1.121452808380127
outputs_pos.loss : 1.0117847919464111
outputs_pos.loss : 1.1807421445846558
outputs_pos.loss : 1.0088144540786743
outputs_pos.loss : 1.4808071851730347
outputs_pos.loss : 0.9812026619911194
outputs_pos.loss : 1.3293253183364868
Epoch 01655: adjusting learning rate of group 0 to 2.3348e-06.
outputs_pos.loss : 1.5235382318496704
outputs_pos.loss : 1.080068588256836
outputs_pos.loss : 1.7649039030075073
outputs_pos.loss : 1.6038756370544434
outputs_pos.loss : 1.2650656700134277
outputs_pos.loss : 1.15883207321167
outputs_pos.loss : 1.5404253005981445
outputs_pos.loss : 1.045097827911377
Epoch 01656: adjusting learning rate of group 0 to 2.3346e-06.
outputs_pos.loss : 1.3666619062423706
outputs_pos.loss : 1.1692522764205933
outputs_pos.loss : 1.0720558166503906
outputs_pos.loss : 0.8814572095870972
outputs_pos.loss : 1.27482008934021
outputs_pos.loss : 1.0485490560531616
outputs_pos.loss : 1.1553767919540405
outputs_pos.loss : 0.9244801998138428
Epoch 01657: adjusting learning rate of group 0 to 2.3344e-06.
outputs_pos.loss : 1.3650681972503662
outputs_pos.loss : 0.9546616673469543
outputs_pos.loss : 0.9528626799583435
outputs_pos.loss : 1.0389481782913208
outputs_pos.loss : 1.2139602899551392
outputs_pos.loss : 1.519531011581421
outputs_pos.loss : 1.166699767112732
outputs_pos.loss : 0.9969527721405029
Epoch 01658: adjusting learning rate of group 0 to 2.3342e-06.
outputs_pos.loss : 1.1413512229919434
outputs_pos.loss : 0.8806470632553101
outputs_pos.loss : 1.1649069786071777
outputs_pos.loss : 1.1415135860443115
outputs_pos.loss : 0.6238504648208618
outputs_pos.loss : 1.0212178230285645
outputs_pos.loss : 1.2798584699630737
outputs_pos.loss : 1.223630428314209
Epoch 01659: adjusting learning rate of group 0 to 2.3340e-06.
outputs_pos.loss : 1.2139030694961548
outputs_pos.loss : 0.9647442102432251
outputs_pos.loss : 1.8901828527450562
outputs_pos.loss : 1.419234037399292
outputs_pos.loss : 0.8805688619613647
outputs_pos.loss : 1.368146300315857
outputs_pos.loss : 1.3356670141220093
outputs_pos.loss : 1.0298789739608765
Epoch 01660: adjusting learning rate of group 0 to 2.3338e-06.
outputs_pos.loss : 1.2122234106063843
outputs_pos.loss : 1.0420538187026978
outputs_pos.loss : 0.9967769384384155
outputs_pos.loss : 1.35136079788208
outputs_pos.loss : 0.9352447390556335
outputs_pos.loss : 1.0694303512573242
outputs_pos.loss : 1.2370210886001587
outputs_pos.loss : 1.1726571321487427
Epoch 01661: adjusting learning rate of group 0 to 2.3336e-06.
outputs_pos.loss : 1.175390362739563
outputs_pos.loss : 0.9771147966384888
outputs_pos.loss : 0.9324072003364563
outputs_pos.loss : 0.8785200715065002
outputs_pos.loss : 1.231391191482544
outputs_pos.loss : 0.792280375957489
outputs_pos.loss : 1.181453824043274
outputs_pos.loss : 1.6799613237380981
Epoch 01662: adjusting learning rate of group 0 to 2.3334e-06.
outputs_pos.loss : 1.237899661064148
outputs_pos.loss : 0.9737407565116882
outputs_pos.loss : 1.1247169971466064
outputs_pos.loss : 1.586811900138855
outputs_pos.loss : 1.1129769086837769
outputs_pos.loss : 1.2327467203140259
outputs_pos.loss : 1.5105332136154175
outputs_pos.loss : 1.3904913663864136
Epoch 01663: adjusting learning rate of group 0 to 2.3333e-06.
outputs_pos.loss : 1.2398536205291748
outputs_pos.loss : 1.0543957948684692
outputs_pos.loss : 1.2808899879455566
outputs_pos.loss : 0.9078612923622131
outputs_pos.loss : 0.9870477914810181
outputs_pos.loss : 0.9035221934318542
outputs_pos.loss : 1.146788239479065
outputs_pos.loss : 1.077749490737915
Epoch 01664: adjusting learning rate of group 0 to 2.3331e-06.
outputs_pos.loss : 1.1319611072540283
outputs_pos.loss : 0.7072991132736206
outputs_pos.loss : 1.3702552318572998
outputs_pos.loss : 1.2787917852401733
outputs_pos.loss : 1.823056697845459
outputs_pos.loss : 0.7835061550140381
outputs_pos.loss : 0.954103946685791
outputs_pos.loss : 1.07158625125885
Epoch 01665: adjusting learning rate of group 0 to 2.3329e-06.
outputs_pos.loss : 1.4643840789794922
outputs_pos.loss : 1.5872043371200562
outputs_pos.loss : 1.227534294128418
outputs_pos.loss : 1.2663910388946533
outputs_pos.loss : 1.1687078475952148
outputs_pos.loss : 1.267951250076294
outputs_pos.loss : 1.0512670278549194
outputs_pos.loss : 1.8969848155975342
Epoch 01666: adjusting learning rate of group 0 to 2.3327e-06.
outputs_pos.loss : 1.4250508546829224
outputs_pos.loss : 1.8447113037109375
outputs_pos.loss : 1.161849021911621
outputs_pos.loss : 1.3393044471740723
outputs_pos.loss : 1.0486046075820923
outputs_pos.loss : 0.9137428402900696
outputs_pos.loss : 1.042143702507019
outputs_pos.loss : 1.621793270111084
Epoch 01667: adjusting learning rate of group 0 to 2.3325e-06.
outputs_pos.loss : 0.8675270080566406
outputs_pos.loss : 0.817518949508667
outputs_pos.loss : 1.155373215675354
outputs_pos.loss : 1.6501232385635376
outputs_pos.loss : 0.7631580233573914
outputs_pos.loss : 0.9447912573814392
outputs_pos.loss : 0.7685904502868652
outputs_pos.loss : 0.8693245053291321
Epoch 01668: adjusting learning rate of group 0 to 2.3323e-06.
outputs_pos.loss : 1.7601306438446045
outputs_pos.loss : 0.9509752988815308
outputs_pos.loss : 1.2536810636520386
outputs_pos.loss : 1.037149429321289
outputs_pos.loss : 1.4934625625610352
outputs_pos.loss : 1.2485448122024536
outputs_pos.loss : 0.9508999586105347
outputs_pos.loss : 0.958713948726654
Epoch 01669: adjusting learning rate of group 0 to 2.3321e-06.
outputs_pos.loss : 1.0133633613586426
outputs_pos.loss : 1.2802587747573853
outputs_pos.loss : 1.203663945198059
outputs_pos.loss : 1.4032258987426758
outputs_pos.loss : 1.3298407793045044
outputs_pos.loss : 0.8565885424613953
outputs_pos.loss : 0.6592668890953064
outputs_pos.loss : 1.4704393148422241
Epoch 01670: adjusting learning rate of group 0 to 2.3319e-06.
outputs_pos.loss : 2.0808169841766357
outputs_pos.loss : 1.5178552865982056
outputs_pos.loss : 1.693040370941162
outputs_pos.loss : 0.6957346796989441
outputs_pos.loss : 1.2265716791152954
outputs_pos.loss : 1.1182130575180054
outputs_pos.loss : 1.1285909414291382
outputs_pos.loss : 0.6539420485496521
Epoch 01671: adjusting learning rate of group 0 to 2.3317e-06.
outputs_pos.loss : 1.1570229530334473
outputs_pos.loss : 1.277621865272522
outputs_pos.loss : 0.9104768633842468
outputs_pos.loss : 1.1083651781082153
outputs_pos.loss : 1.1249558925628662
outputs_pos.loss : 0.9940025806427002
outputs_pos.loss : 0.9849375486373901
outputs_pos.loss : 0.9458317160606384
Epoch 01672: adjusting learning rate of group 0 to 2.3315e-06.
outputs_pos.loss : 1.3344132900238037
outputs_pos.loss : 0.9405504465103149
outputs_pos.loss : 1.4169844388961792
outputs_pos.loss : 1.2286657094955444
outputs_pos.loss : 1.391716480255127
outputs_pos.loss : 1.2189050912857056
outputs_pos.loss : 0.6497868299484253
outputs_pos.loss : 1.0682926177978516
Epoch 01673: adjusting learning rate of group 0 to 2.3313e-06.
outputs_pos.loss : 1.2887204885482788
outputs_pos.loss : 1.0633362531661987
outputs_pos.loss : 1.1509701013565063
outputs_pos.loss : 1.3318544626235962
outputs_pos.loss : 1.4870610237121582
outputs_pos.loss : 1.2837111949920654
outputs_pos.loss : 0.9749416708946228
outputs_pos.loss : 0.756848931312561
Epoch 01674: adjusting learning rate of group 0 to 2.3311e-06.
outputs_pos.loss : 1.6640052795410156
outputs_pos.loss : 1.1711457967758179
outputs_pos.loss : 1.6851835250854492
outputs_pos.loss : 1.2054815292358398
outputs_pos.loss : 0.9743374586105347
outputs_pos.loss : 1.1889984607696533
outputs_pos.loss : 1.0289276838302612
outputs_pos.loss : 1.392842411994934
Epoch 01675: adjusting learning rate of group 0 to 2.3309e-06.
outputs_pos.loss : 1.113765835762024
outputs_pos.loss : 1.055010437965393
outputs_pos.loss : 0.8262020349502563
outputs_pos.loss : 0.890871524810791
outputs_pos.loss : 0.908033013343811
outputs_pos.loss : 0.8864566087722778
outputs_pos.loss : 1.1239513158798218
outputs_pos.loss : 1.1540441513061523
Epoch 01676: adjusting learning rate of group 0 to 2.3307e-06.
outputs_pos.loss : 1.0894434452056885
outputs_pos.loss : 0.8395217657089233
outputs_pos.loss : 2.572197198867798
outputs_pos.loss : 0.9325962066650391
outputs_pos.loss : 1.5064822435379028
outputs_pos.loss : 1.0600476264953613
outputs_pos.loss : 1.6459275484085083
outputs_pos.loss : 1.0379217863082886
Epoch 01677: adjusting learning rate of group 0 to 2.3305e-06.
outputs_pos.loss : 0.8713736534118652
outputs_pos.loss : 1.1231517791748047
outputs_pos.loss : 1.3855602741241455
outputs_pos.loss : 1.613403558731079
outputs_pos.loss : 1.4294326305389404
outputs_pos.loss : 1.3467004299163818
outputs_pos.loss : 1.133278727531433
outputs_pos.loss : 1.5807042121887207
Epoch 01678: adjusting learning rate of group 0 to 2.3303e-06.
outputs_pos.loss : 0.755496084690094
outputs_pos.loss : 1.430910348892212
outputs_pos.loss : 1.4206682443618774
outputs_pos.loss : 0.8921307325363159
outputs_pos.loss : 1.4935153722763062
outputs_pos.loss : 1.4522970914840698
outputs_pos.loss : 1.0822240114212036
outputs_pos.loss : 1.0251864194869995
Epoch 01679: adjusting learning rate of group 0 to 2.3301e-06.
outputs_pos.loss : 0.9017951488494873
outputs_pos.loss : 1.2919970750808716
outputs_pos.loss : 1.1083074808120728
outputs_pos.loss : 0.9143342971801758
outputs_pos.loss : 1.0926421880722046
outputs_pos.loss : 1.1240752935409546
outputs_pos.loss : 1.429714560508728
outputs_pos.loss : 1.5913668870925903
Epoch 01680: adjusting learning rate of group 0 to 2.3299e-06.
outputs_pos.loss : 1.0239418745040894
outputs_pos.loss : 1.0390961170196533
outputs_pos.loss : 1.0784521102905273
outputs_pos.loss : 1.3239893913269043
outputs_pos.loss : 1.1815351247787476
outputs_pos.loss : 1.1160043478012085
outputs_pos.loss : 0.856495201587677
outputs_pos.loss : 0.9478455185890198
Epoch 01681: adjusting learning rate of group 0 to 2.3297e-06.
outputs_pos.loss : 1.0081676244735718
outputs_pos.loss : 0.9629166722297668
outputs_pos.loss : 1.6077854633331299
outputs_pos.loss : 0.7750169634819031
outputs_pos.loss : 1.0296345949172974
outputs_pos.loss : 1.2457456588745117
outputs_pos.loss : 1.11207914352417
outputs_pos.loss : 0.9197456240653992
Epoch 01682: adjusting learning rate of group 0 to 2.3295e-06.
outputs_pos.loss : 1.268515706062317
outputs_pos.loss : 1.0333259105682373
outputs_pos.loss : 1.1430519819259644
outputs_pos.loss : 1.2054580450057983
outputs_pos.loss : 1.068368673324585
outputs_pos.loss : 0.99687260389328
outputs_pos.loss : 1.0433835983276367
outputs_pos.loss : 1.3095735311508179
Epoch 01683: adjusting learning rate of group 0 to 2.3293e-06.
outputs_pos.loss : 1.3759479522705078
outputs_pos.loss : 1.2114778757095337
outputs_pos.loss : 1.1702910661697388
outputs_pos.loss : 0.6956626772880554
outputs_pos.loss : 1.068410038948059
outputs_pos.loss : 1.6761729717254639
outputs_pos.loss : 1.478711724281311
outputs_pos.loss : 0.6023290157318115
Epoch 01684: adjusting learning rate of group 0 to 2.3291e-06.
outputs_pos.loss : 1.4949394464492798
outputs_pos.loss : 0.9443025588989258
outputs_pos.loss : 1.0001384019851685
outputs_pos.loss : 1.1860740184783936
Epoch 01685: adjusting learning rate of group 0 to 2.3289e-06.
Epoch 1 Validation:   2%|â–ˆâ–                                                                   | 31/1500 [02:29<1:34:46,  3.87s/it, val_loss=1.12, batch=30]
outputs_pos.loss : 1.0649354457855225
outputs_pos.loss : 0.8408165574073792
outputs_pos.loss : 0.9224678874015808
outputs_pos.loss : 1.464449405670166
outputs_pos.loss : 1.2136797904968262
outputs_pos.loss : 1.1374635696411133
outputs_pos.loss : 1.229783058166504
outputs_pos.loss : 1.2145135402679443
outputs_pos.loss : 1.2808939218521118
outputs_pos.loss : 1.1399410963058472
outputs_pos.loss : 1.2122845649719238
outputs_pos.loss : 1.2079386711120605
outputs_pos.loss : 1.1140307188034058
outputs_pos.loss : 1.0655821561813354
outputs_pos.loss : 1.2040636539459229
outputs_pos.loss : 1.1543787717819214
outputs_pos.loss : 1.137021780014038
outputs_pos.loss : 1.1124975681304932
outputs_pos.loss : 1.0388023853302002
outputs_pos.loss : 1.1578679084777832
outputs_pos.loss : 1.2850804328918457
outputs_pos.loss : 0.8769981861114502
outputs_pos.loss : 1.1125842332839966
outputs_pos.loss : 1.0927094221115112
outputs_pos.loss : 1.2749205827713013
outputs_pos.loss : 1.2710164785385132
outputs_pos.loss : 1.013643741607666
outputs_pos.loss : 1.2904276847839355
outputs_pos.loss : 1.6816223859786987
outputs_pos.loss : 1.060222864151001
outputs_pos.loss : 0.9998947381973267
outputs_pos.loss : 1.2721267938613892
outputs_pos.loss : 1.4193624258041382
outputs_pos.loss : 1.27467679977417
outputs_pos.loss : 1.6454294919967651
outputs_pos.loss : 0.8042773604393005
outputs_pos.loss : 1.4583637714385986
outputs_pos.loss : 0.8816535472869873
outputs_pos.loss : 1.2815622091293335
outputs_pos.loss : 1.047264575958252
outputs_pos.loss : 1.6731383800506592
outputs_pos.loss : 1.2680554389953613
outputs_pos.loss : 0.8264682292938232
outputs_pos.loss : 1.2563955783843994
outputs_pos.loss : 1.400266170501709
outputs_pos.loss : 0.768721342086792
outputs_pos.loss : 1.071423053741455
outputs_pos.loss : 1.0493996143341064
outputs_pos.loss : 0.9194873571395874
outputs_pos.loss : 1.23631751537323
outputs_pos.loss : 1.5082358121871948
outputs_pos.loss : 1.2886462211608887
outputs_pos.loss : 0.9652298092842102
outputs_pos.loss : 1.4483994245529175
outputs_pos.loss : 1.2326992750167847
outputs_pos.loss : 1.2858515977859497
outputs_pos.loss : 1.414905071258545
outputs_pos.loss : 0.9440894722938538
outputs_pos.loss : 0.7021433711051941
outputs_pos.loss : 1.128734827041626
outputs_pos.loss : 1.2614314556121826
outputs_pos.loss : 1.205575942993164
outputs_pos.loss : 1.2092891931533813
outputs_pos.loss : 0.6424817442893982
outputs_pos.loss : 0.7837156653404236
outputs_pos.loss : 0.7251615524291992
outputs_pos.loss : 1.2241140604019165
outputs_pos.loss : 1.108657717704773
outputs_pos.loss : 0.9155442118644714
outputs_pos.loss : 0.9241921901702881
outputs_pos.loss : 2.068615198135376
outputs_pos.loss : 1.282349705696106
outputs_pos.loss : 1.184596061706543
outputs_pos.loss : 1.5604623556137085
outputs_pos.loss : 0.7724263072013855
outputs_pos.loss : 1.1421148777008057
outputs_pos.loss : 1.3422492742538452
outputs_pos.loss : 1.0463135242462158
outputs_pos.loss : 1.2826257944107056
outputs_pos.loss : 1.1919642686843872
outputs_pos.loss : 1.1236577033996582
outputs_pos.loss : 0.6137535572052002
outputs_pos.loss : 1.1396301984786987
outputs_pos.loss : 1.3159974813461304
outputs_pos.loss : 1.2434378862380981
outputs_pos.loss : 0.9272136688232422
outputs_pos.loss : 1.4153259992599487
outputs_pos.loss : 0.8439568281173706
outputs_pos.loss : 1.121655821800232
outputs_pos.loss : 1.2817208766937256
outputs_pos.loss : 1.109802484512329
outputs_pos.loss : 1.290558934211731
outputs_pos.loss : 1.0316064357757568
outputs_pos.loss : 1.1978297233581543
outputs_pos.loss : 1.1750458478927612
outputs_pos.loss : 0.9245015382766724
outputs_pos.loss : 1.1742596626281738
outputs_pos.loss : 1.5226047039031982
outputs_pos.loss : 1.2938123941421509
outputs_pos.loss : 1.4096001386642456
outputs_pos.loss : 1.3300570249557495
outputs_pos.loss : 1.2028312683105469
outputs_pos.loss : 1.2567839622497559
outputs_pos.loss : 1.1819490194320679
outputs_pos.loss : 1.6543409824371338
outputs_pos.loss : 1.1724685430526733
outputs_pos.loss : 0.6148855090141296
outputs_pos.loss : 1.0144251585006714
outputs_pos.loss : 0.9129247665405273
outputs_pos.loss : 1.0181249380111694
outputs_pos.loss : 1.5842077732086182
outputs_pos.loss : 1.0911519527435303
outputs_pos.loss : 0.9971269369125366
outputs_pos.loss : 1.2467262744903564
outputs_pos.loss : 0.6458703279495239
outputs_pos.loss : 1.1381927728652954
outputs_pos.loss : 1.4475200176239014
outputs_pos.loss : 0.8677384257316589
outputs_pos.loss : 1.5708636045455933
outputs_pos.loss : 1.4821306467056274
outputs_pos.loss : 1.1836621761322021
outputs_pos.loss : 0.9202280640602112
outputs_pos.loss : 1.0891714096069336
outputs_pos.loss : 0.9414970874786377
outputs_pos.loss : 1.0000083446502686
outputs_pos.loss : 1.0161337852478027
outputs_pos.loss : 1.1560516357421875
outputs_pos.loss : 0.8990108370780945
outputs_pos.loss : 1.2430719137191772
outputs_pos.loss : 1.5733927488327026
outputs_pos.loss : 0.84759122133255
outputs_pos.loss : 1.1029329299926758
outputs_pos.loss : 1.4771054983139038
outputs_pos.loss : 0.907468855381012
outputs_pos.loss : 0.840660572052002
outputs_pos.loss : 1.0929217338562012
outputs_pos.loss : 1.0866962671279907
outputs_pos.loss : 0.981127142906189
outputs_pos.loss : 0.7605472803115845
outputs_pos.loss : 1.2216312885284424
outputs_pos.loss : 1.0005995035171509
outputs_pos.loss : 1.2001261711120605
outputs_pos.loss : 1.2094168663024902
outputs_pos.loss : 1.2890154123306274
outputs_pos.loss : 0.679703950881958
outputs_pos.loss : 1.5782346725463867
outputs_pos.loss : 1.2436097860336304
outputs_pos.loss : 1.7692691087722778
outputs_pos.loss : 1.3829102516174316
outputs_pos.loss : 1.416469931602478
outputs_pos.loss : 0.8623180985450745
outputs_pos.loss : 1.5053826570510864
outputs_pos.loss : 0.9546653032302856
outputs_pos.loss : 0.9869008660316467
outputs_pos.loss : 1.2066385746002197
outputs_pos.loss : 0.9227863550186157
outputs_pos.loss : 1.2750040292739868
outputs_pos.loss : 0.7968426942825317
outputs_pos.loss : 1.216370940208435
outputs_pos.loss : 1.2509304285049438
outputs_pos.loss : 1.2054883241653442
outputs_pos.loss : 0.719194233417511
outputs_pos.loss : 0.8141753673553467
outputs_pos.loss : 1.623085379600525
outputs_pos.loss : 1.3430463075637817
outputs_pos.loss : 0.9981825947761536
outputs_pos.loss : 1.3505531549453735
outputs_pos.loss : 0.9512388706207275
outputs_pos.loss : 1.3536438941955566
outputs_pos.loss : 1.5363825559616089
outputs_pos.loss : 1.0311193466186523
outputs_pos.loss : 1.195340633392334
outputs_pos.loss : 1.0628143548965454
outputs_pos.loss : 0.849728524684906
outputs_pos.loss : 1.0063093900680542
outputs_pos.loss : 1.1934925317764282
outputs_pos.loss : 1.1944156885147095
outputs_pos.loss : 1.013180136680603
outputs_pos.loss : 0.730984091758728
outputs_pos.loss : 1.0564067363739014
outputs_pos.loss : 1.4588664770126343
outputs_pos.loss : 1.442581295967102
outputs_pos.loss : 1.0833930969238281
outputs_pos.loss : 0.9930994510650635
outputs_pos.loss : 1.017227292060852
outputs_pos.loss : 1.4757145643234253
outputs_pos.loss : 1.8452969789505005
outputs_pos.loss : 1.2622302770614624
outputs_pos.loss : 0.95222008228302
outputs_pos.loss : 1.1562808752059937
outputs_pos.loss : 0.8801250457763672
outputs_pos.loss : 0.9464561939239502
outputs_pos.loss : 1.2107417583465576
outputs_pos.loss : 1.301324725151062
outputs_pos.loss : 1.0954071283340454
outputs_pos.loss : 1.0416264533996582
outputs_pos.loss : 1.0468956232070923
outputs_pos.loss : 1.1599750518798828
outputs_pos.loss : 1.0971570014953613
outputs_pos.loss : 0.9947279095649719
outputs_pos.loss : 1.3194983005523682
outputs_pos.loss : 1.4390050172805786
outputs_pos.loss : 1.064520001411438
outputs_pos.loss : 1.7991888523101807
outputs_pos.loss : 1.548965334892273
outputs_pos.loss : 1.19097101688385
outputs_pos.loss : 1.1476531028747559
outputs_pos.loss : 1.1450310945510864
outputs_pos.loss : 0.992341935634613
outputs_pos.loss : 0.8864009976387024
outputs_pos.loss : 0.9863739013671875
outputs_pos.loss : 1.2612720727920532
outputs_pos.loss : 1.105054259300232
outputs_pos.loss : 1.2856377363204956
outputs_pos.loss : 1.587032437324524
outputs_pos.loss : 1.8843536376953125
outputs_pos.loss : 0.9896289110183716
outputs_pos.loss : 0.9598884582519531
outputs_pos.loss : 1.343222975730896
outputs_pos.loss : 1.667004108428955
outputs_pos.loss : 1.0228294134140015
outputs_pos.loss : 1.4131358861923218
outputs_pos.loss : 1.0546081066131592
outputs_pos.loss : 1.6730648279190063
outputs_pos.loss : 1.0179858207702637
outputs_pos.loss : 1.3469022512435913
outputs_pos.loss : 1.630054235458374
outputs_pos.loss : 1.4522186517715454
outputs_pos.loss : 1.0991214513778687
outputs_pos.loss : 1.4572604894638062
outputs_pos.loss : 1.799652338027954
outputs_pos.loss : 1.281915307044983
outputs_pos.loss : 1.027513027191162
outputs_pos.loss : 0.8230283260345459
outputs_pos.loss : 1.182018518447876
outputs_pos.loss : 1.025704026222229
outputs_pos.loss : 0.854093611240387
outputs_pos.loss : 0.8925125598907471
outputs_pos.loss : 0.9444565773010254
outputs_pos.loss : 1.5885697603225708
outputs_pos.loss : 0.8465012907981873
outputs_pos.loss : 1.1557098627090454
outputs_pos.loss : 0.809360682964325
outputs_pos.loss : 1.1781878471374512
outputs_pos.loss : 1.2708451747894287
outputs_pos.loss : 0.744837760925293
outputs_pos.loss : 1.0367026329040527
outputs_pos.loss : 1.3282169103622437
outputs_pos.loss : 0.7692807912826538
outputs_pos.loss : 1.107412576675415
outputs_pos.loss : 0.863414466381073
outputs_pos.loss : 1.2698569297790527
outputs_pos.loss : 1.1808741092681885
outputs_pos.loss : 1.5538921356201172
outputs_pos.loss : 1.3686619997024536
outputs_pos.loss : 1.1298929452896118
outputs_pos.loss : 0.8001468181610107
outputs_pos.loss : 0.8061293363571167
outputs_pos.loss : 1.0102531909942627
outputs_pos.loss : 1.2752031087875366
outputs_pos.loss : 0.9897693395614624
outputs_pos.loss : 1.7618677616119385
outputs_pos.loss : 1.4347155094146729
outputs_pos.loss : 1.0828564167022705
outputs_pos.loss : 1.1982542276382446
outputs_pos.loss : 0.33457499742507935
outputs_pos.loss : 0.9828800559043884
outputs_pos.loss : 1.2967286109924316
outputs_pos.loss : 0.8072119355201721
outputs_pos.loss : 1.2118842601776123
outputs_pos.loss : 1.2349913120269775
outputs_pos.loss : 1.2474805116653442
outputs_pos.loss : 1.639231562614441
outputs_pos.loss : 1.3854458332061768
outputs_pos.loss : 0.7341487407684326
outputs_pos.loss : 0.764165997505188
outputs_pos.loss : 0.8236607313156128
outputs_pos.loss : 0.9739029407501221
outputs_pos.loss : 0.7060539722442627
outputs_pos.loss : 1.2677626609802246
outputs_pos.loss : 1.461419701576233
outputs_pos.loss : 1.378950834274292
outputs_pos.loss : 0.8589277267456055
outputs_pos.loss : 2.6092240810394287
outputs_pos.loss : 1.1203696727752686
outputs_pos.loss : 1.1237215995788574
outputs_pos.loss : 1.050814151763916
outputs_pos.loss : 0.9674577713012695
outputs_pos.loss : 1.182092308998108
outputs_pos.loss : 1.2698014974594116
outputs_pos.loss : 1.5377521514892578
outputs_pos.loss : 1.0441340208053589
outputs_pos.loss : 1.126132845878601
outputs_pos.loss : 1.1561896800994873
outputs_pos.loss : 1.0657235383987427
outputs_pos.loss : 1.385484218597412
outputs_pos.loss : 0.9860652685165405
outputs_pos.loss : 0.91645747423172
outputs_pos.loss : 0.9884192943572998
outputs_pos.loss : 1.0413734912872314
outputs_pos.loss : 1.2597686052322388
outputs_pos.loss : 1.1519981622695923
outputs_pos.loss : 1.4855576753616333
outputs_pos.loss : 0.8926098346710205
outputs_pos.loss : 1.2969026565551758
outputs_pos.loss : 1.2483264207839966
outputs_pos.loss : 1.2722952365875244
outputs_pos.loss : 1.2176921367645264
outputs_pos.loss : 0.9177111387252808
outputs_pos.loss : 1.2894885540008545
outputs_pos.loss : 1.386325478553772
outputs_pos.loss : 1.3151702880859375
outputs_pos.loss : 1.473294734954834
outputs_pos.loss : 1.517626404762268
outputs_pos.loss : 1.0176197290420532
outputs_pos.loss : 1.1320924758911133
outputs_pos.loss : 0.9772144556045532
outputs_pos.loss : 1.129077434539795
outputs_pos.loss : 0.987264096736908
outputs_pos.loss : 1.1363728046417236
outputs_pos.loss : 1.4523509740829468
outputs_pos.loss : 1.2698489427566528
outputs_pos.loss : 0.9396101236343384
outputs_pos.loss : 1.2637869119644165
outputs_pos.loss : 1.9639661312103271
outputs_pos.loss : 0.7907745838165283
outputs_pos.loss : 1.3974629640579224
outputs_pos.loss : 0.9279624819755554
outputs_pos.loss : 0.8120805621147156
outputs_pos.loss : 2.0477941036224365
outputs_pos.loss : 0.9468896389007568
outputs_pos.loss : 1.0504387617111206
outputs_pos.loss : 1.1637907028198242
outputs_pos.loss : 0.9409418702125549
outputs_pos.loss : 1.1793581247329712
outputs_pos.loss : 1.0238879919052124
outputs_pos.loss : 1.5805838108062744
outputs_pos.loss : 1.1850550174713135
outputs_pos.loss : 1.3354177474975586
outputs_pos.loss : 1.69581937789917
outputs_pos.loss : 1.3559958934783936
outputs_pos.loss : 0.9626598954200745
outputs_pos.loss : 1.1774001121520996
outputs_pos.loss : 0.9734079837799072
outputs_pos.loss : 1.4533073902130127
outputs_pos.loss : 1.221398115158081
outputs_pos.loss : 1.7390506267547607
outputs_pos.loss : 0.9933478832244873
outputs_pos.loss : 1.240328073501587
outputs_pos.loss : 1.5116143226623535
outputs_pos.loss : 1.1723098754882812
outputs_pos.loss : 0.9856760501861572
outputs_pos.loss : 0.890141487121582
outputs_pos.loss : 1.3324377536773682
outputs_pos.loss : 0.7448462843894958
outputs_pos.loss : 1.624893069267273
outputs_pos.loss : 0.7558562159538269
outputs_pos.loss : 1.1330561637878418
outputs_pos.loss : 1.001996397972107
outputs_pos.loss : 1.357991099357605
outputs_pos.loss : 1.232458233833313
outputs_pos.loss : 1.4030754566192627
outputs_pos.loss : 1.3722068071365356
outputs_pos.loss : 1.0306472778320312
outputs_pos.loss : 1.5032843351364136
outputs_pos.loss : 0.8099592328071594
outputs_pos.loss : 1.225943922996521
outputs_pos.loss : 1.1246342658996582
outputs_pos.loss : 1.9111419916152954
outputs_pos.loss : 1.2887229919433594
outputs_pos.loss : 0.8861197233200073
outputs_pos.loss : 0.8396672010421753
outputs_pos.loss : 1.5655577182769775
outputs_pos.loss : 1.012024164199829
outputs_pos.loss : 0.8652651906013489
outputs_pos.loss : 0.7492823004722595
outputs_pos.loss : 1.266693115234375
outputs_pos.loss : 1.1161468029022217
outputs_pos.loss : 1.085132360458374
outputs_pos.loss : 1.3177464008331299
outputs_pos.loss : 1.0239430665969849
outputs_pos.loss : 1.3333929777145386
outputs_pos.loss : 1.0395944118499756
outputs_pos.loss : 0.7894962430000305
outputs_pos.loss : 0.7140576243400574
outputs_pos.loss : 1.292689323425293
outputs_pos.loss : 1.0559306144714355
outputs_pos.loss : 0.9467653632164001
outputs_pos.loss : 0.6234791874885559
outputs_pos.loss : 1.3809432983398438
outputs_pos.loss : 0.8760383725166321
outputs_pos.loss : 1.3883508443832397
outputs_pos.loss : 1.450905442237854
outputs_pos.loss : 1.3675938844680786
outputs_pos.loss : 1.0704842805862427
outputs_pos.loss : 1.2013275623321533
outputs_pos.loss : 1.0443249940872192
outputs_pos.loss : 1.0146890878677368
outputs_pos.loss : 0.9255945086479187
outputs_pos.loss : 1.006519079208374
outputs_pos.loss : 1.158316731452942
outputs_pos.loss : 1.2553763389587402
outputs_pos.loss : 0.6446532607078552
outputs_pos.loss : 1.7247432470321655
outputs_pos.loss : 1.3838956356048584
outputs_pos.loss : 1.36818265914917
outputs_pos.loss : 1.0960056781768799
outputs_pos.loss : 1.009896159172058
outputs_pos.loss : 0.9612501263618469
outputs_pos.loss : 2.4674575328826904
outputs_pos.loss : 1.435738444328308
outputs_pos.loss : 0.8616570234298706
outputs_pos.loss : 1.2128320932388306
outputs_pos.loss : 0.9513952732086182
outputs_pos.loss : 0.7869328260421753
outputs_pos.loss : 1.2094604969024658
outputs_pos.loss : 1.1851962804794312
outputs_pos.loss : 0.9674901962280273
outputs_pos.loss : 1.7414077520370483
outputs_pos.loss : 1.2602425813674927
outputs_pos.loss : 1.0852137804031372
outputs_pos.loss : 0.880643904209137
outputs_pos.loss : 0.9420310854911804
outputs_pos.loss : 0.8220198154449463
outputs_pos.loss : 1.0997676849365234
outputs_pos.loss : 0.9814262986183167
outputs_pos.loss : 1.3519539833068848
outputs_pos.loss : 1.32797372341156
outputs_pos.loss : 1.6086368560791016
outputs_pos.loss : 0.8908713459968567
outputs_pos.loss : 1.0950204133987427
outputs_pos.loss : 1.2993837594985962
outputs_pos.loss : 1.4799654483795166
outputs_pos.loss : 1.186767339706421
outputs_pos.loss : 0.9490302205085754
outputs_pos.loss : 1.4332275390625
outputs_pos.loss : 1.1534323692321777
outputs_pos.loss : 1.2744532823562622
outputs_pos.loss : 0.9671758413314819
outputs_pos.loss : 0.8242201209068298
outputs_pos.loss : 1.1994863748550415
outputs_pos.loss : 0.38840433955192566
outputs_pos.loss : 1.2624675035476685
outputs_pos.loss : 1.2867237329483032
outputs_pos.loss : 1.3147403001785278
outputs_pos.loss : 0.7580392360687256
outputs_pos.loss : 1.0983059406280518
outputs_pos.loss : 1.1296900510787964
outputs_pos.loss : 1.1200101375579834
outputs_pos.loss : 0.966205894947052
outputs_pos.loss : 1.2446480989456177
outputs_pos.loss : 1.0448148250579834
outputs_pos.loss : 1.2646673917770386
outputs_pos.loss : 1.1460602283477783
outputs_pos.loss : 0.9768803119659424
outputs_pos.loss : 0.7495886087417603
outputs_pos.loss : 0.6095272898674011
outputs_pos.loss : 1.015993356704712
outputs_pos.loss : 0.988904595375061
outputs_pos.loss : 0.8458850979804993
outputs_pos.loss : 0.9697479009628296
outputs_pos.loss : 1.0435656309127808
outputs_pos.loss : 1.0705691576004028
outputs_pos.loss : 1.5311260223388672
outputs_pos.loss : 0.7921611666679382
outputs_pos.loss : 0.9840166568756104
outputs_pos.loss : 1.0439883470535278
outputs_pos.loss : 1.0463486909866333
outputs_pos.loss : 1.182915210723877
outputs_pos.loss : 1.0108832120895386
outputs_pos.loss : 0.8313484191894531
outputs_pos.loss : 0.9035486578941345
outputs_pos.loss : 1.6744714975357056
outputs_pos.loss : 1.2280367612838745
outputs_pos.loss : 1.2381442785263062
outputs_pos.loss : 1.7924085855484009
outputs_pos.loss : 0.775702178478241
outputs_pos.loss : 1.1962342262268066
outputs_pos.loss : 1.1852253675460815
outputs_pos.loss : 1.249424934387207
outputs_pos.loss : 1.091385841369629
outputs_pos.loss : 1.3951644897460938
outputs_pos.loss : 1.0786457061767578
outputs_pos.loss : 1.4613428115844727
outputs_pos.loss : 0.6189606785774231
outputs_pos.loss : 1.0605708360671997
outputs_pos.loss : 0.8398314714431763
outputs_pos.loss : 1.1388218402862549
outputs_pos.loss : 1.047642469406128
outputs_pos.loss : 1.359018087387085
outputs_pos.loss : 1.3847216367721558
outputs_pos.loss : 0.9150031208992004
outputs_pos.loss : 1.146531343460083
outputs_pos.loss : 1.4147268533706665
outputs_pos.loss : 1.1467708349227905
outputs_pos.loss : 1.4388916492462158
outputs_pos.loss : 1.4861574172973633
outputs_pos.loss : 0.8845070004463196
outputs_pos.loss : 0.9021089673042297
outputs_pos.loss : 0.9132993221282959
outputs_pos.loss : 0.8096027374267578
outputs_pos.loss : 0.8729889988899231
outputs_pos.loss : 1.6089848279953003
outputs_pos.loss : 0.7803058624267578
outputs_pos.loss : 1.165601134300232
outputs_pos.loss : 1.2820688486099243
outputs_pos.loss : 1.0985263586044312
outputs_pos.loss : 0.8008382320404053
outputs_pos.loss : 1.2803844213485718
outputs_pos.loss : 1.032853603363037
outputs_pos.loss : 0.8334654569625854
outputs_pos.loss : 0.8352823257446289
outputs_pos.loss : 0.9988448619842529
outputs_pos.loss : 1.3945547342300415
outputs_pos.loss : 1.187933325767517
outputs_pos.loss : 1.4966336488723755
outputs_pos.loss : 1.7095685005187988
outputs_pos.loss : 1.551527738571167
outputs_pos.loss : 0.5422811508178711
outputs_pos.loss : 1.4435272216796875
outputs_pos.loss : 1.0251803398132324
outputs_pos.loss : 1.1455711126327515
outputs_pos.loss : 0.855969250202179
outputs_pos.loss : 1.0836538076400757
outputs_pos.loss : 0.8439682722091675
outputs_pos.loss : 1.0601783990859985
outputs_pos.loss : 1.1320544481277466
outputs_pos.loss : 0.9765307903289795
outputs_pos.loss : 1.2464165687561035
outputs_pos.loss : 1.2121520042419434
outputs_pos.loss : 1.2939571142196655
outputs_pos.loss : 1.0053229331970215
outputs_pos.loss : 1.5177580118179321
outputs_pos.loss : 1.396238088607788
outputs_pos.loss : 1.0762269496917725
outputs_pos.loss : 1.4458539485931396
outputs_pos.loss : 0.9654231667518616
outputs_pos.loss : 1.2022641897201538
outputs_pos.loss : 1.4343266487121582
outputs_pos.loss : 0.9931203126907349
outputs_pos.loss : 0.9965912699699402
outputs_pos.loss : 0.5995051860809326
outputs_pos.loss : 1.3351258039474487
outputs_pos.loss : 1.2225998640060425
outputs_pos.loss : 0.908707857131958
outputs_pos.loss : 1.0766249895095825
outputs_pos.loss : 1.2486027479171753
outputs_pos.loss : 0.9886765480041504
outputs_pos.loss : 1.2942882776260376
outputs_pos.loss : 1.2075514793395996
outputs_pos.loss : 1.340169072151184
outputs_pos.loss : 1.0813196897506714
outputs_pos.loss : 1.208236575126648
outputs_pos.loss : 1.2201026678085327
outputs_pos.loss : 1.32829749584198
outputs_pos.loss : 0.9709122776985168
outputs_pos.loss : 0.7941209673881531
outputs_pos.loss : 1.5283650159835815
outputs_pos.loss : 1.185747742652893
outputs_pos.loss : 1.527463674545288
outputs_pos.loss : 1.1369247436523438
outputs_pos.loss : 2.181732177734375
outputs_pos.loss : 1.0709542036056519
outputs_pos.loss : 1.1498992443084717
outputs_pos.loss : 1.1498738527297974
outputs_pos.loss : 1.1053591966629028
outputs_pos.loss : 1.6262606382369995
outputs_pos.loss : 1.3739773035049438
outputs_pos.loss : 1.4272642135620117
outputs_pos.loss : 1.030078411102295
outputs_pos.loss : 0.8675506114959717
outputs_pos.loss : 0.8908885717391968
outputs_pos.loss : 1.044101357460022
outputs_pos.loss : 1.1416962146759033
outputs_pos.loss : 1.9851298332214355
outputs_pos.loss : 1.0762118101119995
outputs_pos.loss : 1.3608759641647339
outputs_pos.loss : 1.2475742101669312
outputs_pos.loss : 1.257757544517517
outputs_pos.loss : 0.9637900590896606
outputs_pos.loss : 1.1812478303909302
outputs_pos.loss : 1.143175721168518
outputs_pos.loss : 0.9905303716659546
outputs_pos.loss : 1.4857101440429688
outputs_pos.loss : 1.5729209184646606
outputs_pos.loss : 1.2300955057144165
outputs_pos.loss : 1.0283797979354858
outputs_pos.loss : 1.2896969318389893
outputs_pos.loss : 1.080864429473877
outputs_pos.loss : 1.1292749643325806
outputs_pos.loss : 0.7735870480537415
outputs_pos.loss : 1.1228997707366943
outputs_pos.loss : 1.332624912261963
outputs_pos.loss : 1.1032788753509521
outputs_pos.loss : 0.9838438630104065
outputs_pos.loss : 0.7540345191955566
outputs_pos.loss : 0.8959611058235168
outputs_pos.loss : 0.670462965965271
outputs_pos.loss : 1.0026715993881226
outputs_pos.loss : 0.9419994950294495
outputs_pos.loss : 0.8424321413040161
outputs_pos.loss : 1.0068457126617432
outputs_pos.loss : 1.0796432495117188
outputs_pos.loss : 1.7143855094909668
outputs_pos.loss : 0.8942814469337463
outputs_pos.loss : 1.2253400087356567
outputs_pos.loss : 1.152368426322937
outputs_pos.loss : 1.2400226593017578
outputs_pos.loss : 1.0827206373214722
outputs_pos.loss : 1.0123116970062256
outputs_pos.loss : 0.6761388182640076
outputs_pos.loss : 1.1025114059448242
outputs_pos.loss : 1.913273572921753
outputs_pos.loss : 1.5272767543792725
outputs_pos.loss : 1.1109826564788818
outputs_pos.loss : 1.0273890495300293
outputs_pos.loss : 0.9877066612243652
outputs_pos.loss : 1.2526189088821411
outputs_pos.loss : 1.3454126119613647
outputs_pos.loss : 1.0345075130462646
outputs_pos.loss : 0.9705371260643005
outputs_pos.loss : 1.3448677062988281
outputs_pos.loss : 0.9558340907096863
outputs_pos.loss : 1.0758957862854004
outputs_pos.loss : 1.4702860116958618
outputs_pos.loss : 1.1683032512664795
outputs_pos.loss : 1.1397627592086792
outputs_pos.loss : 1.1239500045776367
outputs_pos.loss : 1.4165713787078857
outputs_pos.loss : 1.6004548072814941
outputs_pos.loss : 1.0556806325912476
outputs_pos.loss : 1.2572749853134155
outputs_pos.loss : 1.224077582359314
outputs_pos.loss : 1.2149121761322021
outputs_pos.loss : 1.6547658443450928
outputs_pos.loss : 1.141533613204956
outputs_pos.loss : 1.066628098487854
outputs_pos.loss : 0.9218668341636658
outputs_pos.loss : 1.1404799222946167
outputs_pos.loss : 1.514594554901123
outputs_pos.loss : 0.9854271411895752
outputs_pos.loss : 0.9753548502922058
outputs_pos.loss : 1.199359655380249
outputs_pos.loss : 1.071001648902893
outputs_pos.loss : 0.8279068470001221
outputs_pos.loss : 1.3042404651641846
outputs_pos.loss : 0.7745630145072937
outputs_pos.loss : 1.154220461845398
outputs_pos.loss : 0.9603780508041382
outputs_pos.loss : 1.2046767473220825
outputs_pos.loss : 1.0237369537353516
outputs_pos.loss : 1.0258147716522217
outputs_pos.loss : 1.087875247001648
outputs_pos.loss : 1.153298258781433
outputs_pos.loss : 1.0748605728149414
outputs_pos.loss : 1.3759973049163818
outputs_pos.loss : 0.9166726469993591
outputs_pos.loss : 1.132267713546753
outputs_pos.loss : 1.5579806566238403
outputs_pos.loss : 1.3261297941207886
outputs_pos.loss : 1.3079891204833984
outputs_pos.loss : 0.7302261590957642
outputs_pos.loss : 1.2945234775543213
outputs_pos.loss : 1.2965161800384521
outputs_pos.loss : 1.188176155090332
outputs_pos.loss : 1.3890472650527954
outputs_pos.loss : 1.24430513381958
outputs_pos.loss : 0.9915179014205933
outputs_pos.loss : 1.9129916429519653
outputs_pos.loss : 1.5088852643966675
outputs_pos.loss : 1.2060458660125732
outputs_pos.loss : 1.3191949129104614
outputs_pos.loss : 0.9707682728767395
outputs_pos.loss : 1.2927480936050415
outputs_pos.loss : 1.4261407852172852
outputs_pos.loss : 1.1185232400894165
outputs_pos.loss : 1.4038515090942383
outputs_pos.loss : 1.645798921585083
outputs_pos.loss : 1.2460230588912964
outputs_pos.loss : 1.2421528100967407
outputs_pos.loss : 1.2975057363510132
outputs_pos.loss : 0.9619185328483582
outputs_pos.loss : 1.180298089981079
outputs_pos.loss : 1.2672960758209229
outputs_pos.loss : 0.9438585042953491
outputs_pos.loss : 1.4514415264129639
outputs_pos.loss : 0.8495997190475464
outputs_pos.loss : 1.239173412322998
outputs_pos.loss : 1.6914492845535278
outputs_pos.loss : 0.8628575205802917
outputs_pos.loss : 1.3642747402191162
outputs_pos.loss : 1.7619963884353638
outputs_pos.loss : 0.9083359837532043
outputs_pos.loss : 1.3765162229537964
outputs_pos.loss : 1.2638955116271973
outputs_pos.loss : 1.1062580347061157
outputs_pos.loss : 1.5556480884552002
outputs_pos.loss : 0.8533536195755005
outputs_pos.loss : 0.8614204525947571
outputs_pos.loss : 1.0696173906326294
outputs_pos.loss : 1.5064886808395386
outputs_pos.loss : 1.0872225761413574
outputs_pos.loss : 1.5771701335906982
outputs_pos.loss : 1.4678807258605957
outputs_pos.loss : 0.9957163333892822
outputs_pos.loss : 1.1952654123306274
outputs_pos.loss : 0.9794179201126099
outputs_pos.loss : 1.3611067533493042
outputs_pos.loss : 1.3072866201400757
outputs_pos.loss : 1.298117756843567
outputs_pos.loss : 1.1731501817703247
outputs_pos.loss : 0.9486818909645081
outputs_pos.loss : 1.4126185178756714
outputs_pos.loss : 0.892936110496521
outputs_pos.loss : 0.9990782141685486
outputs_pos.loss : 0.9756634831428528
outputs_pos.loss : 1.0954127311706543
outputs_pos.loss : 0.8780344128608704
outputs_pos.loss : 1.5481764078140259
outputs_pos.loss : 1.4218485355377197
outputs_pos.loss : 1.2298693656921387
outputs_pos.loss : 1.0671426057815552
outputs_pos.loss : 0.8911353945732117
outputs_pos.loss : 1.6057841777801514
outputs_pos.loss : 1.294739842414856
outputs_pos.loss : 1.0806148052215576
outputs_pos.loss : 1.0925681591033936
outputs_pos.loss : 1.21387779712677
outputs_pos.loss : 0.970562756061554
outputs_pos.loss : 1.2499366998672485
outputs_pos.loss : 1.1892210245132446
outputs_pos.loss : 1.1309912204742432
outputs_pos.loss : 1.1601507663726807
outputs_pos.loss : 0.8716961741447449
outputs_pos.loss : 1.4901598691940308
outputs_pos.loss : 1.3296552896499634
outputs_pos.loss : 1.3338418006896973
outputs_pos.loss : 1.7330152988433838
outputs_pos.loss : 0.886729896068573
outputs_pos.loss : 1.1998850107192993
outputs_pos.loss : 1.07014799118042
outputs_pos.loss : 0.8731774687767029
outputs_pos.loss : 1.1390814781188965
outputs_pos.loss : 0.7695319056510925
outputs_pos.loss : 1.169480800628662
outputs_pos.loss : 1.3307571411132812
outputs_pos.loss : 0.638694703578949
outputs_pos.loss : 1.3237957954406738
outputs_pos.loss : 1.0254021883010864
outputs_pos.loss : 1.3188968896865845
outputs_pos.loss : 1.0722945928573608
outputs_pos.loss : 0.9557449817657471
outputs_pos.loss : 0.775266170501709
outputs_pos.loss : 1.0575000047683716
outputs_pos.loss : 0.9569307565689087
outputs_pos.loss : 0.7520628571510315
outputs_pos.loss : 1.2281054258346558
outputs_pos.loss : 1.5334606170654297
outputs_pos.loss : 1.2518635988235474
outputs_pos.loss : 0.9136194586753845
outputs_pos.loss : 1.2583364248275757
outputs_pos.loss : 1.1839205026626587
outputs_pos.loss : 1.8024420738220215
outputs_pos.loss : 1.4163365364074707
outputs_pos.loss : 1.1502165794372559
outputs_pos.loss : 1.2214429378509521
outputs_pos.loss : 1.0714530944824219
outputs_pos.loss : 1.1623374223709106
outputs_pos.loss : 1.4834638833999634
outputs_pos.loss : 1.0228160619735718
outputs_pos.loss : 2.154920816421509
outputs_pos.loss : 1.439868450164795
outputs_pos.loss : 0.8478561043739319
outputs_pos.loss : 0.945764422416687
outputs_pos.loss : 0.9876998662948608
outputs_pos.loss : 1.0312986373901367
outputs_pos.loss : 1.714813470840454
outputs_pos.loss : 1.0024545192718506
outputs_pos.loss : 1.1742645502090454
outputs_pos.loss : 0.7297040224075317
outputs_pos.loss : 1.2069956064224243
outputs_pos.loss : 0.975916862487793
outputs_pos.loss : 0.8676333427429199
outputs_pos.loss : 1.4542683362960815
outputs_pos.loss : 0.7784606218338013
outputs_pos.loss : 1.1607825756072998
outputs_pos.loss : 1.1307034492492676
outputs_pos.loss : 1.4532811641693115
outputs_pos.loss : 0.8946746587753296
outputs_pos.loss : 0.9558395743370056
outputs_pos.loss : 0.903144896030426
outputs_pos.loss : 0.7826533913612366
outputs_pos.loss : 1.319383144378662
outputs_pos.loss : 1.043987512588501
outputs_pos.loss : 1.4809678792953491
outputs_pos.loss : 0.8748849630355835
outputs_pos.loss : 0.9358248114585876
outputs_pos.loss : 1.1989785432815552
outputs_pos.loss : 0.9834232330322266
outputs_pos.loss : 0.8952500820159912
outputs_pos.loss : 1.2514616250991821
outputs_pos.loss : 0.7315710186958313
outputs_pos.loss : 0.8632851839065552
outputs_pos.loss : 1.0260605812072754
outputs_pos.loss : 1.0345453023910522
outputs_pos.loss : 0.9702621102333069
outputs_pos.loss : 1.2393300533294678
outputs_pos.loss : 1.2647868394851685
outputs_pos.loss : 0.9801254272460938
outputs_pos.loss : 1.4784315824508667
outputs_pos.loss : 0.9029130339622498
outputs_pos.loss : 0.9913813471794128
outputs_pos.loss : 1.035402774810791
outputs_pos.loss : 1.1521466970443726
outputs_pos.loss : 1.3710558414459229
outputs_pos.loss : 0.9027354717254639
outputs_pos.loss : 0.8573949337005615
outputs_pos.loss : 1.2759613990783691
outputs_pos.loss : 1.1236169338226318
outputs_pos.loss : 0.8556479811668396
outputs_pos.loss : 0.7629548907279968
outputs_pos.loss : 0.898054301738739
outputs_pos.loss : 0.7940770983695984
outputs_pos.loss : 0.8795055747032166
outputs_pos.loss : 1.3851765394210815
outputs_pos.loss : 0.7957752346992493
outputs_pos.loss : 1.39205002784729
outputs_pos.loss : 1.8273557424545288
outputs_pos.loss : 1.1994242668151855
outputs_pos.loss : 1.2874830961227417
outputs_pos.loss : 1.3052494525909424
outputs_pos.loss : 1.1707804203033447
outputs_pos.loss : 1.1433384418487549
outputs_pos.loss : 1.0021536350250244
outputs_pos.loss : 0.776871383190155
outputs_pos.loss : 1.397895336151123
outputs_pos.loss : 1.3328032493591309
outputs_pos.loss : 1.1853346824645996
outputs_pos.loss : 0.8324995636940002
outputs_pos.loss : 1.7888977527618408
outputs_pos.loss : 1.2416764497756958
outputs_pos.loss : 1.13397216796875
outputs_pos.loss : 1.068533182144165
outputs_pos.loss : 1.0504673719406128
outputs_pos.loss : 1.5795936584472656
outputs_pos.loss : 1.1778072118759155
outputs_pos.loss : 0.9568191170692444
outputs_pos.loss : 1.659859538078308
outputs_pos.loss : 0.8496394753456116
outputs_pos.loss : 1.1044358015060425
outputs_pos.loss : 1.2297805547714233
outputs_pos.loss : 1.1650731563568115
outputs_pos.loss : 0.9866489768028259
outputs_pos.loss : 0.9582312703132629
outputs_pos.loss : 1.0475136041641235
outputs_pos.loss : 0.7874765992164612
outputs_pos.loss : 1.2645659446716309
outputs_pos.loss : 1.0450496673583984
outputs_pos.loss : 1.2067921161651611
outputs_pos.loss : 1.1115249395370483
outputs_pos.loss : 1.071793794631958
outputs_pos.loss : 1.4533871412277222
outputs_pos.loss : 1.2597565650939941
outputs_pos.loss : 0.8307057023048401
outputs_pos.loss : 1.5573346614837646
outputs_pos.loss : 0.6279032826423645
outputs_pos.loss : 1.3220205307006836
outputs_pos.loss : 1.2358704805374146
outputs_pos.loss : 1.028907299041748
outputs_pos.loss : 1.293389916419983
outputs_pos.loss : 1.1255182027816772
outputs_pos.loss : 1.219702124595642
outputs_pos.loss : 0.9564081430435181
outputs_pos.loss : 1.0333163738250732
outputs_pos.loss : 1.0389206409454346
outputs_pos.loss : 1.0596723556518555
outputs_pos.loss : 0.8425818681716919
outputs_pos.loss : 1.0777877569198608
outputs_pos.loss : 1.2774662971496582
outputs_pos.loss : 1.2919026613235474
outputs_pos.loss : 1.4773629903793335
outputs_pos.loss : 1.0809518098831177
outputs_pos.loss : 0.9567225575447083
outputs_pos.loss : 1.2332394123077393
outputs_pos.loss : 0.8079425692558289
outputs_pos.loss : 1.5405848026275635
outputs_pos.loss : 1.2477314472198486
outputs_pos.loss : 1.0016752481460571
outputs_pos.loss : 1.2851285934448242
outputs_pos.loss : 1.3135356903076172
outputs_pos.loss : 1.0316381454467773
outputs_pos.loss : 0.9911344647407532
outputs_pos.loss : 1.6139193773269653
outputs_pos.loss : 0.9857200980186462
outputs_pos.loss : 1.0993508100509644
outputs_pos.loss : 1.0510083436965942
outputs_pos.loss : 0.8748308420181274
outputs_pos.loss : 0.9370781779289246
outputs_pos.loss : 0.6785538792610168
outputs_pos.loss : 0.896541178226471
outputs_pos.loss : 0.8834787011146545
outputs_pos.loss : 1.4476484060287476
outputs_pos.loss : 0.9901564121246338
outputs_pos.loss : 1.1317212581634521
outputs_pos.loss : 1.039117455482483
outputs_pos.loss : 1.5089607238769531
outputs_pos.loss : 1.067222237586975
outputs_pos.loss : 1.132054090499878
outputs_pos.loss : 0.9888073205947876
outputs_pos.loss : 0.7167072892189026
outputs_pos.loss : 1.4264498949050903
outputs_pos.loss : 1.388819694519043
outputs_pos.loss : 1.0201778411865234
outputs_pos.loss : 1.1220048666000366
outputs_pos.loss : 1.22982919216156
outputs_pos.loss : 0.7077829241752625
outputs_pos.loss : 1.0547263622283936
outputs_pos.loss : 0.9330993294715881
outputs_pos.loss : 0.9822854995727539
outputs_pos.loss : 1.0612276792526245
outputs_pos.loss : 0.9245674014091492
outputs_pos.loss : 1.0809249877929688
outputs_pos.loss : 1.0617870092391968
outputs_pos.loss : 1.1422351598739624
outputs_pos.loss : 1.0067402124404907
outputs_pos.loss : 1.1242694854736328
outputs_pos.loss : 1.015828251838684
outputs_pos.loss : 0.9078842401504517
outputs_pos.loss : 1.58369779586792
outputs_pos.loss : 2.1314187049865723
outputs_pos.loss : 1.363532304763794
outputs_pos.loss : 0.962500274181366
outputs_pos.loss : 1.0516581535339355
outputs_pos.loss : 1.2348672151565552
outputs_pos.loss : 1.5856313705444336
outputs_pos.loss : 1.0004850625991821
outputs_pos.loss : 0.8331089019775391
outputs_pos.loss : 1.2127619981765747
outputs_pos.loss : 1.1214728355407715
outputs_pos.loss : 0.6181403398513794
outputs_pos.loss : 1.7079834938049316
outputs_pos.loss : 1.2523982524871826
outputs_pos.loss : 1.2091134786605835
outputs_pos.loss : 1.1740485429763794
outputs_pos.loss : 0.9303543567657471
outputs_pos.loss : 1.1451096534729004
outputs_pos.loss : 1.2229958772659302
outputs_pos.loss : 0.8867162466049194
outputs_pos.loss : 1.1479018926620483
outputs_pos.loss : 1.1620919704437256
outputs_pos.loss : 1.636000394821167
outputs_pos.loss : 0.903647243976593
outputs_pos.loss : 1.0580514669418335
outputs_pos.loss : 1.116377592086792
outputs_pos.loss : 1.384276270866394
outputs_pos.loss : 1.6154818534851074
outputs_pos.loss : 0.9985230565071106
outputs_pos.loss : 1.1491358280181885
outputs_pos.loss : 1.1826980113983154
outputs_pos.loss : 0.9004111289978027
outputs_pos.loss : 1.1549779176712036
outputs_pos.loss : 0.9124332666397095
outputs_pos.loss : 0.9750737547874451
outputs_pos.loss : 1.0944480895996094
outputs_pos.loss : 1.177533507347107
outputs_pos.loss : 1.0009726285934448
outputs_pos.loss : 1.2390214204788208
outputs_pos.loss : 0.9233500957489014
outputs_pos.loss : 0.9121469259262085
outputs_pos.loss : 1.455116868019104
outputs_pos.loss : 0.8161150217056274
outputs_pos.loss : 1.08135986328125
outputs_pos.loss : 1.211771011352539
outputs_pos.loss : 0.7880493998527527
outputs_pos.loss : 1.9400924444198608
outputs_pos.loss : 1.3699437379837036
outputs_pos.loss : 0.9082963466644287
outputs_pos.loss : 1.4574345350265503
outputs_pos.loss : 1.0529067516326904
outputs_pos.loss : 0.963036835193634
outputs_pos.loss : 0.8297727108001709
outputs_pos.loss : 1.0334737300872803
outputs_pos.loss : 1.1060513257980347
outputs_pos.loss : 1.7314571142196655
outputs_pos.loss : 1.074145793914795
outputs_pos.loss : 0.8975170850753784
outputs_pos.loss : 1.5454853773117065
outputs_pos.loss : 0.9815919995307922
outputs_pos.loss : 1.2319674491882324
outputs_pos.loss : 1.2095445394515991
outputs_pos.loss : 1.0537337064743042
outputs_pos.loss : 1.2891511917114258
outputs_pos.loss : 1.3784314393997192
outputs_pos.loss : 1.003851056098938
outputs_pos.loss : 0.915899395942688
outputs_pos.loss : 0.9443922638893127
outputs_pos.loss : 0.9399347305297852
outputs_pos.loss : 1.047126293182373
outputs_pos.loss : 1.056301236152649
outputs_pos.loss : 1.1617966890335083
outputs_pos.loss : 1.6545661687850952
outputs_pos.loss : 1.2166978120803833
outputs_pos.loss : 0.9851110577583313
outputs_pos.loss : 1.2385982275009155
outputs_pos.loss : 0.9718264937400818
outputs_pos.loss : 1.045315146446228
outputs_pos.loss : 1.0585732460021973
outputs_pos.loss : 1.0876567363739014
outputs_pos.loss : 1.1186158657073975
outputs_pos.loss : 1.201444387435913
outputs_pos.loss : 1.3490922451019287
outputs_pos.loss : 1.0604575872421265
outputs_pos.loss : 0.632225513458252
outputs_pos.loss : 0.9592623710632324
outputs_pos.loss : 0.9643714427947998
outputs_pos.loss : 1.0520687103271484
outputs_pos.loss : 1.8205926418304443
outputs_pos.loss : 0.8547409772872925
outputs_pos.loss : 1.1391839981079102
outputs_pos.loss : 1.4580283164978027
outputs_pos.loss : 1.065606951713562
outputs_pos.loss : 1.3488776683807373
outputs_pos.loss : 1.0621566772460938
outputs_pos.loss : 1.2115236520767212
outputs_pos.loss : 1.5392316579818726
outputs_pos.loss : 1.1005533933639526
outputs_pos.loss : 0.9391185641288757
outputs_pos.loss : 1.0840522050857544
outputs_pos.loss : 1.676216721534729
outputs_pos.loss : 1.0572718381881714
outputs_pos.loss : 1.0427414178848267
outputs_pos.loss : 1.6124216318130493
outputs_pos.loss : 0.8943033814430237
outputs_pos.loss : 1.2911854982376099
outputs_pos.loss : 1.1181572675704956
outputs_pos.loss : 1.0636721849441528
outputs_pos.loss : 1.0117721557617188
outputs_pos.loss : 0.8740140795707703
outputs_pos.loss : 0.8891441822052002
outputs_pos.loss : 1.1605780124664307
outputs_pos.loss : 0.9942863583564758
outputs_pos.loss : 1.164452314376831
outputs_pos.loss : 1.0654460191726685
outputs_pos.loss : 1.4467601776123047
outputs_pos.loss : 1.0805063247680664
outputs_pos.loss : 1.0678255558013916
outputs_pos.loss : 1.8226677179336548
outputs_pos.loss : 1.2922381162643433
outputs_pos.loss : 1.9067459106445312
outputs_pos.loss : 1.1876603364944458
outputs_pos.loss : 1.2368245124816895
outputs_pos.loss : 1.0571993589401245
outputs_pos.loss : 1.0012187957763672
outputs_pos.loss : 1.1404621601104736
outputs_pos.loss : 0.9322295784950256
outputs_pos.loss : 1.1001068353652954
outputs_pos.loss : 1.4272096157073975
outputs_pos.loss : 1.147518515586853
outputs_pos.loss : 1.5771524906158447
outputs_pos.loss : 0.9145978689193726
outputs_pos.loss : 1.0426180362701416
outputs_pos.loss : 1.2220948934555054
outputs_pos.loss : 1.003591537475586
outputs_pos.loss : 1.2568120956420898
outputs_pos.loss : 0.8503274917602539
outputs_pos.loss : 0.831356406211853
outputs_pos.loss : 0.868493914604187
outputs_pos.loss : 1.1367160081863403
outputs_pos.loss : 1.2844867706298828
outputs_pos.loss : 1.1616448163986206
outputs_pos.loss : 1.2079697847366333
outputs_pos.loss : 1.116955041885376
outputs_pos.loss : 1.0104199647903442
outputs_pos.loss : 1.3678110837936401
outputs_pos.loss : 0.9960166215896606
outputs_pos.loss : 0.9885499477386475
outputs_pos.loss : 0.903814435005188
outputs_pos.loss : 1.0261837244033813
outputs_pos.loss : 0.9546608328819275
outputs_pos.loss : 1.3815724849700928
outputs_pos.loss : 0.8934730887413025
outputs_pos.loss : 1.5834221839904785
outputs_pos.loss : 0.9961860179901123
outputs_pos.loss : 1.1443545818328857
outputs_pos.loss : 0.9475365877151489
outputs_pos.loss : 1.0089133977890015
outputs_pos.loss : 1.7412261962890625
outputs_pos.loss : 1.2143248319625854
outputs_pos.loss : 0.9987999200820923
outputs_pos.loss : 1.02182936668396
outputs_pos.loss : 1.2506685256958008
outputs_pos.loss : 0.9630976319313049
outputs_pos.loss : 1.0556364059448242
outputs_pos.loss : 0.7779251337051392
outputs_pos.loss : 1.3618172407150269
outputs_pos.loss : 1.637237310409546
outputs_pos.loss : 1.0867880582809448
outputs_pos.loss : 0.8648900389671326
outputs_pos.loss : 1.3828758001327515
outputs_pos.loss : 1.212493658065796
outputs_pos.loss : 1.2163375616073608
outputs_pos.loss : 1.321123480796814
outputs_pos.loss : 1.438011646270752
outputs_pos.loss : 1.4626754522323608
outputs_pos.loss : 1.2927591800689697
outputs_pos.loss : 1.1498769521713257
outputs_pos.loss : 1.0125783681869507
outputs_pos.loss : 0.9186469912528992
outputs_pos.loss : 1.3978344202041626
outputs_pos.loss : 1.539909839630127
outputs_pos.loss : 1.1469371318817139
outputs_pos.loss : 1.0869413614273071
outputs_pos.loss : 1.2903342247009277
outputs_pos.loss : 1.1647640466690063
outputs_pos.loss : 0.7319293022155762
outputs_pos.loss : 1.3802549839019775
outputs_pos.loss : 1.3746976852416992
outputs_pos.loss : 0.9835188388824463
outputs_pos.loss : 0.9606829285621643
outputs_pos.loss : 1.1009596586227417
outputs_pos.loss : 1.4782390594482422
outputs_pos.loss : 0.8435809016227722
outputs_pos.loss : 1.2372876405715942
outputs_pos.loss : 1.7189834117889404
outputs_pos.loss : 1.3321309089660645
outputs_pos.loss : 1.0008306503295898
outputs_pos.loss : 1.195541501045227
outputs_pos.loss : 1.459855079650879
outputs_pos.loss : 1.135788083076477
outputs_pos.loss : 1.121393084526062
outputs_pos.loss : 1.2968778610229492
outputs_pos.loss : 1.3611103296279907
outputs_pos.loss : 1.2821696996688843
outputs_pos.loss : 0.9831826090812683
outputs_pos.loss : 0.8359962105751038
outputs_pos.loss : 1.2201272249221802
outputs_pos.loss : 1.245643138885498
outputs_pos.loss : 0.9692344069480896
outputs_pos.loss : 0.9082356691360474
outputs_pos.loss : 1.245674967765808
outputs_pos.loss : 1.0784837007522583
outputs_pos.loss : 1.6019381284713745
outputs_pos.loss : 0.5709403157234192
outputs_pos.loss : 1.065690517425537
outputs_pos.loss : 1.2676329612731934
outputs_pos.loss : 1.2885372638702393
outputs_pos.loss : 0.9722858667373657
outputs_pos.loss : 1.2376375198364258
outputs_pos.loss : 1.1866651773452759
outputs_pos.loss : 1.1970621347427368
outputs_pos.loss : 1.168288230895996
outputs_pos.loss : 0.9768608212471008
outputs_pos.loss : 1.1991182565689087
outputs_pos.loss : 1.4792009592056274
outputs_pos.loss : 1.2166030406951904
outputs_pos.loss : 1.0235613584518433
outputs_pos.loss : 1.3789435625076294
outputs_pos.loss : 0.8127778768539429
outputs_pos.loss : 1.1398793458938599
outputs_pos.loss : 1.5297279357910156
outputs_pos.loss : 1.5581223964691162
outputs_pos.loss : 0.9849991202354431
outputs_pos.loss : 1.163365125656128
outputs_pos.loss : 1.0629507303237915
outputs_pos.loss : 1.1107197999954224
outputs_pos.loss : 0.8996028900146484
outputs_pos.loss : 0.8257576823234558
outputs_pos.loss : 1.2971574068069458
outputs_pos.loss : 1.5569336414337158
outputs_pos.loss : 0.8265377283096313
outputs_pos.loss : 1.3931798934936523
outputs_pos.loss : 0.9565873146057129
outputs_pos.loss : 0.8530580401420593
outputs_pos.loss : 1.0006637573242188
outputs_pos.loss : 1.4271023273468018
outputs_pos.loss : 0.7415057420730591
outputs_pos.loss : 1.729137897491455
outputs_pos.loss : 1.0877617597579956
outputs_pos.loss : 1.0490472316741943
outputs_pos.loss : 1.0619533061981201
outputs_pos.loss : 1.3598499298095703
outputs_pos.loss : 0.986783504486084
outputs_pos.loss : 2.4365148544311523
outputs_pos.loss : 1.745285153388977
outputs_pos.loss : 0.9159120321273804
outputs_pos.loss : 1.0588710308074951
outputs_pos.loss : 1.2435022592544556
outputs_pos.loss : 0.7746487855911255
outputs_pos.loss : 1.1824315786361694
outputs_pos.loss : 1.0786644220352173
outputs_pos.loss : 0.9906575083732605
outputs_pos.loss : 0.916252613067627
outputs_pos.loss : 1.2167747020721436
outputs_pos.loss : 1.2149488925933838
outputs_pos.loss : 1.1611368656158447
outputs_pos.loss : 1.4230326414108276
outputs_pos.loss : 0.914747416973114
outputs_pos.loss : 1.4751288890838623
outputs_pos.loss : 0.5327029228210449
outputs_pos.loss : 0.8842014074325562
outputs_pos.loss : 1.2604265213012695
outputs_pos.loss : 1.3642209768295288
outputs_pos.loss : 1.1734131574630737
outputs_pos.loss : 0.9874220490455627
outputs_pos.loss : 1.8011343479156494
outputs_pos.loss : 1.172844648361206
outputs_pos.loss : 1.2588090896606445
outputs_pos.loss : 0.8271133899688721
outputs_pos.loss : 0.8924180865287781
outputs_pos.loss : 1.0863720178604126
outputs_pos.loss : 1.0142645835876465
outputs_pos.loss : 1.2372865676879883
outputs_pos.loss : 0.775462806224823
outputs_pos.loss : 0.7683551907539368
outputs_pos.loss : 1.2429084777832031
outputs_pos.loss : 0.9393134117126465
outputs_pos.loss : 1.1171936988830566
outputs_pos.loss : 1.2240155935287476
outputs_pos.loss : 1.097141981124878
outputs_pos.loss : 1.366448998451233
outputs_pos.loss : 1.102037787437439
outputs_pos.loss : 1.3027557134628296
outputs_pos.loss : 1.3108718395233154
outputs_pos.loss : 1.328747034072876
outputs_pos.loss : 1.6067332029342651
outputs_pos.loss : 1.0354838371276855
outputs_pos.loss : 0.8555566072463989
outputs_pos.loss : 1.2248200178146362
outputs_pos.loss : 1.165930151939392
outputs_pos.loss : 0.848401665687561
outputs_pos.loss : 1.2984002828598022
outputs_pos.loss : 1.4175922870635986
outputs_pos.loss : 1.3934484720230103
outputs_pos.loss : 1.3096261024475098
outputs_pos.loss : 0.930187463760376
outputs_pos.loss : 0.7416056990623474
outputs_pos.loss : 1.0865578651428223
outputs_pos.loss : 1.1825424432754517
outputs_pos.loss : 1.4358030557632446
outputs_pos.loss : 1.119541049003601
outputs_pos.loss : 1.2636209726333618
outputs_pos.loss : 1.2131999731063843
outputs_pos.loss : 0.7889997959136963
outputs_pos.loss : 1.2690671682357788
outputs_pos.loss : 1.557611107826233
outputs_pos.loss : 1.0214672088623047
outputs_pos.loss : 0.7023503184318542
outputs_pos.loss : 1.3694123029708862
outputs_pos.loss : 1.4979631900787354
outputs_pos.loss : 1.4516785144805908
outputs_pos.loss : 1.0387147665023804
outputs_pos.loss : 1.170594334602356
outputs_pos.loss : 1.0579359531402588
outputs_pos.loss : 0.8718544244766235
outputs_pos.loss : 0.8815348148345947
outputs_pos.loss : 0.9154773950576782
outputs_pos.loss : 1.0791077613830566
outputs_pos.loss : 1.0634492635726929
outputs_pos.loss : 1.246382713317871
outputs_pos.loss : 1.8441487550735474
outputs_pos.loss : 0.9686588644981384
outputs_pos.loss : 1.2148325443267822
outputs_pos.loss : 1.068461537361145
outputs_pos.loss : 1.3030973672866821
outputs_pos.loss : 1.1970698833465576
outputs_pos.loss : 1.3562350273132324
outputs_pos.loss : 1.3669265508651733
outputs_pos.loss : 1.1155482530593872
outputs_pos.loss : 1.4125123023986816
outputs_pos.loss : 0.7445821762084961
outputs_pos.loss : 1.2438052892684937
outputs_pos.loss : 1.093785285949707
outputs_pos.loss : 0.9371991157531738
outputs_pos.loss : 1.0149275064468384
outputs_pos.loss : 1.541773796081543
outputs_pos.loss : 1.3519905805587769
outputs_pos.loss : 0.9523552656173706
outputs_pos.loss : 0.9281430244445801
outputs_pos.loss : 1.0765873193740845
outputs_pos.loss : 0.8692774176597595
outputs_pos.loss : 0.8421984314918518
outputs_pos.loss : 1.12478506565094
outputs_pos.loss : 1.1696771383285522
outputs_pos.loss : 1.913427710533142
outputs_pos.loss : 1.1809858083724976
outputs_pos.loss : 0.8352891802787781
outputs_pos.loss : 1.9493603706359863
outputs_pos.loss : 1.9107321500778198
outputs_pos.loss : 1.322661280632019
outputs_pos.loss : 1.232380986213684
outputs_pos.loss : 1.0298173427581787
outputs_pos.loss : 1.7552752494812012
outputs_pos.loss : 1.3224722146987915
outputs_pos.loss : 1.2957226037979126
outputs_pos.loss : 1.0507533550262451
outputs_pos.loss : 1.0797638893127441
outputs_pos.loss : 0.9838334918022156
outputs_pos.loss : 0.9023227691650391
outputs_pos.loss : 0.8613504767417908
outputs_pos.loss : 0.7638674378395081
outputs_pos.loss : 1.073554277420044
outputs_pos.loss : 0.9843491315841675
outputs_pos.loss : 1.7541753053665161
outputs_pos.loss : 1.260372519493103
outputs_pos.loss : 0.8341466784477234
outputs_pos.loss : 1.2535929679870605
outputs_pos.loss : 1.1210843324661255
outputs_pos.loss : 1.6042741537094116
outputs_pos.loss : 1.0514237880706787
outputs_pos.loss : 0.9579572677612305
outputs_pos.loss : 1.1133917570114136
outputs_pos.loss : 1.414434552192688
outputs_pos.loss : 1.6522527933120728
outputs_pos.loss : 1.1087182760238647
outputs_pos.loss : 1.3451426029205322
outputs_pos.loss : 1.1707589626312256
outputs_pos.loss : 0.9226245284080505
outputs_pos.loss : 1.175480842590332
outputs_pos.loss : 0.782945990562439
outputs_pos.loss : 1.6258289813995361
outputs_pos.loss : 1.0733829736709595
outputs_pos.loss : 1.621120810508728
outputs_pos.loss : 0.9405811429023743
outputs_pos.loss : 1.3346487283706665
outputs_pos.loss : 1.3100342750549316
outputs_pos.loss : 0.835989773273468
outputs_pos.loss : 1.044144868850708
outputs_pos.loss : 1.2529261112213135
outputs_pos.loss : 1.0742700099945068
outputs_pos.loss : 1.011916995048523
outputs_pos.loss : 1.131777286529541
outputs_pos.loss : 0.9010111689567566
outputs_pos.loss : 1.0487459897994995
outputs_pos.loss : 1.0813475847244263
outputs_pos.loss : 0.8974742293357849
outputs_pos.loss : 0.9601477980613708
outputs_pos.loss : 1.0207098722457886
outputs_pos.loss : 0.9528528451919556
outputs_pos.loss : 0.9831127524375916
outputs_pos.loss : 1.1959434747695923
outputs_pos.loss : 1.180046796798706
outputs_pos.loss : 1.1157443523406982
outputs_pos.loss : 0.8641132116317749
outputs_pos.loss : 1.3692985773086548
outputs_pos.loss : 1.2634968757629395
outputs_pos.loss : 0.9929057359695435
outputs_pos.loss : 1.3481565713882446
outputs_pos.loss : 0.813170313835144
outputs_pos.loss : 1.23112952709198
outputs_pos.loss : 1.143592357635498
outputs_pos.loss : 1.0304768085479736
outputs_pos.loss : 1.1087690591812134
outputs_pos.loss : 1.1941896677017212
outputs_pos.loss : 1.5792397260665894
outputs_pos.loss : 1.1295409202575684
outputs_pos.loss : 1.1912065744400024
outputs_pos.loss : 1.1176435947418213
outputs_pos.loss : 1.0033247470855713
outputs_pos.loss : 1.2859145402908325
outputs_pos.loss : 1.218701720237732
outputs_pos.loss : 1.2899421453475952
outputs_pos.loss : 1.1937518119812012
outputs_pos.loss : 0.8204846978187561
outputs_pos.loss : 1.1773109436035156
outputs_pos.loss : 1.0480778217315674
outputs_pos.loss : 1.2579498291015625
outputs_pos.loss : 1.013615608215332
outputs_pos.loss : 1.1466220617294312
outputs_pos.loss : 1.1021946668624878
outputs_pos.loss : 1.7770731449127197
outputs_pos.loss : 1.1172504425048828
outputs_pos.loss : 1.223663568496704
outputs_pos.loss : 0.9170163869857788
outputs_pos.loss : 0.9958239197731018
outputs_pos.loss : 0.7487598061561584
outputs_pos.loss : 1.3312321901321411
outputs_pos.loss : 0.9105865955352783
outputs_pos.loss : 1.4726215600967407
outputs_pos.loss : 1.2499809265136719
outputs_pos.loss : 1.2345472574234009
outputs_pos.loss : 1.2013359069824219
outputs_pos.loss : 1.2895596027374268
outputs_pos.loss : 0.9151014685630798
outputs_pos.loss : 1.490139126777649
outputs_pos.loss : 1.1654592752456665
outputs_pos.loss : 0.8878114223480225
outputs_pos.loss : 1.395729422569275
outputs_pos.loss : 1.3154428005218506
outputs_pos.loss : 0.7022000551223755
outputs_pos.loss : 1.185742974281311
outputs_pos.loss : 1.3417946100234985
outputs_pos.loss : 1.1830919981002808
outputs_pos.loss : 1.451786994934082
outputs_pos.loss : 1.472513198852539
outputs_pos.loss : 1.0937951803207397
outputs_pos.loss : 1.1209744215011597
outputs_pos.loss : 1.125235676765442
outputs_pos.loss : 0.9310526251792908
outputs_pos.loss : 1.7095390558242798
outputs_pos.loss : 1.3482540845870972
outputs_pos.loss : 1.4210312366485596
outputs_pos.loss : 1.323237657546997
outputs_pos.loss : 1.1950316429138184
outputs_pos.loss : 0.9502367377281189
outputs_pos.loss : 1.0305947065353394
outputs_pos.loss : 1.0823173522949219
outputs_pos.loss : 1.1827938556671143
outputs_pos.loss : 1.3644740581512451
outputs_pos.loss : 1.231569766998291
outputs_pos.loss : 0.8455570936203003
outputs_pos.loss : 1.0944337844848633
outputs_pos.loss : 1.0676807165145874
outputs_pos.loss : 1.1687804460525513
outputs_pos.loss : 1.2196851968765259
outputs_pos.loss : 1.2563689947128296
outputs_pos.loss : 0.9323487281799316
outputs_pos.loss : 1.1693613529205322
outputs_pos.loss : 1.4013605117797852
outputs_pos.loss : 1.134751796722412
outputs_pos.loss : 1.7898246049880981
outputs_pos.loss : 0.9970859885215759
outputs_pos.loss : 1.4521355628967285
outputs_pos.loss : 1.180661678314209
outputs_pos.loss : 1.0253934860229492
outputs_pos.loss : 0.8127788305282593
outputs_pos.loss : 0.8713075518608093
outputs_pos.loss : 1.4584771394729614
outputs_pos.loss : 0.9234001040458679
outputs_pos.loss : 0.961506724357605
outputs_pos.loss : 1.6596072912216187
outputs_pos.loss : 0.9716762900352478
outputs_pos.loss : 1.0141346454620361
outputs_pos.loss : 1.0828667879104614
outputs_pos.loss : 1.2517424821853638
outputs_pos.loss : 0.9115967750549316
outputs_pos.loss : 1.2777843475341797
outputs_pos.loss : 1.2846038341522217
outputs_pos.loss : 1.3969584703445435
outputs_pos.loss : 1.1579992771148682
outputs_pos.loss : 0.76644366979599
outputs_pos.loss : 0.8665618300437927
outputs_pos.loss : 1.489151120185852
outputs_pos.loss : 0.7039895057678223
outputs_pos.loss : 0.9000291228294373
outputs_pos.loss : 1.3468250036239624
outputs_pos.loss : 0.9283630847930908
outputs_pos.loss : 1.2818853855133057
outputs_pos.loss : 1.0189404487609863
outputs_pos.loss : 0.9861981272697449
outputs_pos.loss : 1.2185381650924683
outputs_pos.loss : 0.8709492683410645
outputs_pos.loss : 0.9005324244499207
outputs_pos.loss : 1.3173587322235107
outputs_pos.loss : 1.0575968027114868
outputs_pos.loss : 1.3073585033416748
outputs_pos.loss : 0.6918476819992065
outputs_pos.loss : 1.2513483762741089
outputs_pos.loss : 1.1065036058425903
outputs_pos.loss : 1.354032039642334
outputs_pos.loss : 1.1786677837371826
outputs_pos.loss : 1.2745435237884521
outputs_pos.loss : 0.9375619292259216
outputs_pos.loss : 1.1905767917633057
outputs_pos.loss : 0.8165568113327026
outputs_pos.loss : 1.3598291873931885
outputs_pos.loss : 0.7136656045913696
outputs_pos.loss : 1.0329737663269043
outputs_pos.loss : 1.519452691078186
outputs_pos.loss : 0.9275134205818176
outputs_pos.loss : 1.096192717552185
outputs_pos.loss : 0.9844350218772888
outputs_pos.loss : 1.1974804401397705
outputs_pos.loss : 1.2936720848083496
outputs_pos.loss : 1.3917882442474365
outputs_pos.loss : 1.4095560312271118
outputs_pos.loss : 0.9629096984863281
outputs_pos.loss : 1.3147248029708862
outputs_pos.loss : 1.205826997756958
outputs_pos.loss : 1.1976879835128784
outputs_pos.loss : 1.615186333656311
outputs_pos.loss : 1.055349349975586
outputs_pos.loss : 0.8471229672431946
outputs_pos.loss : 0.9623546004295349
outputs_pos.loss : 1.0524368286132812
outputs_pos.loss : 0.744329571723938
outputs_pos.loss : 1.150287389755249
outputs_pos.loss : 1.6173232793807983
outputs_pos.loss : 0.9598588347434998
outputs_pos.loss : 1.2524100542068481
outputs_pos.loss : 1.1322431564331055
outputs_pos.loss : 1.0339750051498413
outputs_pos.loss : 0.8324555158615112
outputs_pos.loss : 1.1111868619918823
outputs_pos.loss : 1.1568412780761719
outputs_pos.loss : 1.0406262874603271
outputs_pos.loss : 1.0633454322814941
outputs_pos.loss : 1.3366373777389526
outputs_pos.loss : 0.7190408706665039
outputs_pos.loss : 1.3058875799179077
outputs_pos.loss : 1.2882397174835205
max length: 512
Token indices sequence length is longer than the specified maximum sequence length for this model (81 > 77). Running this sequence through the model will result in indexing errors
Candidates: [("[INST]<image>\nIs there anything in the suitcase?[/INST]Yes, there is a suitcase in the image. The suitcase is placed on the ground near the woman who is sitting on the bench. It's likely that the suitcase contains personal belongings or items for her trip. The presence of the suitcase suggests that she might be traveling or has recently arrived at her destination. ", -inf), ('[INST]<image>\nIs there anything in the suitcase?[/INST]Yes, there is something in the suitcase. The image shows a person holding a suitcase with a towel inside it. This suggests that the person may be preparing for a trip or has just returned from one and is unpacking their belongings. The presence of the towel indicates that they might be planning to use it for various purposes such as cleaning, drying, or simply packing it for personal use during their travels. ', -inf), ("[INST]<image>\nIs there anything in the suitcase?[/INST]No, there is no suitcase in the image. The focus of the image is on a group of people sitting on the grass and enjoying their time outdoors. The presence of the suitcase would suggest that they are traveling or have just arrived at their destination, but it's not visible in this particular scene. ", -inf)]
Decoded response: [INST]<image>
Is there anything in the suitcase?[/INST]Yes, there is a suitcase in the image. The suitcase is placed on the ground near the woman who is sitting on the bench. It's likely that the suitcase contains personal belongings or items for her trip. The presence of the suitcase suggests that she might be traveling or has recently arrived at her destination.
max length: 512
Token indices sequence length is longer than the specified maximum sequence length for this model (205 > 77). Running this sequence through the model will result in indexing errors
Candidates: [("[INST]<image>\nIdentify and describe each object in the image in detail.[/INST]In the image, there are several objects that can be identified. The main focus is on a large, white airplane with a distinctive red and blue stripe on its body. The plane is in the process of taking off from a runway, suggesting it's either departing or arriving at an airport.\n\nIn addition to the airplane, there are also multiple people present in the scene. Some of them appear to be standing near the runway, possibly airport staff or ground crew members who are involved in the operation of the aircraft. Their presence adds a sense of scale and activity to the image, highlighting the complex processes that take place at airports.\n\nThe combination of the airplane, runway, and people creates a dynamic scene that captures a moment in time at an airport. It's a snapshot of the constant movement and activity that characterizes such locations, where every second counts and every detail plays a role in ensuring safe and efficient travel. ", -inf), ("[INST]<image>\nIdentify and describe each object in the image in detail.[/INST]In the image, there's a man who appears to be in his 50s or 60s. He has short gray hair and is wearing a black jacket. The man is sitting at a table with a red tablecloth, and on the table, there's a plate of food that includes a piece of meat and some vegetables. The setting suggests that the man might be in a restaurant or a similar dining environment. The overall atmosphere seems to be casual and relaxed, with the man enjoying his meal. The image captures a moment of everyday life, possibly during lunchtime. The man's attire and the presence of the food indicate that this could be a typical dining experience for someone in his age group. The red tablecloth adds a touch of color to the scene, making it more visually appealing. The image does not contain any text or other objects that can be confidently identified. The focus is on the man and his meal, providing a snapshot of a simple yet meaningful moment. ", -inf), ("[INST]<image>\nIdentify and describe each object in the image in detail.[/INST]The image features a young man with a vibrant blue and white baseball cap. He is wearing a white shirt adorned with red and blue patterns, paired with a pair of black pants. The man is seated on a chair, holding a pink and white object in his hands. The setting appears to be an indoor space, possibly a room or a hall. The overall scene captures a casual moment of the individual enjoying his time. The objects and their arrangement suggest a relaxed atmosphere, with the man being the central focus of the image. The colors and patterns on his shirt add a touch of personality and style to the scene. The pink and white object he holds might indicate some form of activity or interest he has. The indoor setting provides a comfortable and familiar backdrop for this moment. Overall, the image presents a snapshot of everyday life, showcasing the individual's style and the environment he is in. ", -inf)]
Decoded response: [INST]<image>
Identify and describe each object in the image in detail.[/INST]In the image, there are several objects that can be identified. The main focus is on a large, white airplane with a distinctive red and blue stripe on its body. The plane is in the process of taking off from a runway, suggesting it's either departing or arriving at an airport.

In addition to the airplane, there are also multiple people present in the scene. Some of them appear to be standing near the runway, possibly airport staff or ground crew members who are involved in the operation of the aircraft. Their presence adds a sense of scale and activity to the image, highlighting the complex processes that take place at airports.

The combination of the airplane, runway, and people creates a dynamic scene that captures a moment in time at an airport. It's a snapshot of the constant movement and activity that characterizes such locations, where every second counts and every detail plays a role in ensuring safe and efficient travel.
max length: 512
Token indices sequence length is longer than the specified maximum sequence length for this model (153 > 77). Running this sequence through the model will result in indexing errors
Candidates: [('[INST]<image>\nWhy is the person in the white shirt wearing a large leather glove on his left hand?[/INST]The person in the white shirt is wearing a large leather glove on his left hand because he is playing the cello. Cello players often wear gloves to protect their hands from the pressure and friction caused by pressing the strings on the instrument. The glove helps to reduce the risk of blisters and injuries, allowing the musician to play for longer periods without discomfort. Additionally, gloves can also provide better grip and control over the instrument, improving the overall performance of the player. In this case, the person is likely using the glove to enhance his playing experience and maintain a high level of skill while performing on the cello. ', -inf), ('[INST]<image>\nWhy is the person in the white shirt wearing a large leather glove on his left hand?[/INST]The person in the white shirt is wearing a large leather glove on his left hand likely because he is playing baseball. Baseball players often wear gloves to protect their hands from the impact of the ball and to improve their grip on it during gameplay. The glove helps prevent injuries and allows for better control over the ball. In this image, the player seems to be in action, possibly catching or throwing the ball, which requires the use of a glove for safety and performance. ', -inf), ("[INST]<image>\nWhy is the person in the white shirt wearing a large leather glove on his left hand?[/INST]The person in the white shirt is wearing a large leather glove on his left hand because it's a part of his baseball uniform. The glove, also known as a catcher's mitt, is essential equipment for catchers in baseball games. It helps protect their hands from the high-speed pitches thrown by pitchers and allows them to catch and securely hold onto the ball more effectively. The glove is typically made of leather and has extra padding to provide additional protection and comfort during gameplay. In this image, the catcher is likely preparing to catch a pitch or waiting for the ball to be thrown by the pitcher. ", -inf)]
Decoded response: [INST]<image>
Why is the person in the white shirt wearing a large leather glove on his left hand?[/INST]The person in the white shirt is wearing a large leather glove on his left hand because he is playing the cello. Cello players often wear gloves to protect their hands from the pressure and friction caused by pressing the strings on the instrument. The glove helps to reduce the risk of blisters and injuries, allowing the musician to play for longer periods without discomfort. Additionally, gloves can also provide better grip and control over the instrument, improving the overall performance of the player. In this case, the person is likely using the glove to enhance his playing experience and maintain a high level of skill while performing on the cello.
max length: 512
Token indices sequence length is longer than the specified maximum sequence length for this model (131 > 77). Running this sequence through the model will result in indexing errors
Candidates: [("[INST]<image>\nLook at the image and describe the celebrity's facial expressions, clothing, and any distinctive features.[/INST]The celebrity, a young man with long hair, is wearing a black shirt and has a serious facial expression. He is also holding a book in his hands, suggesting that he might be reading or studying. The setting appears to be indoors, possibly in a room with a window. The combination of his attire and the book he's holding gives an impression of focus and dedication to his work or study. The image captures a moment that reflects the intensity and commitment often associated with academic or professional pursuits. ", -inf), ("[INST]<image>\nLook at the image and describe the celebrity's facial expressions, clothing, and any distinctive features.[/INST]The celebrity, a woman, is wearing a black dress and has a serious facial expression. Her distinctive features include her blonde hair and the earrings she's wearing. The image captures her in a moment of deep thought or concentration. The black dress she's wearing adds to the overall mood of the photo. The combination of her attire and expression suggests she might be in a professional or formal setting. However, without more context, it's hard to determine the exact nature of the event or situation she's in. ", -inf), ("[INST]<image>\nLook at the image and describe the celebrity's facial expressions, clothing, and any distinctive features.[/INST]The celebrity is a woman with a surprised or shocked expression, wearing a black jacket. She has blonde hair and is holding a microphone. Her attire and the context suggest she might be at an event or gathering where she's addressing an audience. The distinctive features of her outfit include the black jacket, which adds a touch of formality to her appearance, and the microphone, indicating her role as a speaker or presenter. The surprised expression on her face could be a reaction to an unexpected event or announcement, adding an element of intrigue to the scene. Overall, the image captures a moment of surprise and engagement, with the woman in the black jacket as the focal point. ", -inf)]
Decoded response: [INST]<image>
Look at the image and describe the celebrity's facial expressions, clothing, and any distinctive features.[/INST]The celebrity, a young man with long hair, is wearing a black shirt and has a serious facial expression. He is also holding a book in his hands, suggesting that he might be reading or studying. The setting appears to be indoors, possibly in a room with a window. The combination of his attire and the book he's holding gives an impression of focus and dedication to his work or study. The image captures a moment that reflects the intensity and commitment often associated with academic or professional pursuits.
